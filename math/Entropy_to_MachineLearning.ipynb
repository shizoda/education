{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i6y6ieZpyci"
   },
   "source": [
    "# 🎯 MNISTで学ぶAIの「学習」と「賢さ」の測り方\n",
    "\n",
    "## 📚 この課題で学ぶこと\n",
    "\n",
    "こんにちは！この課題では、手書き数字の認識を通して、AIがどのようにして賢くなるのか、その「**学習の仕組み**」を情報理論の視点から探ります。\n",
    "\n",
    "### 🤔 AIはどうやって学習するの？\n",
    "\n",
    "AIが画像を見て「これは7です！」と答える裏側では、膨大な計算が行われています。AIは最初から賢いわけではありません。たくさんのデータを見て、「正解」と自分の「予測」の「**間違い**」を計算し、その間違いが少しでも小さくなるように自分自身を何度も何度も更新していきます。このプロセスが「**学習**」です。\n",
    "\n",
    "この「間違いの大きさ」を測る物差しが、今回のテーマである**交差エントロピー**です。\n",
    "\n",
    "### 📖 今日学ぶキーワード\n",
    "\n",
    "* **Softmax関数**: AIの出力を、0から9までの各数字である「確率」に変換する魔法の関数 🎲。\n",
    "* **One-hot エンコーディング**: コンピュータが「正解」を扱いやすい形にするための表現方法。\n",
    "* **交差エントロピー (損失関数)**: AIの「予測」と「正解」のズレを測る**学習の道しるべ** 📏。AIはこれを小さくすることを目指します。\n",
    "* **エントロピー**: 学習が終わったAIの予測が、どれくらい「確信」に満ちているか（または「迷っている」か）を測る**信頼度の指標** 💪。\n",
    "* **不確実性**: AIの「うーん、7かな？1かな？」という\"迷い\"のこと。エントロピーで数値化できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXLoVACupxrJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# seabornスタイルの設定（バージョン対応）\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    try:\n",
    "        plt.style.use('seaborn')\n",
    "    except:\n",
    "        pass  # デフォルトスタイルを使用\n",
    "\n",
    "!pip install japanize-matplotlib\n",
    "import japanize_matplotlib\n",
    "\n",
    "\n",
    "print(\"🎉 ライブラリの読み込み完了！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG49z3R5qSN2"
   },
   "source": [
    "## 📊 データの準備とビジュアライゼーション\n",
    "\n",
    "MNISTデータセットを読み込んで、どんな手書き数字があるか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWjHeA7OqVXZ"
   },
   "outputs": [],
   "source": [
    "# MNISTデータセットの読み込み（サンプル数を制限してCPUでも扱いやすくする）\n",
    "print(\"📥 MNISTデータセットを読み込み中...\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "\n",
    "# データサイズを制限（CPUで扱いやすくするため）\n",
    "n_samples = 10000\n",
    "X, y = mnist.data[:n_samples], mnist.target[:n_samples]\n",
    "y = y.astype(int)\n",
    "\n",
    "print(f\"✅ 読み込み完了！ データサイズ: {X.shape}\")\n",
    "print(f\"   画像サイズ: 28×28ピクセル\")\n",
    "print(f\"   クラス数: {len(np.unique(y))}クラス (0-9の数字)\")\n",
    "\n",
    "japanize_matplotlib.japanize()\n",
    "# いくつかの手書き数字を表示\n",
    "fig, axes = plt.subplots(2, 10, figsize=(15, 3))\n",
    "for i in range(10):\n",
    "    # 各数字の最初の2つの例を表示\n",
    "    indices = np.where(y == i)[0][:2]\n",
    "    for j, idx in enumerate(indices):\n",
    "        axes[j, i].imshow(X.iloc[idx].values.reshape(28, 28), cmap='gray')\n",
    "        axes[j, i].set_title(f'Label: {i}', fontsize=10)\n",
    "        axes[j, i].axis('off')\n",
    "\n",
    "plt.suptitle('MNISTデータセットの例', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2Yr7e3SqZn1"
   },
   "source": [
    "## 🧠 シンプルなニューラルネットワークの訓練\n",
    "\n",
    "手書き数字を分類するモデルを作ります。今回は軽量なMLP分類器を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzqO4ergqhhH"
   },
   "outputs": [],
   "source": [
    "# データの前処理\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 正規化（0-1の範囲に調整）\n",
    "X_train_scaled = X_train / 255.0\n",
    "X_test_scaled = X_test / 255.0\n",
    "\n",
    "print(f\"📚 訓練データ: {X_train_scaled.shape[0]}枚\")\n",
    "print(f\"🧪 テストデータ: {X_test_scaled.shape[0]}枚\")\n",
    "\n",
    "# ニューラルネットワークの作成と訓練\n",
    "print(\"\\n🧠 ニューラルネットワークを訓練中...\")\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),  # 隠れ層：128個、64個のニューロン\n",
    "    activation='relu',\n",
    "    max_iter=10,  # 軽量化のため回数を制限\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 訓練完了後の精度確認\n",
    "train_accuracy = model.score(X_train_scaled, y_train)\n",
    "test_accuracy = model.score(X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\n✅ 訓練完了！\")\n",
    "print(f\"📊 訓練精度: {train_accuracy:.3f}\")\n",
    "print(f\"🎯 テスト精度: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTcidL4l8GDe"
   },
   "source": [
    "### 📈 訓練過程を覗いてみよう\n",
    "\n",
    "訓練中のログに `loss` という項目がありましたね。\n",
    "\n",
    "`Iteration 1, loss = ...`\n",
    "`Iteration 2, loss = ...`\n",
    "...\n",
    "`Iteration 8, loss = ...`\n",
    "\n",
    "この `loss`（損失）の値が、どんどん小さくなっているのがわかりますか？ これこそが、AIが学習している証拠です。AIは、この **loss = 誤差・損失（今回の場合は誤差関数として交差エントロピーを使用）が最小になるように、内部のパラメータ（重み）を自動で調整している**のです。\n",
    "\n",
    "**誤差を計算するには出力と正解を比較する**わけですが、出力は確率に変換しなければいけません。どういうことでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmcoRgnpqlN4"
   },
   "source": [
    "## 🎲 Softmax関数：AIの出力を「確率」に変換する\n",
    "\n",
    "ニューラルネットワークは、入力画像に対し、各数字（0〜9）の「っぽさ」を示すスコアを出力します。これでは比較しにくいため、**Softmax関数**を使って、合計が100%（=1.0）になる「**確率**」に変換します。\n",
    "\n",
    "### 🤖 なぜ確率に変換するの？\n",
    "後のステップで、AIの予測と正解を比較して「間違いの大きさ（損失）」を計算するために、確率という統一された尺度が必要だからです。\n",
    "\n",
    "### 📐 Softmaxの計算方法\n",
    "$$ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{N} e^{x_j}} $$\n",
    "\n",
    "数式が苦手な方もご安心ください。これは以下の2ステップを行っています。\n",
    "\n",
    "1.  **スコアを増幅する**: まず `e` という数字（約2.718）を使い、各スコアを増幅させます。これはスコアの差を強調する効果があり、最も「っぽい」スコアを際立たせます。\n",
    "    > たとえ話：人気投票でAさん10票、Bさん8票を、Aさん60点、Bさん30点のように差を広げて見せるイメージです。\n",
    "2.  **全体で割って割合を出す**: 増幅した全スコアの合計で、個々の増幅スコアを割ります。これにより、全体におけるそれぞれの「分け前（＝確率）」が計算できます。\n",
    "\n",
    "**AIが最終的に予測するのは、この確率が最も大きくなったクラスです。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYfiUL9TrHUk"
   },
   "source": [
    "# 📊 エントロピー：学習済みモデルの「予測の信頼度」を測る\n",
    "\n",
    "エントロピーは確率分布の「不確かさ」や「予測の難しさ」を表す指標だと学びましたね。確率が全ての選択肢で均等なとき、エントロピーは最大になりました。ここではその知識を応用して、学習が終わったAIの予測がどれだけ「確信」に満ちているか（あるいは「迷っている」か）を見ていきましょう。この「迷いの度合い」を測るのが、ここでのエントロピーの役割です。\n",
    "\n",
    "学習が終わったモデルが、ある画像を見て「これは9です！」と予測したとします。このとき、モデルはどれくらいその予測に「自信」を持っているのでしょうか？\n",
    "\n",
    "-   「9である確率99%、他の数字はほぼ0%」 → **自信満々**\n",
    "-   「9である確率40%、4である確率35%、他は...」 → **かなり迷っている**\n",
    "\n",
    "この予測の「自信のなさ」や「迷いの度合い」（＝**不確実性**）を数値で表すのが**エントロピー**です。\n",
    "\n",
    "### 💡 エントロピーのポイント\n",
    "-   **役割**: 学習済みモデルの**評価・分析**に使う道具。\n",
    "-   **見方**:\n",
    "    -   **エントロピーが低い**: 確率がどれか一つに集中している状態。予測への**信頼度が高い**（自信がある）。\n",
    "    -   **エントロピーが高い**: 確率が複数の選択肢に分散している状態。予測への**信頼度が低い**（迷っている）。\n",
    "\n",
    "エントロピーは、AIがどんな問題が苦手なのかを分析するのに役立ちますが、**AIが学習する際に直接使う「間違いの物差し」ではありません。** その役割を担うのが、次に出てくる「交差エントロピー」です。\n",
    "\n",
    "### 🧮 エントロピーの計算式\n",
    "$$ H(X) = -\\sum_{i=1}^{n} p(x_i) \\log_2 p(x_i) $$\n",
    "- $p(x_i)$: ある一つの予測における、クラス $i$ の確率。\n",
    "- $n$: クラス数。\n",
    "\n",
    "まずは仮想的なデータで確かめてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIdQsOvarhlK"
   },
   "outputs": [],
   "source": [
    "# エントロピー計算関数\n",
    "def calculate_entropy(probabilities):\n",
    "    \"\"\"エントロピーを計算する関数\"\"\"\n",
    "    # 0の確率を除外（log(0)は未定義のため）\n",
    "    p_filtered = probabilities[probabilities > 0]\n",
    "    return -np.sum(p_filtered * np.log2(p_filtered))\n",
    "\n",
    "# 様々な確率分布でエントロピーを計算\n",
    "distributions = {\n",
    "    \"確実な予測\": np.array([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    \"ほぼ確実\": np.array([0.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    \"やや確実\": np.array([0.7, 0.2, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    \"不確実\": np.array([0.4, 0.3, 0.2, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    \"非常に不確実\": np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "}\n",
    "\n",
    "print(\"🎯 様々な確率分布のエントロピー\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "japanize_matplotlib.japanize()\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, dist) in enumerate(distributions.items()):\n",
    "    entropy = calculate_entropy(dist)\n",
    "    print(f\"{name:15s}: エントロピー = {entropy:.3f} bit\")\n",
    "\n",
    "    # 各分布を可視化\n",
    "    if i < len(axes):\n",
    "        axes[i].bar(range(10), dist, color='lightblue', alpha=0.7)\n",
    "        axes[i].set_title(f'{name}\\nエントロピー: {entropy:.3f} bit')\n",
    "        axes[i].set_xlabel('クラス')\n",
    "        axes[i].set_ylabel('確率')\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].set_xticks(range(10))\n",
    "\n",
    "# 最後のサブプロットを非表示\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 ポイント:\")\n",
    "print(\"   - エントロピーが低い = 確実性が高い\")\n",
    "print(\"   - エントロピーが高い = 不確実性が高い\")\n",
    "print(\"   - 最大エントロピー = log₂(10) ≈ 3.32 bit（10クラス均等分布）\")\n",
    "\n",
    "# テストデータから予測確率を取得\n",
    "print(\"🔍 テストデータで予測確率を計算中...\")\n",
    "pred_probs = model.predict_proba(X_test_scaled)\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# 各予測のエントロピーを計算\n",
    "entropies = []\n",
    "for prob in pred_probs:\n",
    "    entropy = calculate_entropy(prob)\n",
    "    entropies.append(entropy)\n",
    "\n",
    "entropies = np.array(entropies)\n",
    "\n",
    "# 予測の正確性を確認\n",
    "correct_predictions = (predictions == y_test)\n",
    "\n",
    "print(f\"\\n📊 予測結果の統計\")\n",
    "print(f\"平均エントロピー: {np.mean(entropies):.3f} bit\")\n",
    "print(f\"エントロピーの標準偏差: {np.std(entropies):.3f} bit\")\n",
    "print(f\"正解率: {np.mean(correct_predictions):.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQa8NVokr4DW"
   },
   "source": [
    "## 🎭 予測の信頼度による画像の分類\n",
    "\n",
    "エントロピーを使って、モデルがどの画像を「確信を持って予測」し、どの画像を「迷って予測」したかを見てみましょう。まずは具体的な例ではなく、仮想的な10クラスの出力をもとに可視化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYn3Zb4crFBX"
   },
   "outputs": [],
   "source": [
    "# Softmax関数の実装\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax関数の実装\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x))  # 数値の安定性のため最大値を引く\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# 例：ネットワークのロジット（生のスコア）\n",
    "raw_scores = np.array([2.0, 1.0, 0.1, 0.5, 0.2, 0.3, 0.8, 0.4, 0.6, 0.9])\n",
    "probabilities = softmax(raw_scores)\n",
    "\n",
    "print(\"🎯 Softmax関数の動作例\")\n",
    "print(\"生のスコア:\", raw_scores)\n",
    "print(\"確率:\", np.round(probabilities, 2).tolist())\n",
    "print(f\"確率の合計: {np.sum(probabilities):.6f}\")\n",
    "\n",
    "# 可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# 生のスコア\n",
    "japanize_matplotlib.japanize()\n",
    "ax1.bar(range(10), raw_scores, color='skyblue', alpha=0.7)\n",
    "ax1.set_title('生のスコア（logits）')\n",
    "ax1.set_xlabel('クラス (0-9)')\n",
    "ax1.set_ylabel('スコア')\n",
    "ax1.set_xticks(range(10))\n",
    "\n",
    "# 確率\n",
    "ax2.bar(range(10), probabilities, color='lightcoral', alpha=0.7)\n",
    "ax2.set_title('Softmax 後の確率')\n",
    "ax2.set_xlabel('クラス (0-9)')\n",
    "ax2.set_ylabel('確率')\n",
    "ax2.set_xticks(range(10))\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🎯 予測クラス: {np.argmax(probabilities)} (確率: {np.max(probabilities):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSNX3fL0UicE"
   },
   "source": [
    "### 🤔 課題1: ロジットと softmax\n",
    "\n",
    "- クラス 3 に分類されるよう、上記のコードにおけるロジットを修正してください。\n",
    "\n",
    "\n",
    "- 正解がクラス 3 だったとしたら、One-hot エンコーディングはどうなりますか？\n",
    "\n",
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVZY9k5hrfjM"
   },
   "source": [
    "## 🎯 実際の予測でのエントロピー分析\n",
    "\n",
    "訓練したモデルを使って、実際の予測におけるエントロピーを調べてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joYUILEzr3Xh"
   },
   "outputs": [],
   "source": [
    "# エントロピーでソートして、確信度の高い・低い画像を抽出\n",
    "sorted_indices = np.argsort(entropies)\n",
    "\n",
    "# 最も確信度の高い画像（エントロピーが低い）\n",
    "most_confident_indices = sorted_indices[:10]\n",
    "# 最も確信度の低い画像（エントロピーが高い）\n",
    "least_confident_indices = sorted_indices[-10:]\n",
    "\n",
    "def show_predictions(indices, title, entropies, pred_probs, X_test, y_test, predictions):\n",
    "    \"\"\"予測結果を可視化する関数\"\"\"\n",
    "\n",
    "    japanize_matplotlib.japanize()\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        # 画像表示\n",
    "        axes[i].imshow(X_test.iloc[idx].values.reshape(28, 28), cmap='gray')\n",
    "\n",
    "        # タイトル情報\n",
    "        true_label = y_test.iloc[idx]\n",
    "        pred_label = predictions[idx]\n",
    "        entropy = entropies[idx]\n",
    "        max_prob = np.max(pred_probs[idx])\n",
    "\n",
    "        # 正解・不正解の判定\n",
    "        is_correct = \"[〇]\" if true_label == pred_label else \"[×]\"\n",
    "\n",
    "        axes[i].set_title(f'{is_correct} 正解:{true_label} 予測:{pred_label}\\n'\n",
    "                         f'エントロピー:{entropy:.2f}\\n確率:{max_prob:.2f}',\n",
    "                         fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 確信度の高い予測\n",
    "print(\"最も確信度の高い予測（エントロピーが小さい）\")\n",
    "print(\"=\"*50)\n",
    "show_predictions(most_confident_indices, \"最も確信度の高い予測\",\n",
    "                entropies, pred_probs, X_test_scaled, y_test, predictions)\n",
    "\n",
    "# 確信度の低い予測\n",
    "print(\"\\n最も確信度の低い予測（エントロピーが大きい）\")\n",
    "print(\"=\"*50)\n",
    "show_predictions(least_confident_indices, \"最も確信度の低い予測\",\n",
    "                entropies, pred_probs, X_test_scaled, y_test, predictions)\n",
    "\n",
    "# 統計情報 - インデックスの問題を修正\n",
    "# y_testのインデックスをリセットして連続にする\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "correct_predictions_reset = (predictions == y_test_reset)\n",
    "\n",
    "high_conf_correct = np.mean(correct_predictions_reset[most_confident_indices])\n",
    "low_conf_correct = np.mean(correct_predictions_reset[least_confident_indices])\n",
    "\n",
    "print(f\"\\n📊 統計情報\")\n",
    "print(f\"高確信度予測の正解率: {high_conf_correct:.2f}\")\n",
    "print(f\"低確信度予測の正解率: {low_conf_correct:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9FRf-EtThAn"
   },
   "outputs": [],
   "source": [
    "# エントロピーの分布を可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 全体のエントロピー分布\n",
    "axes[0, 0].hist(entropies, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(np.mean(entropies), color='red', linestyle='--', label=f'平均: {np.mean(entropies):.3f}')\n",
    "axes[0, 0].set_title('全予測のエントロピー分布')\n",
    "axes[0, 0].set_xlabel('エントロピー (bit)')\n",
    "axes[0, 0].set_ylabel('頻度')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 正解・不正解別のエントロピー分布\n",
    "correct_entropies = entropies[correct_predictions]\n",
    "incorrect_entropies = entropies[~correct_predictions]\n",
    "\n",
    "axes[0, 1].hist(correct_entropies, bins=30, alpha=0.7, color='green', label='正解', density=True)\n",
    "axes[0, 1].hist(incorrect_entropies, bins=30, alpha=0.7, color='red', label='不正解', density=True)\n",
    "axes[0, 1].set_title('正解・不正解別エントロピー')\n",
    "axes[0, 1].set_xlabel('エントロピー (bit)')\n",
    "axes[0, 1].set_ylabel('密度')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "print(f\"\\n🎯 正解時の平均エントロピー: {np.mean(correct_entropies):.3f} bit\")\n",
    "print(f\"❌ 不正解時の平均エントロピー: {np.mean(incorrect_entropies):.3f} bit\")\n",
    "\n",
    "# 最大確率とエントロピーの関係\n",
    "max_probs = np.max(pred_probs, axis=1)\n",
    "axes[1, 0].scatter(entropies, max_probs, alpha=0.5, s=1)\n",
    "axes[1, 0].set_title('エントロピーと最大確率の関係')\n",
    "axes[1, 0].set_xlabel('エントロピー (bit)')\n",
    "axes[1, 0].set_ylabel('最大確率')\n",
    "\n",
    "# クラス別エントロピー\n",
    "class_entropies = []\n",
    "for i in range(10):\n",
    "    class_mask = (y_test == i)\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_entropies.append(np.mean(entropies[class_mask]))\n",
    "    else:\n",
    "        class_entropies.append(0)\n",
    "\n",
    "axes[1, 1].bar(range(10), class_entropies, color='lightcoral', alpha=0.7)\n",
    "axes[1, 1].set_title('クラス別 平均エントロピー')\n",
    "axes[1, 1].set_xlabel('数字クラス')\n",
    "axes[1, 1].set_ylabel('平均エントロピー (bit)')\n",
    "axes[1, 1].set_xticks(range(10))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 観察ポイント:\")\n",
    "print(\"   - 正解時のエントロピーは低い傾向\")\n",
    "print(\"   - 不正解時のエントロピーは高い傾向\")\n",
    "print(\"   - エントロピーが低い = モデルが確信を持った予測\")\n",
    "print(\"   - エントロピーが高い = モデルが迷った予測\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1ENfcSnVLOS"
   },
   "source": [
    "### 🤔 課題 2: エントロピーと正解・不正解\n",
    "\n",
    "エントロピーが大きいほど正解しやすいですか、それとも間違えやすいですか？\n",
    "\n",
    "上記のグラフを読み取りつつ、理由とともに回答してください。\n",
    "\n",
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wvz4KHZMrkQi"
   },
   "source": [
    "# 🎯 交差エントロピー：学習の道しるべとなる「損失関数」\n",
    "\n",
    "いよいよ、AIが学習するための最重要パーツである**損失関数**です。AIは、**予測と正解の「ズレ」**を計算し、それを最小化するのでしたね。この「ズレ」を測る物差しが**交差エントロピー**です。\n",
    "\n",
    "\n",
    "### 🧮 交差エントロピーの計算方法\n",
    "$$ H(p, q) = -\\sum_{i=1}^{n} p(x_i) \\log q(x_i) $$\n",
    "- $p(x_i)$: **正解**の確率分布（One-hotエンコーディング）\n",
    "- $q(x_i)$: **予測**の確率分布（Softmaxの出力）\n",
    "\n",
    "この計算が賢いのは、**実質的に「正解」だったクラスに対して、AIがどれくらいの確率を予測したかだけを見ている**点です。\n",
    "\n",
    "例えば正解が「7」の場合、この計算は最終的に `–log(AIが7と予測した確率)` というシンプルな形になります。\n",
    "\n",
    "-   AIが「7である確率95%」と高く予測すれば、`–log(0.95)` は**小さな値**になり、AIは褒められます（損失が小さい）。\n",
    "-   AIが「7である確率5%」と低く予測すれば、`–log(0.05)` は**大きな値**になり、ペナルティを受けます（損失が大きい）。\n",
    "\n",
    "> たとえ話：🎯 ダーツの的当てゲームです。交差エントロピーは、的のど真ん中（正解）から矢が外れた**距離（＝間違いの大きさ）**です。AIはこの距離が常に最小になるよう、投げるフォームを何度も微調整（学習）していくのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deBI-VciB-2y"
   },
   "source": [
    "### ⚙️ 誤差逆伝播法：AIが「反省」して賢くなる仕組み\n",
    "\n",
    "さて、AIは「間違いの大きさ（＝交差エントロピー損失）」を計算できるようになりました。\n",
    "では、その数値をどうやってAIの成長、つまり「学習」に繋げるのでしょうか？\n",
    "\n",
    "その答えが、ニューラルネットワークの学習における心臓部、**誤差逆伝播法（Backpropagation）**です。\n",
    "\n",
    "これは、**最終的な「誤差」について、出力側から入力側へさかのぼり、ネットワーク内部の全ての「重み」を少しずつ賢い方向に修正していく**手法です。\n",
    "\n",
    "#### 🤔 どんな仕組み？\n",
    "\n",
    "学習の1サイクルは、大きく4つのステップで構成されます。\n",
    "\n",
    "1.  ➡️ **予測（順伝播 - Forward Propagation）**\n",
    "    まず、入力データ（画像）がネットワークの入り口から入ります。データは層を一つずつ進むたびに、各ニューロンの**重み**によって計算され、変換されていきます。そして最終的に「**出力**（予測された確率）」となって出てきます。この一方向の流れを**順伝播**と呼びます。\n",
    "\n",
    "2.  😠 **間違いの計算（損失の計算）**\n",
    "    順伝播によって得られた「出力（予測）」と「正解ラベル」を比べ、その**間違いの大きさ**を**交差エントロピー**のような「誤差関数」で計算します。その結果が「誤差」です。\n",
    "\n",
    "3.  👈 **間違いの原因をさかのぼる（誤差逆伝播 - Backpropagation）**\n",
    "    計算した「間違い」は、元をたどればネットワーク内部の無数の**重み**が少しずつ不適切な設定だったせいです。誤差逆伝播法は、まず**出力に最も近い層の重み**が、最終的な間違いにどれだけ影響したかを計算します。\n",
    "    次に、その結果を利用して、**一つ手前の層の重み**がどれだけ影響したかを計算します。このプロセスを、**出力から入力へ**と逆方向に繰り返していきます。これにより、**ネットワーク内の全ての重みを「**間違いが少しでも小さくなる方向**」に、ほんの僅かだけ修正（更新）します。\n",
    "\n",
    "この**「①予測 → ②間違いを計算 → ③原因をさかのぼる → ④重みを修正」**というサイクルを何千、何万回と繰り返すことで、ネットワーク全体の重みが最適化され、AIは賢くなっていくのです。\n",
    "\n",
    "最初の訓練ログで `loss` が少しずつ減っていたのは、まさにこの誤差逆伝播法によってAIが「反省と改善」を繰り返していたからです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E13WytQcr1ZM"
   },
   "outputs": [],
   "source": [
    "# 交差エントロピー計算関数\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    \"\"\"交差エントロピーを計算する関数\"\"\"\n",
    "    # 数値安定性のため、非常に小さい値で下限を設定\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "# One-hot エンコーディングの例\n",
    "print(\"🎯 One-hot エンコーディングの例\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "for digit in [0, 3, 7, 9]:\n",
    "    one_hot = np.zeros(10)\n",
    "    one_hot[digit] = 1\n",
    "    print(f\"数字 {digit}: {one_hot}\")\n",
    "\n",
    "# 交差エントロピーの実例\n",
    "print(\"\\n🧮 交差エントロピーの計算例\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 正解: 数字「3」\n",
    "true_label = np.zeros(10)\n",
    "true_label[3] = 1.0\n",
    "\n",
    "# 様々な予測パターン\n",
    "prediction_cases = {\n",
    "    \"完璧な予測\": np.array([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n",
    "    \"良い予測\": np.array([0.05, 0.05, 0.05, 0.7, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0]),\n",
    "    \"まずまずの予測\": np.array([0.1, 0.1, 0.1, 0.4, 0.1, 0.1, 0.05, 0.05, 0.0, 0.0]),\n",
    "    \"悪い予測\": np.array([0.2, 0.2, 0.2, 0.2, 0.1, 0.05, 0.03, 0.02, 0.0, 0.0]),\n",
    "    \"非常に悪い予測\": np.array([0.5, 0.3, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "}\n",
    "\n",
    "print(f\"正解ラベル: {true_label}\\n\")\n",
    "\n",
    "results = []\n",
    "for name, pred in prediction_cases.items():\n",
    "    ce = cross_entropy(true_label, pred)\n",
    "    entropy = calculate_entropy(pred)\n",
    "    confidence = np.max(pred)\n",
    "    results.append((name, ce, entropy, confidence))\n",
    "    print(f\"{name:15s}: 交差エントロピー = {ce:.3f}, エントロピー = {entropy:.3f}, 最大確率 = {confidence:.3f}\")\n",
    "\n",
    "# 可視化\n",
    "japanize_matplotlib.japanize()\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, pred) in enumerate(prediction_cases.items()):\n",
    "    if i < len(axes):\n",
    "        ce = cross_entropy(true_label, pred)\n",
    "        axes[i].bar(range(10), pred, color='lightgreen', alpha=0.7)\n",
    "        axes[i].bar(3, pred[3], color='red', alpha=0.8)  # 正解クラスを強調\n",
    "        axes[i].set_title(f'{name}\\n交差エントロピー: {ce:.3f}')\n",
    "        axes[i].set_xlabel('クラス')\n",
    "        axes[i].set_ylabel('予測確率')\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].set_xticks(range(10))\n",
    "\n",
    "# 最後のサブプロットを非表示\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 ポイント:\")\n",
    "print(\"   - 正解クラスの確率が高いほど交差エントロピーは小さい\")\n",
    "print(\"   - 正解クラスの確率が低いほど交差エントロピーは大きい\")\n",
    "print(\"   - 交差エントロピーが小さい = より良い予測\")\n",
    "\n",
    "# テストデータのone-hotエンコーディング\n",
    "def to_one_hot(labels, num_classes=10):\n",
    "    \"\"\"ラベルをone-hotエンコーディングに変換\"\"\"\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "# テストデータのone-hot化\n",
    "y_test_one_hot = to_one_hot(y_test)\n",
    "\n",
    "# 各サンプルの交差エントロピーを計算\n",
    "cross_entropies = []\n",
    "for i in range(len(y_test)):\n",
    "    ce = cross_entropy(y_test_one_hot[i], pred_probs[i])\n",
    "    cross_entropies.append(ce)\n",
    "\n",
    "cross_entropies = np.array(cross_entropies)\n",
    "\n",
    "print(f\"📊 交差エントロピーの統計\")\n",
    "print(f\"平均交差エントロピー: {np.mean(cross_entropies):.3f}\")\n",
    "print(f\"交差エントロピーの標準偏差: {np.std(cross_entropies):.3f}\")\n",
    "print(f\"最小交差エントロピー: {np.min(cross_entropies):.3f}\")\n",
    "print(f\"最大交差エントロピー: {np.max(cross_entropies):.3f}\")\n",
    "\n",
    "# 正解・不正解別の交差エントロピー\n",
    "correct_ce = cross_entropies[correct_predictions]\n",
    "incorrect_ce = cross_entropies[~correct_predictions]\n",
    "\n",
    "print(f\"\\n✅ 正解時の平均交差エントロピー: {np.mean(correct_ce):.3f}\")\n",
    "print(f\"❌ 不正解時の平均交差エントロピー: {np.mean(incorrect_ce):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ny-BG4jWV3fV"
   },
   "source": [
    "### 🤔 課題 3: One-hot エンコーディングと交差エントロピー\n",
    "\n",
    "上記のそれぞれのグラフでは、**何と比較して** 交差エントロピーを求めていますか？比較対象となるベクトルを書いてください。\n",
    "\n",
    "回答： $ [ 0, この続きを埋める ] $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JngZ76iryhy"
   },
   "source": [
    "## 🔍 実際のテストデータでの交差エントロピー分析\n",
    "\n",
    "訓練したモデルの実際の予測について、交差エントロピーを詳しく分析してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLt9fqb8p6vz"
   },
   "outputs": [],
   "source": [
    "# 可視化\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# 交差エントロピーの分布\n",
    "axes[0, 0].hist(cross_entropies, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].axvline(np.mean(cross_entropies), color='red', linestyle='--', label=f'平均: {np.mean(cross_entropies):.3f}')\n",
    "axes[0, 0].set_title('交差エントロピーの分布')\n",
    "axes[0, 0].set_xlabel('交差エントロピー')\n",
    "axes[0, 0].set_ylabel('頻度')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 正解・不正解別の交差エントロピー\n",
    "axes[0, 1].hist(correct_ce, bins=5, alpha=0.7, color='green', label='正解', density=True)\n",
    "axes[0, 1].hist(incorrect_ce, bins=30, alpha=0.7, color='red', label='不正解', density=True)\n",
    "axes[0, 1].set_title('正解・不正解別交差エントロピー')\n",
    "axes[0, 1].set_xlabel('交差エントロピー')\n",
    "axes[0, 1].set_ylabel('密度')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 交差エントロピーとエントロピーの関係\n",
    "axes[1, 0].scatter(entropies, cross_entropies, alpha=0.5, s=1)\n",
    "axes[1, 0].set_title('エントロピー vs 交差エントロピー')\n",
    "axes[1, 0].set_xlabel('エントロピー')\n",
    "axes[1, 0].set_ylabel('交差エントロピー')\n",
    "\n",
    "# 最大確率と交差エントロピーの関係\n",
    "axes[1, 1].scatter(max_probs, cross_entropies, alpha=0.5, s=1)\n",
    "axes[1, 1].set_title('最大確率 vs 交差エントロピー')\n",
    "axes[1, 1].set_xlabel('最大確率')\n",
    "axes[1, 1].set_ylabel('交差エントロピー')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpku3wvXXPZ6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4wuLzH6sOEV"
   },
   "source": [
    "### 🤔  課題 5: 不確実性による画像の難易度分析\n",
    "\n",
    "テストデータの中から、特定の数字（あなたが選んだ数字）について、エントロピーが高い（予測が困難な）画像と低い（予測が容易な）画像を比較分析してください。\n",
    "\n",
    "- 予測しやすい画像の特徴は何ですか？\n",
    "\n",
    "答え:\n",
    "\n",
    "- なぜモデルは一部の画像で迷ったと思いますか？\n",
    "\n",
    "答え:\n",
    "\n",
    "#### 💡 観察ポイント:\"\n",
    "- 確信度の高い予測は正解率が高い傾向\n",
    "- 確信度の低い予測では字が汚い、紛らわしいケースが多い\n",
    "- エントロピーは予測の信頼性の指標として使える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7ZRYMbqshAb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 📝 課題3: 数字の難易度分析\n",
    "\n",
    "# TODO: あなたが分析したい数字を選んでください（0-9）\n",
    "target_digit = 8  # ここを変更してください\n",
    "\n",
    "print(f\"📝 課題3: 数字'{target_digit}'の難易度分析\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# y_testをNumPy配列に変換（まだされていない場合）\n",
    "y_test_array = np.array(y_test)\n",
    "correct_predictions_array = (predictions == y_test_array)\n",
    "\n",
    "# 選択した数字のインデックスを取得\n",
    "target_indices = np.where(y_test_array == target_digit)[0]\n",
    "print(f\"数字{target_digit}のテスト画像数: {len(target_indices)}枚\")\n",
    "\n",
    "if len(target_indices) > 0:\n",
    "    # その数字のエントロピーと予測結果を取得\n",
    "    target_entropies = entropies[target_indices]\n",
    "    target_predictions = predictions[target_indices]\n",
    "    target_correct = correct_predictions_array[target_indices]\n",
    "    target_probs = pred_probs[target_indices]\n",
    "\n",
    "    # 統計情報\n",
    "    print(f\"\\n📊 数字{target_digit}の統計:\")\n",
    "    print(f\"   平均エントロピー: {np.mean(target_entropies):.3f}\")\n",
    "    print(f\"   正解率: {np.mean(target_correct):.3f}\")\n",
    "    print(f\"   最小エントロピー: {np.min(target_entropies):.3f}\")\n",
    "    print(f\"   最大エントロピー: {np.max(target_entropies):.3f}\")\n",
    "\n",
    "    # エントロピーでソート\n",
    "    sorted_target_indices = np.argsort(target_entropies)\n",
    "\n",
    "    # 最も予測しやすい画像（エントロピー低）\n",
    "    easy_indices = target_indices[sorted_target_indices[:5]]\n",
    "    # 最も予測しにくい画像（エントロピー高）\n",
    "    hard_indices = target_indices[sorted_target_indices[-5:]]\n",
    "\n",
    "    # 可視化関数\n",
    "    def analyze_difficulty(indices, title):\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            axes[i].imshow(X_test_scaled.iloc[idx].values.reshape(28, 28), cmap='gray')\n",
    "\n",
    "            entropy = entropies[idx]\n",
    "            pred_label = predictions[idx]\n",
    "            max_prob = np.max(pred_probs[idx])\n",
    "            is_correct = \"[〇]\" if y_test_array[idx] == pred_label else \"[×]\"\n",
    "\n",
    "            axes[i].set_title(f'{is_correct} 予測:{pred_label}\\nエントロピー:{entropy:.2f}\\n確率:{max_prob:.2f}',\n",
    "                             fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.suptitle(title, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 結果表示\n",
    "    print(f\"\\n🟢 予測しやすい数字{target_digit}の画像（エントロピー低）:\")\n",
    "    analyze_difficulty(easy_indices, f\"予測しやすい数字 {target_digit}\")\n",
    "\n",
    "    print(f\"\\n🔴 予測しにくい数字{target_digit}の画像（エントロピー高）:\")\n",
    "    analyze_difficulty(hard_indices, f\"予測しにくい数字 {target_digit}\")\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️ 数字{target_digit}のテストデータが見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xi1avSMssnwS"
   },
   "source": [
    "### 🤔 課題 6: 混同行列と不確実性の関係\n",
    "\n",
    "混同行列（Confusion Matrix）を作成し、どの数字同士が間違えやすいかを読み取ってください。どのような誤りパターンが多いですか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFtZh693sm-r"
   },
   "outputs": [],
   "source": [
    "# 📝 課題4: 混同行列と不確実性の関係\n",
    "\n",
    "print(\"📝 課題4: 混同行列と不確実性の関係\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 混同行列の作成と可視化\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title('混同行列（Confusion Matrix）')\n",
    "    plt.xlabel('予測ラベル')\n",
    "    plt.ylabel('正解ラベル')\n",
    "    plt.show()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Seaborn が見つかりません。matplotlib を使用して代替表示します。\")\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('混同行列（Confusion Matrix）')\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(10)\n",
    "    plt.xticks(tick_marks, tick_marks)\n",
    "    plt.yticks(tick_marks, tick_marks)\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.xlabel('予測ラベル')\n",
    "    plt.ylabel('正解ラベル')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 最も間違えやすい組み合わせを探す\n",
    "print(\"\\n🔍 最も間違えやすい数字の組み合わせ:\")\n",
    "errors = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            errors.append((cm[i, j], i, j))\n",
    "\n",
    "errors.sort(reverse=True)\n",
    "for count, true_label, pred_label in errors[:10]:\n",
    "    print(f\"   {true_label} → {pred_label}: {count}回の間違い\")\n",
    "\n",
    "# 高エントロピー予測での間違いパターン分析\n",
    "high_entropy_threshold = np.percentile(entropies, 80)  # 上位20%の高エントロピー\n",
    "high_entropy_mask = entropies > high_entropy_threshold\n",
    "high_entropy_incorrect = high_entropy_mask & (~correct_predictions)\n",
    "\n",
    "print(f\"\\n📈 高エントロピー予測（>{high_entropy_threshold:.2f}）での間違い分析:\")\n",
    "if np.sum(high_entropy_incorrect) > 0:\n",
    "    high_entropy_true = y_test[high_entropy_incorrect]\n",
    "    high_entropy_pred = predictions[high_entropy_incorrect]\n",
    "\n",
    "    # 高エントロピーでの間違いパターン\n",
    "    from collections import Counter\n",
    "    error_patterns = Counter(zip(high_entropy_true, high_entropy_pred))\n",
    "\n",
    "    print(\"   高エントロピー時の主な間違いパターン:\")\n",
    "    for (true_val, pred_val), count in error_patterns.most_common(5):\n",
    "        print(f\"     {true_val} → {pred_val}: {count}回\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjFkeTydssXm"
   },
   "source": [
    "# 🎓 総括課題: 情報理論と不確実性のまとめ\n",
    "\n",
    "今回の課題を通して学んだことを、以下のキーワードを**すべて含めて**、それぞれの**関係性**がわかるように自分の言葉でまとめてください。\n",
    "\n",
    "### 🔑 必須キーワード\n",
    "- **Softmax関数**\n",
    "- **One-hot エンコーディング**\n",
    "- **エントロピー**\n",
    "- **交差エントロピー**\n",
    "- **損失関数**\n",
    "\n",
    "### 💡 まとめの観点\n",
    "1.  AIの「学習」というプロセスにおいて、それぞれのキーワードが**どの段階でどのような役割**を果たしているか。\n",
    "2.  **エントロピー**と**交差エントロピー**の**重要な違い**は何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcsA35jDsxhX"
   },
   "source": [
    "## 🎉 おつかれさまでした！\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

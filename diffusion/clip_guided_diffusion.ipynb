{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/diffusion/clip_guided_diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwMUyt9LHG1"
      },
      "source": [
        "# テキストプロンプトからの画像生成\n",
        "\n",
        "今回は以下の3つの技術を用いて、テキストプロンプトからの画像生成を行います。\n",
        "\n",
        "#### 主要な用語\n",
        "\n",
        "- **CLIP** (Contrastive Language–Image Pre-training)\n",
        "テキストと画像の両方から埋め込みベクトルをつくるモデル\n",
        "\n",
        "- **拡散モデル**\n",
        "ノイズ画像から徐々に画像を除去していくことによる画像生成モデル\n",
        "\n",
        "- **guided-diffusion**\n",
        "拡散モデルにおいてノイズを除去する際、CLIP などのガイダンスを用いられるようにしたモデル\n",
        "\n",
        "#### 本ノートブックの出典\n",
        "\n",
        "https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj\n",
        "\n",
        "Many thanks to the original author!\n",
        "\n",
        "> By Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). It uses OpenAI's 256x256 unconditional ImageNet diffusion model (https://github.com/openai/guided-diffusion) together with CLIP (https://github.com/openai/CLIP) to connect text prompts with images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIqUfrmvLIhg",
        "cellView": "form"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Katherine Crowson\n",
        "# Copyright (c) 2024 Hirohisa Oda\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CLIP**（Contrastive Language–Image Pre-training）\n",
        "\n",
        "CLIP は OpenAI が開発したモデルで、テキストと画像を同じ空間に「埋め込みベクトル」としてマッピングできます。\n",
        "\n",
        "- テキストと画像の類似度を数値化。\n",
        "- テキストプロンプトを用いて画像生成や検索が可能。\n",
        "\n",
        "<a title=\"OpenAI, MIT &lt;http://opensource.org/licenses/mit-license.php&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Contrastive_Language-Image_Pretraining.png\"><img width=\"512\" alt=\"Contrastive Language-Image Pretraining\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/Contrastive_Language-Image_Pretraining.png/512px-Contrastive_Language-Image_Pretraining.png?20240906194850\"></a>\n",
        "\n",
        "#### 埋め込みベクトル\n",
        "\n",
        "埋め込みベクトルは、テキストや画像を数学的な表現（数値のリスト）に変換したものです。この表現により、似ているもの同士は「近い位置」に配置されます。例えば：\n",
        "\n",
        "- **「日本」＋「首都」＝「東京」** という類推が、数値ベースで可能になります。\n",
        "- 画像の場合、富士山の写真が「日本の象徴」という特徴を捉えれば、関連するテキスト（例：「日本」や「富士山」）のベクトルと近くなります。\n",
        "\n",
        "#### テキストと画像の共通空間\n",
        "\n",
        "CLIPは、テキストと画像の埋め込みベクトルを同じ空間に配置します。その結果、たとえ入力がテキストでも画像でも、同じ概念を持つものは近い位置に配置されます。例えば，「犬」というテキストと、犬の写真は非常に似たベクトルになります。"
      ],
      "metadata": {
        "id": "YdPQNSFELV9I"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_UVMZCIAq_r"
      },
      "source": [
        "# Check CUDA\n",
        "import torch\n",
        "assert torch.cuda.is_available()\n",
        "\n",
        "# Install dependencies\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/crowsonkb/guided-diffusion\n",
        "!pip install -e ./CLIP\n",
        "!pip install -e ./guided-diffusion\n",
        "!pip install lpips\n",
        "\n",
        "# Download the diffusion model\n",
        "!curl -OL 'https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmbrcrhpBPC6"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./guided-diffusion')\n",
        "\n",
        "import clip\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファイルをダウンロードするライブラリ\n",
        "import urllib.request\n",
        "\n",
        "# PIL, NumPy, Matplotlibのインポート\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocess_image_from_url(image_url, output_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    指定されたURLから画像をダウンロードし、中心をクロップして縮小する。\n",
        "\n",
        "    Args:\n",
        "        image_url (str): 画像のURL。\n",
        "        output_size (tuple): 出力画像のサイズ（デフォルトは224x224）。\n",
        "\n",
        "    Returns:\n",
        "        PIL.Image: 前処理後の画像。\n",
        "    \"\"\"\n",
        "    # 画像をダウンロードして保存\n",
        "    temp_path = '/content/temp_image.jpg'\n",
        "    urllib.request.urlretrieve(image_url, temp_path)\n",
        "\n",
        "    # 画像を読み込む\n",
        "    img = Image.open(temp_path)\n",
        "\n",
        "    # 画像の幅と高さを取得\n",
        "    width, height = img.size\n",
        "\n",
        "    # 中心をクロップ\n",
        "    crop_size = min(width, height)  # 正方形になるよう最小辺を基準にする\n",
        "    left = (width - crop_size) // 2\n",
        "    top = (height - crop_size) // 2\n",
        "    right = left + crop_size\n",
        "    bottom = top + crop_size\n",
        "    img_cropped = img.crop((left, top, right, bottom))\n",
        "\n",
        "    # 縮小して224x224にリサイズ\n",
        "    img_resized = img_cropped.resize(output_size, Image.BICUBIC)\n",
        "\n",
        "    return img_resized\n",
        "\n",
        "# サンプルのURLを使用して前処理\n",
        "image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/MtFuji_FujiCity.jpg/320px-MtFuji_FujiCity.jpg'\n",
        "image = preprocess_image_from_url(image_url)\n",
        "\n",
        "# 画像の表示\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(np.array(image))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a4ifSRKlWj7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CLIP model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Function to calculate embedding vectors for images\n",
        "def calculate_clip_embedding(image):\n",
        "    \"\"\"\n",
        "    CLIPモデルを使用して画像の埋め込みベクトルを計算する。\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image): 前処理済みのPIL形式の画像。\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: 埋め込みベクトル。\n",
        "    \"\"\"\n",
        "    # Transform the PIL image into a CLIP-compatible tensor\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.ToTensor(),  # Convert to Tensor\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "    ])\n",
        "    image_tensor = preprocess(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Calculate embedding vector using CLIP\n",
        "    with torch.no_grad():\n",
        "        image_embedding = clip_model.encode_image(image_tensor).cpu().numpy()\n",
        "    return image_embedding\n",
        "\n",
        "# Function to calculate embedding vector for a text input\n",
        "def calculate_text_embedding(text):\n",
        "    text_tokens = clip.tokenize([text]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_embedding = clip_model.encode_text(text_tokens).cpu().numpy()\n",
        "    return text_embedding\n",
        "\n",
        "# Compare image and text embeddings\n",
        "def cosine_similarity(image_embedding, text_embedding):\n",
        "    # 確認と変換: 入力を1次元に変換\n",
        "    image_embedding = torch.tensor(image_embedding).squeeze()  # (1, 512) -> (512,)\n",
        "    text_embedding = torch.tensor(text_embedding).squeeze()    # (1, 512) -> (512,)\n",
        "\n",
        "    # コサイン類似度を計算\n",
        "    similarity = torch.nn.functional.cosine_similarity(image_embedding, text_embedding, dim=0)\n",
        "    return similarity.item()"
      ],
      "metadata": {
        "id": "r6SBCxqHSIyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  コサイン類似度\n",
        "\n",
        "コサイン類似度は、2つのベクトルがどれだけ類似しているかをその**方向**（角度）に基づいて測定します。コサイン類似度の数式は次の通りです：\n",
        "\n",
        "$$\n",
        "\\text{cosine_similarity}(\\mathbf{u}, \\mathbf{v}) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}\n",
        "$$\n",
        "\n",
        "ここで：\n",
        "- $\\mathbf{u}$ と $\\mathbf{v}$ は比較する2つのベクトル。\n",
        "- $\\mathbf{u} \\cdot \\mathbf{v}$ は内積（dot product）。\n",
        "- $\\|\\mathbf{u}\\|$ と $\\|\\mathbf{v}\\|$ はそれぞれのベクトルのノルム（長さ）。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. コサイン類似度の値の範囲**\n",
        "コサイン類似度は$[-1,1]の範囲をとります。\n",
        "\n",
        "- 1 のとき：$\\mathbf{u}$ と $\\mathbf{v}$ が完全に同じ方向を向いている（高い類似性）。\n",
        "- 0 のとき： $\\mathbf{u}$ と $\\mathbf{v}$が直交している（全く関連性がない）。\n",
        "- -1 のとき： $\\mathbf{u}$ と$\\mathbf{v}$ が完全に逆方向を向いている（正反対の意味）。\n",
        "\n",
        "---\n",
        "\n",
        "### **3. コサイン類似度が一般的に使用される理由**\n",
        "1. **方向性を重視**:\n",
        "   - コサイン類似度はベクトルの方向（角度）に基づくため、ベクトルの長さ（スケール）の影響を受けません。\n",
        "   - 埋め込み空間では、意味的な関連性が方向に表現されるため、適しています。\n",
        "\n",
        "2. **高次元データに適している**:\n",
        "   - 埋め込みベクトルは通常、高次元空間（例: 512次元）に存在します。コサイン類似度は次元数に依存せず、効率的に計算できます。\n",
        "\n",
        "3. **距離ではなく類似性を測る**:\n",
        "   - ユークリッド距離（L2ノルム）では、スケールや密度の違いが影響を与える場合がありますが、コサイン類似度はこれらの影響を受けにくいです。\n"
      ],
      "metadata": {
        "id": "hz5rL0Z69XPz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 演習１\n",
        "\n",
        "とりあえず現在のプロンプトに対するコサイン類似度を記録しておいてください．その上で，\n",
        "- より画像に近い説明のプロンプトで，コサイン類似度が大きくなることを確認\n",
        "- より画像に近い説明のプロンプトで，コサイン類似度が小さくなることを確認"
      ],
      "metadata": {
        "id": "kKVJI6OuaKh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = \"Anime with a car and two boys\"\n",
        "\n",
        "text_embedding = calculate_text_embedding(text_input)\n",
        "\n",
        "image_embedding = calculate_clip_embedding(image)\n",
        "print(\"Image embegging\")\n",
        "print(image_embedding[0,0:7], \"...\")\n",
        "print()\n",
        "\n",
        "# Calculate difference\n",
        "embedding_difference = cosine_similarity(image_embedding, text_embedding)\n",
        "\n",
        "print(f\"Embedding of text input: {text_input}\")\n",
        "print(text_embedding[0,0:7], \"...\")\n",
        "print()\n",
        "print(f\"Cosine similarity: {embedding_difference}\")\n"
      ],
      "metadata": {
        "id": "f0VVvJDbSU_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **拡散モデル**\n",
        "\n",
        "ノイズを加える「拡散」とノイズを除去する「逆拡散」をもとに、画像を生成するモデルです。\n",
        "\n",
        "#### ノイズの加え方:\n",
        "拡散モデルでは、元の画像に段階的にノイズを加えます。このプロセスは数学的に定義されており、通常、ランダムなガウスノイズを少しずつ足していきます。\n",
        "この「ノイズを加える過程」は事前に決められた固定のルール（拡散プロセス）に基づいています。\n",
        "\n",
        "#### ノイズの除去を学習:\n",
        "学習の目的は「ノイズを取り除いて元の画像を復元する方法」をモデルに教えることです。拡散モデルでは、「ノイズの段階」と「ノイズを取り除いた結果の予測」を繰り返し学習します。\n",
        "\n",
        "<a title=\"Benlisquare, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png\"><img width=\"512\" alt=\"X-Y plot of algorithmically-generated AI art of European-style castle in Japan demonstrating DDIM diffusion steps\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/99/X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png/512px-X-Y_plot_of_algorithmically-generated_AI_art_of_European-style_castle_in_Japan_demonstrating_DDIM_diffusion_steps.png?20221031225518\"></a>\n",
        "\n",
        "### **guided-diffusion**\n",
        "\n",
        "これも OpenAI が開発したモデルで、拡散モデルの基本的な仕組みを利用しつつ、生成画像に「ガイダンス」を与えることができます。「ガイダンス」とは、特定の目標に基づいて生成過程を誘導することを意味します。これにより、単なるランダムな画像生成ではなく、特定の条件（例: テキストプロンプトやラベル）に基づく生成が可能となります。\n",
        "\n",
        "#### CLIP ガイダンス\n",
        "\n",
        "ここでは活用して画像生成を制御します。\n",
        "\n",
        "- 生成中の画像が、指定されたテキストプロンプトにどれだけ一致しているかを CLIP で評価。この評価に基づき、ノイズ除去の方向を調整し、テキストに一致する画像を作り出します。\n",
        "\n",
        "これにより、テキストやラベルといった条件に基づき、画像生成を制御できます。通常の拡散モデルよりも、より意味的な制約を反映した画像生成が可能となります。"
      ],
      "metadata": {
        "id": "6xjj8n-cKiT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 準備\n",
        "\n",
        "関連するコードやモデルをダウンロードし，ライブラリをインポートします．\n",
        "\n",
        "また，処理中で必要となるクラスや関数を定義します．"
      ],
      "metadata": {
        "id": "7S05U51fJeQe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOj78Yvx8jP"
      },
      "source": [
        "# Define necessary functions\n",
        "\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "def range_loss(input):\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpbody2NCR7w"
      },
      "source": [
        "# Model settings\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "model_config.update({\n",
        "    'attention_resolutions': '32, 16, 8',\n",
        "    'class_cond': False,\n",
        "    'diffusion_steps': 1000,\n",
        "    'rescale_timesteps': True,\n",
        "    'timestep_respacing': '1000',  # Modify this value to decrease the number of\n",
        "                                   # timesteps.\n",
        "    'image_size': 256,\n",
        "    'learn_sigma': True,\n",
        "    'noise_schedule': 'linear',\n",
        "    'num_channels': 256,\n",
        "    'num_head_channels': 64,\n",
        "    'num_res_blocks': 2,\n",
        "    'resblock_updown': True,\n",
        "    'use_checkpoint': False,\n",
        "    'use_fp16': True,\n",
        "    'use_scale_shift_norm': True,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnQjGugaDZPJ"
      },
      "source": [
        "# Load models\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "model.load_state_dict(torch.load('256x256_diffusion_uncond.pt', map_location='cpu'))\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "if model_config['use_fp16']:\n",
        "    model.convert_to_fp16()\n",
        "\n",
        "# CLIPのバックボーンモデル（ViT-B/16）をロード\n",
        "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "# 入力解像度（clip_size）も設定\n",
        "clip_size = clip_model.visual.input_resolution\n",
        "\n",
        "# 画像の正規化の設定\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### guided-diffusion での画像生成\n",
        "\n",
        "プロンプトを記入して実行してください。"
      ],
      "metadata": {
        "id": "DTXoMSxpMpwB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PwzFZbLfcy"
      },
      "source": [
        "prompts = ['Expressways and cars in Japan']\n",
        "\n",
        "\n",
        "image_prompts = []\n",
        "batch_size = 1\n",
        "clip_guidance_scale = 1000  # Controls how much the image should look like the prompt.\n",
        "tv_scale = 150              # Controls the smoothness of the final output.\n",
        "range_scale = 50            # Controls how far out of range RGB values are allowed to be.\n",
        "cutn = 16\n",
        "n_batches = 1\n",
        "init_image = None   # This can be an URL or Colab local path and must be in quotes.\n",
        "skip_timesteps = 0  # This needs to be between approx. 200 and 500 when using an init image.\n",
        "                    # Higher values make the output look more like the init.\n",
        "init_scale = 0      # This enhances the effect of the init image, a good value is 1000.\n",
        "seed = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5gODNAMEUCR"
      },
      "source": [
        "def do_run():\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    make_cutouts = MakeCutouts(clip_size, cutn)\n",
        "    side_x = side_y = model_config['image_size']\n",
        "\n",
        "    target_embeds, weights = [], []\n",
        "\n",
        "    for prompt in prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        target_embeds.append(clip_model.encode_text(clip.tokenize(txt).to(device)).float())\n",
        "        weights.append(weight)\n",
        "\n",
        "    for prompt in image_prompts:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), transforms.InterpolationMode.LANCZOS)\n",
        "        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "        embed = clip_model.encode_image(normalize(batch)).float()\n",
        "        target_embeds.append(embed)\n",
        "        weights.extend([weight / cutn] * cutn)\n",
        "\n",
        "    target_embeds = torch.cat(target_embeds)\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        "\n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
        "        init = TF.to_tensor(init).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "\n",
        "    cur_t = None\n",
        "\n",
        "    def cond_fn(x, t, out, y=None):\n",
        "        n = x.shape[0]\n",
        "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "        clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
        "        image_embeds = clip_model.encode_image(clip_in).float()\n",
        "        dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
        "        dists = dists.view([cutn, n, -1])\n",
        "        losses = dists.mul(weights).sum(2).mean(0)\n",
        "        tv_losses = tv_loss(x_in)\n",
        "        range_losses = range_loss(out['pred_xstart'])\n",
        "        loss = losses.sum() * clip_guidance_scale + tv_losses.sum() * tv_scale + range_losses.sum() * range_scale\n",
        "        if init is not None and init_scale:\n",
        "            init_losses = lpips_model(x_in, init)\n",
        "            loss = loss + init_losses.sum() * init_scale\n",
        "        return -torch.autograd.grad(loss, x)[0]\n",
        "\n",
        "    if model_config['timestep_respacing'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        samples = sample_fn(\n",
        "            model,\n",
        "            (batch_size, 3, side_y, side_x),\n",
        "            clip_denoised=False,\n",
        "            model_kwargs={},\n",
        "            cond_fn=cond_fn,\n",
        "            progress=True,\n",
        "            skip_timesteps=skip_timesteps,\n",
        "            init_image=init,\n",
        "            randomize_class=True,\n",
        "            cond_fn_with_grad=True,\n",
        "        )\n",
        "\n",
        "        for j, sample in enumerate(samples):\n",
        "            cur_t -= 1\n",
        "            if j % 100 == 0 or cur_t == -1:\n",
        "                print()\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    filename = f'progress_{i * batch_size + k:05}.png'\n",
        "                    TF.to_pil_image(image.add(1).div(2).clamp(0, 1)).save(filename)\n",
        "                    tqdm.write(f'Batch {i}, step {j}, output {k}:')\n",
        "                    display.display(display.Image(filename))\n",
        "\n",
        "gc.collect()\n",
        "do_run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
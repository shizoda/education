{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuuVcD7/rtzpYSrTlnDUXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/vae/vae-mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VAE による画像生成\n",
        "\n",
        "VAE（Variational Autoencoder、変分オートエンコーダ）は、データを「圧縮」し、そしてその圧縮されたデータから元のデータに似たものを「生成」できるように設計されたモデルです。大きな画像を少ない次元で表現できるのです。\n",
        "\n",
        "### 1. データの圧縮（低次元空間での表現）\n",
        "\n",
        "VAEでは、元のデータ（例えば28x28ピクセルの画像）を低次元空間と呼ばれる小さな数値の集合で表現します。この低次元空間は、「データの本質的な特徴」だけを保持したシンプルな表現です。例えば、「手書き数字の形」を捉えた2次元や3次元の点として表現することができます。\n",
        "\n",
        "今回の実験では、手書き数字画像データセット「MNIST」を使います。このデータセットには、0から9までの手書き数字（28x28ピクセル）の画像が含まれています。VAEを用いてこれらの画像を圧縮し、28×28の784次元から、たった数次元の潜在空間に情報を収める仕組みを確認します。\n",
        "\n",
        "### 2. データの生成（解凍）\n",
        "\n",
        "圧縮されたデータ（潜在変数）をもとに、元のようなデータを再び作り出すことを「生成」と呼びます。VAEは、この潜在変数から、新しいデータを生成できる点が特徴です。潜在変数は、「手書き数字」を表現する低次元空間の中の点です。この空間のどこかに新しい点を1つ取れば、それを元にして新しい手書き数字を作り出すことができます。\n",
        "\n",
        "MNISTを用いた今回の実験では、潜在変数の空間からランダムに点を選び、そこから生成される手書き数字を観察します。また、潜在空間を2次元に設定しているので、数字ごとの分布を視覚化し、圧縮された特徴がどのように学習されているかを確認できます。\n",
        "\n",
        "## 演習\n",
        "\n",
        "それでは、ライブラリのインストールから進めていきましょう。\n",
        "事前に GPU ランタイムを設定してください。"
      ],
      "metadata": {
        "id": "MevM5Agslakj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9fIGhdJQBYx"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインストール\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# デバイスの設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# MNISTデータセットのロード\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MNISTデータセットを観察する\n",
        "\n",
        "まず、手書き数字データセット「MNIST」に含まれる画像を観察してみます。MNISTは、0から9までの手書き数字が28x28ピクセルのグレースケール画像として収められたデータセットです。\n",
        "\n",
        "とりあえず、それぞれの数字（0～9）を1枚ずつ取得し、横に並べて表示してみます。"
      ],
      "metadata": {
        "id": "iG-xGUdunRDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数字ごとの画像を1枚ずつ取得\n",
        "digit_images = [None] * 10  # 0から9までの数字用\n",
        "for images, labels in train_loader:\n",
        "    for i in range(len(labels)):\n",
        "        digit = labels[i].item()\n",
        "        if digit_images[digit] is None:\n",
        "            digit_images[digit] = images[i].squeeze(0)  # 画像データを保存\n",
        "        if all(img is not None for img in digit_images):  # 全ての数字の画像が揃ったら終了\n",
        "            break\n",
        "    if all(img is not None for img in digit_images):  # 全ての数字の画像が揃ったら終了\n",
        "        break\n",
        "\n",
        "# 横に並べて表示\n",
        "fig, ax = plt.subplots(1, 10, figsize=(15, 3))\n",
        "for i in range(10):\n",
        "    ax[i].imshow(digit_images[i], cmap='gray')\n",
        "    ax[i].axis('off')\n",
        "    ax[i].set_title(str(i))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CtS4H4YpEBzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ネットワーク構造\n",
        "\n",
        "VAE（Variational Autoencoder）を構築するためのPyTorchモデル ConvVAE を示します。以下のような2つの主要な部分で構成されています。\n",
        "\n",
        "- **エンコーダ（Encoder）**\n",
        "\n",
        "    入力画像  (1,28,28)  を畳み込み層で処理し、低次元の潜在空間（latent space）に変換します。この変換は、潜在変数 z を求めるときに平均 (μ) と分散 (log⁡σ2) を用います。\n",
        "\n",
        "- **デコーダ（Decoder）**\n",
        "\n",
        "    潜在変数 z をもとに、転置畳み込み（逆畳み込み）層を使用して、潜在空間から画像の形状を再構築します。\n",
        "\n",
        "#### ネットワークの主要なステップ\n",
        "\n",
        "\n",
        "- Decoder\n",
        "\n",
        "    潜在変数 z を全結合層と転置畳み込み層を用いて、元の画像サイズに戻します。"
      ],
      "metadata": {
        "id": "GAJFHDoTnq_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvVAE, self).__init__()\n",
        "        # Encoder部分の定義\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  # 畳み込み層1: 入力 (B, 1, 28, 28) -> 出力 (B, 32, 14, 14)\n",
        "            nn.ReLU(),  # 活性化関数: 非線形性を導入\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 畳み込み層2: (B, 32, 14, 14) -> (B, 64, 7, 7)\n",
        "            nn.ReLU(),  # 活性化関数\n",
        "            nn.Flatten()  # Flatten: 畳み込みの出力を1次元ベクトルに変換 (B, 64*7*7)\n",
        "        )\n",
        "        # 潜在変数の平均 (mu) を計算する全結合層\n",
        "        self.fc_mu = nn.Linear(64 * 7 * 7, 2)  # 平均: 出力次元は潜在変数の次元 (ここでは2)\n",
        "        # 潜在変数の対数分散 (logvar) を計算する全結合層\n",
        "        self.fc_logvar = nn.Linear(64 * 7 * 7, 2)  # 対数分散: 出力次元は潜在変数の次元 (ここでは2)\n",
        "\n",
        "        # Decoder部分の定義\n",
        "        # 潜在変数zを全結合層で元のFlattenされた形式に戻す\n",
        "        self.fc_z = nn.Linear(2, 64 * 7 * 7)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Unflatten(1, (64, 7, 7)),  # 1次元ベクトルを元の形状に戻す (B, 64*7*7) -> (B, 64, 7, 7)\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),  # 転置畳み込み層1: (B, 64, 7, 7) -> (B, 32, 14, 14)\n",
        "            nn.ReLU(),  # 活性化関数\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),  # 転置畳み込み層2: (B, 32, 14, 14) -> (B, 1, 28, 28)\n",
        "            nn.Sigmoid()  # 出力を0~1の範囲に正規化\n",
        "        )\n",
        "\n",
        "    # エンコーダ部分: 入力画像を潜在変数の平均と分散に変換\n",
        "    def encode(self, x):\n",
        "        h1 = self.encoder(x)  # 畳み込みとFlattenで特徴を抽出\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)  # 平均と分散を計算\n",
        "\n",
        "    # 再パラメータ化トリック: 潜在変数zを生成\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)  # 分散を標準偏差に変換\n",
        "        eps = torch.randn_like(std)  # 正規分布から乱数を生成\n",
        "        return mu + eps * std  # 潜在変数z = 平均 + 標準偏差 * 乱数\n",
        "\n",
        "    # デコーダ部分: 潜在変数zから元の画像を再構築\n",
        "    def decode(self, z):\n",
        "        h3 = self.fc_z(z)  # 潜在変数を全結合層でFlattenの形状に戻す\n",
        "        return self.decoder(h3)  # 転置畳み込みを用いて元の画像サイズに変換\n",
        "\n",
        "    # 順伝播: 入力画像を潜在変数zを通して再構築\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)  # エンコーダで平均と分散を計算\n",
        "        z = self.reparameterize(mu, logvar)  # 再パラメータ化トリックで潜在変数を生成\n",
        "        return self.decode(z), mu, logvar  # 再構築画像、平均、分散を返す\n"
      ],
      "metadata": {
        "id": "_6qh9Ze_n4Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **損失関数**\n",
        "VAEの損失関数は、次の2つの項から構成されています：\n",
        "\n",
        "1. **再構築誤差（Binary Cross Entropy, BCE）**  \n",
        "   元の入力画像とモデルが再構築した画像のピクセル単位の誤差を表します。この誤差を小さくすることで、モデルが元の画像に近いデータを生成する能力を向上させます。\n",
        "\n",
        "   - 各ピクセルの確率分布（0～1の値）に基づき、元の画像と再構築画像がどれだけ一致しているかを計算します。\n",
        "   - 数式で表すと次のようになります：  \n",
        "     \\[\n",
        "     BCE = - \\sum \\left( x \\cdot \\log(\\hat{x}) + (1 - x) \\cdot \\log(1 - \\hat{x}) \\right)\n",
        "     \\]\n",
        "\n",
        "2. **KLダイバージェンス（Kullback-Leibler Divergence, KLD）**  \n",
        "   エンコーダによって学習された潜在分布（\\( q(z|x) \\)）が、標準正規分布（\\( p(z) \\)）にどれだけ近いかを測ります。これにより、潜在空間を正規化し、生成されたデータの多様性が向上します。\n",
        "\n",
        "   - 数式で表すと次のようになります：  \n",
        "     \\[\n",
        "     KLD = - \\frac{1}{2} \\sum \\left( 1 + \\log(\\sigma^2) - \\mu^2 - \\sigma^2 \\right)\n",
        "     \\]\n",
        "\n",
        "### **最終的な損失**\n",
        "損失は、再構築誤差（BCE）とKLダイバージェンス（KLD）の合計で計算されます：\n",
        "\\[\n",
        "\\text{Loss} = \\text{BCE} + \\text{KLD}\n",
        "\\]\n",
        "これにより、モデルは元のデータを正確に再構築しつつ、潜在空間の構造を規則化します。\n"
      ],
      "metadata": {
        "id": "onSPda2CppAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失関数\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return BCE + KLD\n"
      ],
      "metadata": {
        "id": "yx4gBO1IDyTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **学習ループ**\n",
        "学習ループでは、以下のプロセスを繰り返します：\n",
        "\n",
        "1. **ミニバッチごとにデータを取得**  \n",
        "   データセットを分割して少量のデータ（ミニバッチ）を順に取り出します。これにより、大規模なデータセットを効率的に学習できます。\n",
        "\n",
        "2. **データをデバイスに転送**  \n",
        "   モデルと同じデバイス（GPU）にデータを移動します。\n",
        "\n",
        "3. **モデルの出力を計算**  \n",
        "   入力データをモデルに通し、再構築画像（recon_batch）、潜在変数の平均（\\(\\mu\\)）、分散の対数（\\(\\log \\sigma^2\\)）を取得します。\n",
        "\n",
        "4. **損失関数を計算**  \n",
        "   再構築誤差（BCE）とKLダイバージェンス（KLD）を用いて損失を計算します。\n",
        "\n",
        "5. **勾配を計算し、モデルを更新**  \n",
        "   損失に基づいて勾配を計算（逆伝播）し、オプティマイザを使ってモデルのパラメータを更新します。\n",
        "\n",
        "6. **損失を記録**  \n",
        "   各エポックでの平均損失を計算し、学習の進捗をモニタリングします。\n"
      ],
      "metadata": {
        "id": "CX6aQVBJptcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの初期化\n",
        "model = ConvVAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 学習ループ\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {train_loss / len(train_loader.dataset):.4f}\")\n"
      ],
      "metadata": {
        "id": "-jmd96FFpszK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 潜在変数から画像を生成\n",
        "\n",
        "この関数 `generate_image_from_latent` を使って、任意の潜在変数から画像を生成し、その結果を可視化してみましょう。任意の潜在変数ベクトル（例えば \\([-0.6, 1.2]\\)）をデコーダに入力し、そこから生成された画像を観察できます。\n",
        "\n",
        "1. **潜在空間に任意の点を指定**  \n",
        "   潜在空間の中から1つの点（ベクトル）を選びます。このベクトルは、潜在変数の値を指定するためのもので、-3 から 3 くらいまでの範囲で自由に決めることができます。\n",
        "\n",
        "2. **選んだ点から画像を生成**  \n",
        "   デコーダを使い、この点が表現する画像を生成します。潜在空間の中の点がどのようなデータ（例えば手書き数字）を表現しているかを観察できます。\n",
        "\n",
        "潜在変数の値を変更することで、生成される画像がどのように変化するかを確認できます。これにより、潜在空間の各次元がデータにどのような意味を持つのかを視覚的に理解することができます。"
      ],
      "metadata": {
        "id": "IoUPkkPPrYEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image_from_latent(latent_vector, model, device):\n",
        "    \"\"\"\n",
        "    潜在変数ベクトルから画像を生成して表示する関数\n",
        "\n",
        "    Args:\n",
        "        latent_vector (list or np.ndarray): 潜在変数のリストまたは配列 [z0, z1]\n",
        "        model (torch.nn.Module): 学習済みのVAEモデル\n",
        "        device (torch.device): モデルのデバイス（CPU or GPU）\n",
        "    \"\"\"\n",
        "    model.eval()  # 評価モードに設定\n",
        "    with torch.no_grad():\n",
        "        # 潜在変数をテンソルに変換し、デバイスに送る\n",
        "        z = torch.tensor([latent_vector], device=device, dtype=torch.float32)\n",
        "        # 潜在変数から生成された画像を取得\n",
        "        generated_image = model.decode(z).cpu().numpy().reshape(28, 28)\n",
        "\n",
        "    # 画像を表示\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(generated_image, cmap=\"gray\")\n",
        "    plt.title(f\"Latent Vector: {latent_vector}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# 使用例\n",
        "generate_image_from_latent([-0.6, 1.2], model, device)\n"
      ],
      "metadata": {
        "id": "h8-Vu8YJTl-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **潜在空間の可視化**\n",
        "\n",
        "VAEの特徴のひとつは、潜在空間におけるデータの分布を学習し、その空間から新しいデータを生成できることです。このステップでは、学習された潜在空間を2次元の格子点として可視化します。\n",
        "\n",
        "具体的には：\n",
        "1. 潜在空間の範囲を \\([-3, 3]\\) とし、この範囲内の格子点を生成します。\n",
        "2. 各格子点を潜在変数 \\( z \\) としてデコーダに入力し、生成された画像を可視化します。\n",
        "3. この可視化により、潜在空間内の異なる位置がどのようなデータ（画像）を表現しているかを直感的に理解できます。\n",
        "\n",
        "図の横軸と縦軸は、それぞれ潜在空間の \\( z \\) の第1次元（横軸）と第2次元（縦軸）それぞれ -3 から 3 までの結果を示します。"
      ],
      "metadata": {
        "id": "gytNq-oFqAUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 潜在空間を可視化\n",
        "model.eval()\n",
        "x_vals = np.arange(-3, 3, 0.3)  # 潜在空間の横軸（第1次元）の範囲と刻み幅\n",
        "y_vals = np.arange(-3, 3, 0.3)  # 潜在空間の縦軸（第2次元）の範囲と刻み幅\n",
        "\n",
        "fig, ax = plt.subplots(len(y_vals), len(x_vals), figsize=(10, 10))  # 画像全体のサイズを調整\n",
        "\n",
        "# 潜在空間上の格子点をループして画像を生成\n",
        "for i, yi in enumerate(y_vals):\n",
        "    for j, xi in enumerate(x_vals):\n",
        "        z = torch.tensor([[xi, yi]], device=device, dtype=torch.float32)  # 格子点を潜在変数zとして設定\n",
        "\n",
        "        with torch.no_grad():  # 勾配を計算せずに推論モードでデコーダを実行\n",
        "            sample = model.decode(z).cpu().numpy()  # 生成された画像をNumPy形式に変換\n",
        "        ax[i, j].imshow(sample.reshape(28, 28), cmap=\"gray\")  # 画像をグリッドに表示\n",
        "        ax[i, j].axis(\"off\")  # 軸目盛りを非表示にする\n",
        "\n",
        "# 軸と数値ラベルの設定\n",
        "for i, yi in enumerate(y_vals):\n",
        "    ax[i, 0].set_ylabel(f\"{yi:.1f}\", fontsize=6, labelpad=2)  # 縦軸に数値ラベルを追加\n",
        "for j, xi in enumerate(x_vals):\n",
        "    ax[-1, j].set_xlabel(f\"{xi:.1f}\", fontsize=6, labelpad=2)  # 横軸に数値ラベルを追加\n",
        "\n",
        "# グラフ全体のラベルを追加\n",
        "fig.text(0.5, 0.01, \"Latent Space Dimension 1 (z1)\", ha=\"center\", fontsize=12)  # 横軸ラベル\n",
        "fig.text(0.01, 0.5, \"Latent Space Dimension 2 (z2)\", va=\"center\", rotation=\"vertical\", fontsize=12)  # 縦軸ラベル\n",
        "\n",
        "plt.subplots_adjust(wspace=0.05, hspace=0.05)  # プロット間の余白を最小化\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tJ-yBeUzRbG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
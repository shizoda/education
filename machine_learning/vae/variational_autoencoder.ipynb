{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIgH6XgzAK0jnFaYWRktKe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/vae/variational_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoder (VAE)\n",
        "\n",
        "基本の Autoencoder (AE) が分かったところで、VAE について見ていきましょう。"
      ],
      "metadata": {
        "id": "-cAgXJGlQBWL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b7ZX_-YFPoio"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの変換処理\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# CIFAR-10のトレーニングデータセット\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# CIFAR-10のテストデータセット\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrAnXwo2QA2c",
        "outputId": "f71933ab-9763-4856-9b7f-24a327573e83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### エンコーダーとデコーダー\n",
        "\n",
        "AEでは、エンコーダーは入力を直接低次元の潜在空間にマッピングし、デコーダーはその潜在表現を使って入力データを再構成します。\n",
        "\n",
        "それに対してVAEの場合、エンコーダーは入力データから潜在空間の確率分布のパラメータ（平均と分散）を推定し、デコーダーは潜在空間からサンプリングされた値を使って入力データを再構成します。これにより、データ生成プロセスに確率論的な要素が導入されます。"
      ],
      "metadata": {
        "id": "ArFHNf7nQpdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VAEの定義\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        # エンコーダー\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)  # muを出力\n",
        "        self.fc22 = nn.Linear(400, 20)  # logvarを出力\n",
        "\n",
        "        # デコーダー\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 32 * 8 * 8)\n",
        "        self.convtranspose1 = nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1)\n",
        "        self.convtranspose2 = nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1)\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc21(x), self.fc22(x)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = F.relu(self.fc3(z))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = x.view(-1, 32, 8, 8)\n",
        "        x = F.relu(self.convtranspose1(x))\n",
        "        x = torch.sigmoid(self.convtranspose2(x))\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ],
      "metadata": {
        "id": "Gqi95WFQQykb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 損失関数\n",
        "\n",
        "AEの損失関数は通常、再構成誤差（例：MSE）のみを考慮します。\n",
        "\n",
        "それに対してVAEでは、再構成誤差とKLダイバージェンス（エンコーダーによって推定された潜在変数の分布と事前分布との間の差異を表す）の和を損失関数として使用します。"
      ],
      "metadata": {
        "id": "tsPva9bBQ-iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失関数\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return MSE + KLD"
      ],
      "metadata": {
        "id": "SNzATOtWQ6EG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルのインスタンス化、最適化手法の設定\n",
        "model = VAE()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 訓練ループ\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(trainloader.dataset)} ({100. * batch_idx / len(trainloader):.0f}%)]\\tLoss: {loss.item() / len(data):.6f}')\n",
        "\n",
        "    print(f'====> Epoch: {epoch} Average loss: {train_loss / len(trainloader.dataset):.4f}')\n",
        "\n",
        "# 訓練の実行\n",
        "num_epochs = 10\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtYjfH-ORD78",
        "outputId": "edab8935-e33e-4753-d75b-586e94466d77"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1692.541992\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 756.455444\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 670.369263\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 602.703430\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 657.566284\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 619.935974\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 601.623047\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 604.410828\n",
            "====> Epoch: 1 Average loss: 658.8315\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 623.811890\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 641.154358\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 594.066589\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 621.073792\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 634.598022\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 575.671204\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 557.222290\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 542.423279\n",
            "====> Epoch: 2 Average loss: 567.9112\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 510.234924\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 544.700745\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 601.004944\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 538.581116\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 579.690063\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 596.769836\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 511.469849\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 532.674316\n",
            "====> Epoch: 3 Average loss: 554.4059\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 568.288818\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 520.374084\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 514.587463\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 447.851318\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 587.659790\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 533.596130\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 608.796448\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 560.996643\n",
            "====> Epoch: 4 Average loss: 549.8287\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 582.505737\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 574.631592\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 578.951965\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 520.504883\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 647.593689\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 532.987793\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 518.619019\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 547.115784\n",
            "====> Epoch: 5 Average loss: 547.1178\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 523.372742\n",
            "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 554.279907\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 596.914124\n",
            "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 590.013245\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 608.704163\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 615.686707\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 523.266113\n",
            "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 556.229797\n",
            "====> Epoch: 6 Average loss: 545.4278\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 558.452271\n",
            "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 502.432556\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 486.824371\n",
            "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 538.052002\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 553.545410\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 593.273987\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 561.587158\n",
            "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 580.591980\n",
            "====> Epoch: 7 Average loss: 544.0600\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 478.139954\n",
            "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 554.517639\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 577.846069\n",
            "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 572.444153\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 473.226532\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 461.073578\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 518.349121\n",
            "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 561.987549\n",
            "====> Epoch: 8 Average loss: 543.1740\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 561.256287\n",
            "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 576.009766\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 532.323853\n",
            "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 577.204834\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 550.431335\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 513.808655\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 558.063843\n",
            "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 514.203491\n",
            "====> Epoch: 9 Average loss: 542.3724\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 601.274963\n",
            "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 579.040527\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 492.204468\n",
            "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 459.116028\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 585.288818\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 517.810303\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 561.942444\n",
            "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 498.533691\n",
            "====> Epoch: 10 Average loss: 541.4784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練が終わったら、いくつかの画像でどのように動作するかを確認\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "output = model(images.cuda())  # CUDAを使用している場合\n",
        "images = images.numpy()\n",
        "output = output.detach().cpu().numpy()\n",
        "\n",
        "# 元の画像と再構成された画像を表示\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True, figsize=(12,5))\n",
        "for images, row in zip([images, output], axes):\n",
        "    for img, ax in zip(images, row):\n",
        "        ax.imshow(np.transpose(img, (1, 2, 0)))\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "prBN1nWwRHqv",
        "outputId": "b57cef3e-3013-468a-e68b-df58fdaa74ee"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d09acd0e877e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# CUDAを使用している場合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    }
  ]
}
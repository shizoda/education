{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzmeW+7+T/dlhAdM7MctNx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 最適化器（オプティマイザ）\n",
        "\n",
        "機械学習を行うときに **初期学習率 (learning rate)** を設定するのが重要であることは、すでに理解されているものと思います。これによって学習の速度が変わり、大きすぎると過学習が起きるし、小さすぎると学習の進みが遅くなりますね。\n",
        "\n",
        "ただ、学習が進むにつれて学習率は徐々に調節すべきです。最初の段階では、モデルがどのようにデータを学べばよいのか全く分からないため、大きな学習率で大まかな方向性を探ります。しかし、学習が進むにつれて、モデルが正しい方向に近づいていきます。この段階では、学習率を小さくして慎重に調整する必要があります。大きな学習率のままだと、良い解に近づいているのに、再び遠ざかってしまう可能性があります。"
      ],
      "metadata": {
        "id": "7C3n7lHYPzbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### そもそも最小化とは\n",
        "\n",
        "機械学習の目的は、モデルが誤差（損失関数）を最小限に抑えることです。損失関数とは、モデルがどれだけ正しい予測をしているかを数値で示したものです。例えば、予測と実際の値が大きく離れているほど損失関数の値は大きくなり、逆に一致しているほど小さくなります。\n",
        "\n",
        "この損失関数を最小化するためには、数学的には関数の最小値を求める「最適化問題」を解く必要があります。ここで着目すべきなのが **勾配（関数の傾き）**です。\n",
        "\n",
        "### 最小化の手順\n",
        "\n",
        "基本的な **勾配降下法 (Gradient Descent)** という方法で、関数が最小となるパラメータを探してみましょう。\n",
        "\n",
        "- 勾配の計算\n",
        "\n",
        "損失関数を入力パラメータについて微分して、**現時点からどの方向に進めば関数の値が減るか** を調べます。\n",
        "\n",
        "- パラメータの更新\n",
        "\n",
        "勾配が示す方向とは逆の方向に少しだけ進むことで、損失を少しずつ減らします。この際、更新の幅を決めるのが **学習率（learning rate） ** です。\n",
        "\n",
        "- 繰り返し\n",
        "\n",
        "この操作を何度も繰り返して、損失が最小になるパラメータを探します。"
      ],
      "metadata": {
        "id": "BZ2DfRNnQ2Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 初期値と学習率\n",
        "x = 0  # 初期点\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 最大ステップ数\n",
        "steps = 20\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "# 損失関数とその勾配を定義\n",
        "def loss_function(x):\n",
        "    return x**4 - 3*x**3 - 7*x**2 + 5*x  # 損失関数\n",
        "\n",
        "def gradient(x):\n",
        "    return 4*x**3 - 9*x**2 - 8*x + 5  # 損失関数の微分\n",
        "\n",
        "# 記録用リスト\n",
        "path = [x]\n",
        "\n",
        "# 最適化\n",
        "for _ in trange(steps):\n",
        "    grad = gradient(x)  # 勾配の計算\n",
        "    x -= learning_rate * grad  # パラメータの更新\n",
        "    path.append(x)\n",
        "\n",
        "# 可視化\n",
        "x_vals = np.linspace(-3, 5, 500)\n",
        "y_vals = loss_function(x_vals)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x_vals, y_vals, label=\"Loss Function: $y = x^4 - 3x^3 - 4x^2 + 5x$\", color=\"blue\")\n",
        "plt.scatter(path, [loss_function(x) for x in path], color=\"red\", label=\"SGD Steps\")\n",
        "plt.title(\"Gradient Descent on $y = x^4 - 3x^3 - 4x^2 + 5x$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.axhline(0, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
        "plt.axvline(0, color=\"black\", linestyle=\"--\", linewidth=0.5)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BaNME8idQIM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 演習１\n",
        "\n",
        "初期値や学習率を調節して、大域解に到達するようにしてください。\n",
        "\n",
        "また、なるべく高速に（少ない回数で）到達できるようにしてください。初期値が適切でも、あまりにも学習率を上げすぎるとうまくいかないことも確認してください。\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "PkxQ-UjtbKJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 確率的勾配降下法（SGD）\n",
        "\n",
        "最小化を効率よく行うために用いられるのが **確率的勾配降下法（Stochastic Gradient Descent, SGD）** です。上記の勾配降下法に加えてミニバッチを使うため、そのため、毎回の最適化には不確実性があります。"
      ],
      "metadata": {
        "id": "XfwW9uW9TRYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 演習２\n",
        "\n",
        "SGD を用いて iris の分類問題を実行し、どのようなスピードで学習率が低下していくのか確認してみましょう。"
      ],
      "metadata": {
        "id": "-1R_EWvEXTKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Iris データセットの読み込み\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Virginica とそれ以外の2クラスに限定\n",
        "y = (y == 2).astype(int)  # Virginica: 1, それ以外: 0\n",
        "\n",
        "# 特徴量を2次元に限定（Sepal Length, Sepal Width）\n",
        "X = X[:, :2]\n",
        "\n",
        "# データの標準化\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# データをトレーニングセットとテストセットに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# サンプル点の可視化\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], c=\"green\", marker=\"o\", label=\"Train - Not Versicolor\")\n",
        "plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], c=\"blue\", marker=\"o\", label=\"Train - Versicolor\")\n",
        "plt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], c=\"green\", marker=\"x\", label=\"Test - Not Versicolor\")\n",
        "plt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], c=\"blue\", marker=\"x\", label=\"Test - Versicolor\")\n",
        "plt.title(\"Train/Test Split Visualization (Versicolor vs Others)\")\n",
        "plt.xlabel(\"Feature 1 (Standardized)\")\n",
        "plt.ylabel(\"Feature 2 (Standardized)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kYq9M_NPTsHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from logging import critical\n",
        "# オプティマイザのリスト\n",
        "optimizers = {\n",
        "    \"SGD\": lambda: optim.SGD(model.parameters(), lr=0.1),\n",
        "    # 他にも 「\"名前\": lambda: 呼び出し文」 という書き方で追加してください\n",
        "}\n",
        "\n",
        "\n",
        "# シンプルなMLPモデル\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 3),  # 特徴量数: 2, 出力: 3（3クラス分類）\n",
        "    nn.Softmax(dim=1)  # 出力を確率分布に変換\n",
        ")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失を定義"
      ],
      "metadata": {
        "id": "QoqzFyDbV04u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# トレーニング関数\n",
        "def train(optimizer_name):\n",
        "    optimizer = optimizers[optimizer_name]()  # オプティマイザを作成\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.9)  # 学習率スケジューラ\n",
        "    losses = []  # ロスを記録するリスト\n",
        "    lrs = []     # 学習率を記録するリスト\n",
        "\n",
        "    for epoch in range(300):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_train_tensor)\n",
        "        loss = criterion(y_pred, y_train_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # 学習率を更新\n",
        "\n",
        "        # 各エポックでのロスと学習率を記録\n",
        "        losses.append(loss.item())\n",
        "        lrs.append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    return losses, lrs  # ロスと学習率のリストを返す\n",
        "\n",
        "# トレーニングと可視化\n",
        "for optimizer_name in optimizers.keys():\n",
        "    model.apply(lambda m: m.reset_parameters() if hasattr(m, \"reset_parameters\") else None)  # 重み初期化\n",
        "    losses, lrs = train(optimizer_name)  # 修正後の train 関数\n",
        "\n",
        "    # 横並びのグラフ作成\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # 学習曲線と学習率の変化を左側にプロット\n",
        "    ax1 = axes[0]\n",
        "    ax2 = ax1.twinx()\n",
        "    ax1.plot(losses, label=\"Loss\", color=\"blue\")\n",
        "    ax1.set_xlabel(\"Epoch\")\n",
        "    ax1.set_ylabel(\"Loss\", color=\"blue\")\n",
        "    ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
        "    ax2.plot(lrs, label=\"Learning Rate\", color=\"red\")\n",
        "    ax2.set_ylabel(\"Learning Rate\", color=\"red\")\n",
        "    ax2.tick_params(axis='y', labelcolor=\"red\")\n",
        "    ax1.set_title(f\"Loss and Learning Rate ({optimizer_name})\")\n",
        "\n",
        "    # 決定境界を右側にプロット\n",
        "    ax3 = axes[1]\n",
        "    xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
        "    grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
        "    preds = model(grid).detach().numpy().reshape(xx.shape)\n",
        "    ax3.contourf(xx, yy, preds, levels=50, cmap=\"RdBu\", alpha=0.3)\n",
        "    ax3.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], c=\"red\", marker=\"o\", label=\"Not Virginica\")\n",
        "    ax3.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], c=\"blue\", marker=\"s\", label=\"Virginica\")\n",
        "    ax3.set_title(f\"Decision Boundary ({optimizer_name})\")\n",
        "    ax3.legend()\n",
        "\n",
        "    # レイアウト調整して表示\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "02EvQsaXVOmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 演習３\n",
        "\n",
        "SGD 以外にも、いくつか有名なオプティマイザがあります。optimizers に追加して実行し、SGD との速度や結果の違いを見てみましょう。\n",
        "\n",
        "| オプティマイザ名   | 特徴                                                                 | PyTorchの関数名                |\n",
        "|--------------------|----------------------------------------------------------------------|--------------------------------|\n",
        "| SGD               | 確率的勾配降下法。シンプルで計算コストが低いが、収束が遅いことがある。  | `torch.optim.SGD`             |\n",
        "| Momentum          | SGD の改良版。前回の更新方向を加味して振動を抑え、安定した収束を実現。 | `torch.optim.SGD`（`momentum` オプションを使用） |\n",
        "| AdaGrad           | パラメータごとに学習率を調整。学習率が小さくなりすぎることが課題。      | `torch.optim.Adagrad`         |\n",
        "| RMSprop           | AdaGrad の学習率低下問題を解決。適応的な学習率を維持し、安定した学習。  | `torch.optim.RMSprop`         |\n",
        "| Adam              | Momentum と RMSprop を組み合わせたもの。高速収束と安定性が特徴。       | `torch.optim.Adam`            |\n"
      ],
      "metadata": {
        "id": "3LvfIcodaQMR"
      }
    }
  ]
}
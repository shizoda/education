{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/cnn/cifar10_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNNでの画像分類\n",
        "\n",
        "## CNNとは\n",
        "\n",
        "CNN (Convolutional Neural Network，畳み込みニューラルネットワーク )は、特に画像や映像の認識，解析において高い性能を発揮するディープラーニングの一種です．CNNは，入力データから特徴を自動的に学習し，識別や分類を行う能力を持っています．以下に，CNNの構造，動作原理，主な用途について説明します．\n",
        "\n",
        "<a title=\"Aphex34, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Typical_cnn.png\"><img width=\"512\" alt=\"Typical cnn\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/512px-Typical_cnn.png?20151217030420\"></a>\n",
        "\n",
        "### 主な構成要素\n",
        "\n",
        "- 畳み込み層\n",
        "\n",
        "畳み込み層は，画像内の局所的な特徴抽出を行います．隣接するピクセル間におけるエッジや色の変化といった局所的な特徴を検出し，画像内の情報を保持しつつ，高度な特徴抽出を実現します．\n",
        "\n",
        "<a title=\"Michael Plotke, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:2D_Convolution_Animation.gif\"><img width=\"256\" alt=\"2D Convolution Animation\" src=\"https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif?20130203224852\"></a>\n",
        "\n",
        "- プーリング層\n",
        "\n",
        "プーリング層では，畳み込み層で抽出された特徴が移動しても影響を受けないようにする役割を担います．畳み込み層から出力される特徴は局所的なものです．対象の特徴を維持しながら位置に関する情報をそぎ落とすことで，重要な情報のみを保持し，特徴量のサイズを小さくすることができます．\n",
        "\n",
        "最大プーリング (Max Pooling) と平均プーリング (Average Pooling) がよく使われます．\n",
        "\n",
        "<a title=\"Muhamad Yani, et al.; Creative Commons Attribution 3.0 Unported &lt;https://creativecommons.org/licenses/by/3.0/&gt;\" href=\"https://www.researchgate.net/figure/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max_fig2_333593451\"><img width = \"256\" alt =\"Creative Commons Attribution 3.0 Unported\" src=\"https://www.researchgate.net/publication/333593451/figure/fig2/AS:765890261966848@1559613876098/Illustration-of-Max-Pooling-and-Average-Pooling-Figure-2-above-shows-an-example-of-max.png\"></a>\n",
        "\n",
        "- 全結合層\n",
        "\n",
        "畳み込み層やプーリング層で抽出された特徴を基に，最終的な分類や予測を行う層です．分類であれば，入力データがどのクラスに属するかを決定します．"
      ],
      "metadata": {
        "id": "9KT4XxutH8R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "4iWMC66Jj-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CymrOWOeRR5o"
      },
      "source": [
        "## CNN での画像分類\n",
        "\n",
        "ここでは単純な CNN モデルを画像分類用にトレーニングし、評価します。\n",
        "\n",
        "データセットとして [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) を用います．CIFAR-10 は、10 の異なるクラス（飛行機、自動車、鳥、猫など）に属する全 60,000 枚の小さなカラー画像（32x32ピクセル）を含むデータセットです。\n",
        "\n",
        "**実行する場合には、Google Colab の GPU ランタイムをオンにしてください**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/shizoda/education/main/machine_learning/cnn/runtime1.png\" height=\"300\"> <img src=\"https://raw.githubusercontent.com/shizoda/education/main/machine_learning/cnn/runtime2.png\" height=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch のインポート\n",
        "\n",
        "深層学習のための主要なフレームワークとして、Google が開発した TensorFlow と、Meta (Facebook) が開発した PyTorch が有名です。今回は PyTorch を使用します。\n",
        "\n",
        "<a title=\"PyTorch, BSD &lt;http://opensource.org/licenses/bsd-license.php&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:PyTorch_logo_black.svg\"><img width=\"256\" alt=\"PyTorch logo black\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/PyTorch_logo_black.svg/256px-PyTorch_logo_black.svg.png?20200318230141\"></a>\n",
        "\n",
        "前述のとおり、高速な並列演算のために GPU を使用します。\n",
        "ランタイムの設定ができていない場合はエラーが出ます。"
      ],
      "metadata": {
        "id": "Uc5NiCEx7XgV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1TTBTvqRR5q"
      },
      "outputs": [],
      "source": [
        "# PyTorch 関連のライブラリをインポートします\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# ネットワーク構造を可視化する torchviz をインストール\n",
        "!pip install torchviz\n",
        "\n",
        "# GPU が利用可能であることを確認\n",
        "assert torch.cuda.is_available(), \"GPU が使えません。ランタイムの設定を確認してください。\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データのロード\n",
        "\n",
        "CIFAR-10データセットをダウンロードします。\n",
        "\n",
        "- 学習に用いる **学習**データセット\n",
        "\n",
        "- 学習中に精度を調べ，早期停止に用いる **検証**データセット\n",
        "\n",
        "- 学習後に精度を調べる **テスト**データセット\n",
        "\n",
        "の各データセットとなります。ちなみに\n",
        "\n",
        "- `transforms.Compose` は、データに適用する一連の前処理手順を定義します。この例では、画像を numpy 配列から「PyTorch テンソル」とよばれる形式に変換して、PyTorch が扱えるようにしています。\n",
        "\n",
        "- `DataLoader` は、大量にあるデータをミニバッチ (mini-batch) という単位で少しずつ取り出せるようにするものです。今回は画像を 100 枚ずつ取り出します。"
      ],
      "metadata": {
        "id": "18allCzL9tHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# 学習用データセットをロードし、検証用データセットに分割\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_size = int(0.8 * len(trainset))\n",
        "validation_size = len(trainset) - train_size\n",
        "train_dataset, validation_dataset = random_split(trainset, [train_size, validation_size])\n",
        "\n",
        "# ミニバッチ (mini-batch) サイズを100とし、学習用データローダと検証用データローダを定義\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "validationloader = torch.utils.data.DataLoader(validation_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# テスト用データセットをロード\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "FF9tDsRO9dxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **課題１**\n",
        "\n",
        "- CNN は ＿＿＿ ＿＿＿ Network の略で、日本語では ＿＿＿＿＿＿＿ ネットワーク。\n",
        " - ＿＿＿＿＿＿層や＿＿＿＿＿＿層を含む。\n",
        "- iris のデータセットによる演習では\n",
        " - 入力：４つの特徴量（ガクの長さ，ガクの幅，花弁の長さ，花弁の幅）\n",
        " - 出力：３つのクラス（アヤメの種類 setosa, versicolor, verginica）ごとの確率\n",
        "\n",
        " だったのに対し，今回の画像分類では\n",
        " - 入力：\n",
        " - 出力：（列挙する必要はありません）\n",
        "- データセットを３つに分割しました。それぞれ\n",
        " - 　学習データセット： 学習に使う。\n",
        " - ＿＿＿データセット： ＿＿＿＿＿＿＿＿＿＿＿＿ に使う。（← 続きの処理を試してから回答しても OK）\n",
        " - ＿＿＿データセット： 最終的なテストに使う。"
      ],
      "metadata": {
        "id": "iQGa8EH5WV7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "それでは、いくつか画像を見てみましょう。"
      ],
      "metadata": {
        "id": "M7suQbyWE7WQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7T1y9MRRR5r"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 画像を表示\n",
        "def imshow(img, ax):\n",
        "    img = img / 2 + 0.5  # 正規化を元に戻す\n",
        "    npimg = img.numpy()\n",
        "    ax.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# DataLoader から画像を取得して表示する関数\n",
        "def show_images(dataloader, num_images, classes=classes):\n",
        "    dataiter = iter(dataloader)\n",
        "    images, labels = next(dataiter)  # バッチを一つ取得\n",
        "\n",
        "    # 画像を表示\n",
        "    fig, axes = plt.subplots(1, num_images, figsize=(num_images * 2.5, 2.5))\n",
        "    for idx in range(num_images):\n",
        "        ax = axes[idx]\n",
        "        imshow(images[idx], ax)\n",
        "        ax.set_title(f'{classes[labels[idx]]:5s}')\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# 使用例：trainloader から 4 枚の画像を表示\n",
        "show_images(trainloader, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN モデルの定義\n",
        "\n",
        "`forward` メソッドを１行ずつ確認して、どのような構成になっているか確認してみましょう。\n",
        "\n",
        "畳み込み層の後には ReLu という活性化関数を入れています。[活性化関数って？ (Zenn)](https://zenn.dev/nekoallergy/articles/ml-basic-act-01)"
      ],
      "metadata": {
        "id": "gW1nr-R992We"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCBWoA1lRR5r"
      },
      "outputs": [],
      "source": [
        "# Net クラスは、CNNモデルのアーキテクチャを定義します。\n",
        "# モデルは畳み込み層（nn.Conv2d）、プーリング層（nn.MaxPool2d）、全結合層（nn.Linear）から構成されています。\n",
        "# forward メソッドは、ネットワークを通じて入力がどのように進むかを定義します\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # 畳み込み層1: 入力チャンネル数 3、出力チャンネル数 6、カーネルサイズ 5x5\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "\n",
        "        # プーリング層: カーネルサイズ 2x2、ストライド 2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # 畳み込み層2: 入力チャンネル数 6、出力チャンネル数 16、カーネルサイズ 5x5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        # 全結合層1: 入力サイズ 16 * 5 * 5、出力サイズ 120\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "\n",
        "        # 全結合層2: 入力サイズ 120、出力サイズ 84\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "\n",
        "        # 全結合層3: 入力サイズ 84、出力サイズ 10 (クラス数)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 入力 x を畳み込み層1に通す\n",
        "        x = self.conv1(x)\n",
        "        # 活性化関数 ReLU を適用\n",
        "        x = F.relu(x)\n",
        "        # プーリング層に通す\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # 畳み込み層2に通す\n",
        "        x = self.conv2(x)\n",
        "        # 活性化関数 ReLU を適用\n",
        "        x = F.relu(x)\n",
        "        # プーリング層に通す\n",
        "        x = self.pool(x)\n",
        "\n",
        "        # テンソルをフラットな形に変形する\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "\n",
        "        # 全結合層1に通す\n",
        "        x = self.fc1(x)\n",
        "        # 活性化関数 ReLU を適用\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # 全結合層2に通す\n",
        "        x = self.fc2(x)\n",
        "        # 活性化関数 ReLU を適用\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # 全結合層3に通す（最終的な出力層）\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# インスタンス化。Net クラス（設計図）のオブジェクト（実体）を net とする\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net = Net().to(device)\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image as IPImage, display\n",
        "from torchviz import make_dot\n",
        "\n",
        "def visualize_network_structure(model, input_size=(1, 3, 32, 32)):\n",
        "    def forward_hook(module, input, output):\n",
        "        layer_types.append(type(module).__name__)\n",
        "\n",
        "    layer_types = []\n",
        "    hooks = []\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, (nn.Conv2d, nn.MaxPool2d, nn.ReLU, nn.Linear, nn.BatchNorm2d)):\n",
        "            hooks.append(module.register_forward_hook(forward_hook))\n",
        "\n",
        "    x = torch.randn(input_size).to(device)\n",
        "    model(x)\n",
        "\n",
        "    for hook in hooks:\n",
        "        hook.remove()\n",
        "\n",
        "\n",
        "    y = model(x)\n",
        "    dot = make_dot(y)\n",
        "    return dot\n",
        "\n",
        "dot = visualize_network_structure(net)\n",
        "display(IPImage(dot.pipe(format='png')))"
      ],
      "metadata": {
        "id": "8dysJVbRDgpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **課題２**\n",
        "\n",
        "上記の forward() 関数は、画像を x という引数で受け取り、畳み込み層(`conv1`）、活性化関数 (`ReLU`)… と通していきます．このうち重要な\n",
        "\n",
        "- 畳み込み層（２回）\n",
        "- プーリング層（２回）\n",
        "- 全結合層\n",
        "\n",
        "について、どの順に通されていくかを並べて書いてください．"
      ],
      "metadata": {
        "id": "t7uA_ipDV31j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 損失関数とオプティマイザ\n",
        "\n",
        "初期学習率を指定します。小さすぎると学習が収束するまでに時間がかかる可能性がありますが、大きすぎると学習が不安定になることがあります。Adam オプティマイザ (`optim.Adam`) がその後の学習率を調整していきます。\n",
        "\n",
        "**損失関数**（誤差関数，ロスともいう）としてクロスエントロピー (`nn.CrossEntropyLoss`) を用います。後述のとおり、損失関数はモデルの出力と正解がどれくらいズレているかを測定するものです。\n"
      ],
      "metadata": {
        "id": "OXyxfvaJAXyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_lr = 0.001 # 初期学習率\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=initial_lr) # Adam オプティマイザ\n",
        "criterion = nn.CrossEntropyLoss() # クロスエントロピー損失関数"
      ],
      "metadata": {
        "id": "WJLid24gAd2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習\n",
        "\n",
        "学習データセットから画像をランダムに数枚取り出した **ミニバッチ** (mini-batch) をネットワークに入力すると，何らかの出力が得られます．上記の損失関数によって出力と正解を比較し，「出力がどれくらい正解に近いか」を評価します．損失の値が小さいほど，正解に近いことを表します．\n",
        "\n",
        "それを繰り返しつつ，少しずつ損失を小さくするようにネットワークの重み（畳み込みに用いるカーネル）を更新していきます．この更新の仕組みは **誤差逆伝播法** (back-propagation) とよばれます．損失が小さくなる様子をグラフで確認してみてください．損失が小さくなっていくということは，画像に対して正解に近い出力が得られるようになってくることを意味します．\n",
        "\n",
        "毎回、学習に使用しない検証データに対しても誤差を計算します。この誤差が一定回数改善しない場合は学習を終了します。学習しすぎると **過学習** を起こすことがあるためです。<br>[機械学習における過学習とは何か？ (TRIETING)](https://www.tryeting.jp/column/6846/)\n",
        "\n",
        "更新はミニバッチごとに行われ，今回はミニバッチ１個あたり100枚の画像となっています（冒頭付近での設定）．それを繰り返して，ひととおり学習データをすべて使い切ることを 1 **エポック** と数えます．今回は最大10エポックの学習を行うこととしています．"
      ],
      "metadata": {
        "id": "XhO0vVH2AfVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNB7FZK6RR5r"
      },
      "outputs": [],
      "source": [
        "max_epoch = 10     # 最大エポック数\n",
        "patience = 5       # 改善が見られないエポック数の許容回数\n",
        "trigger_times = 0  # 改善が見られないエポック数のカウンター\n",
        "\n",
        "best_val_loss = float(\"inf\")  # 初期値として無限大を設定\n",
        "train_losses = []  # 学習データセットの損失を保存するリスト\n",
        "val_losses = []  # 検証データセットの損失を保存するリスト\n",
        "\n",
        "# エポック数のループ\n",
        "for epoch in range(max_epoch):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # 学習データセットからミニバッチを得るたびに…\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)  # 入力データをGPUに送る\n",
        "        labels = labels.to(device)  # ラベルをGPUに送る\n",
        "\n",
        "        optimizer.zero_grad()  # 勾配の初期化\n",
        "\n",
        "        outputs = net(inputs)  # ネットワークに入力データを渡して出力を取得\n",
        "        loss = criterion(outputs, labels)  # 損失を計算\n",
        "        loss.backward()  # 逆伝播を行い、勾配を計算\n",
        "        optimizer.step()  # パラメータを更新\n",
        "\n",
        "        running_loss += loss.item()  # ミニバッチの損失を累積\n",
        "\n",
        "    # エポックごとの訓練データセットに対する平均損失を計算\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 検証データセットに対する損失を計算\n",
        "    val_loss = 0.0\n",
        "    net.eval()  # モデルを評価モードに切り替える\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # 検証データセットからミニバッチを得るたびに…\n",
        "        for data in validationloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)  # 入力データをGPUに送る\n",
        "            labels = labels.to(device)  # ラベルをGPUに送る\n",
        "\n",
        "            outputs = net(images)  # ネットワークに入力データを渡して出力を取得\n",
        "            loss = criterion(outputs, labels)  # 損失を計算\n",
        "            val_loss += loss.item()  # 損失を累積\n",
        "\n",
        "    # エポックごとの検証データセットに対する平均損失を計算\n",
        "    val_loss = val_loss / len(validationloader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # 損失の値をグラフで表示\n",
        "    plt.clf()  # 前のグラフをクリア\n",
        "    plt.plot(train_losses, label='Training loss')  # 訓練データセットの損失\n",
        "    plt.plot(val_losses, label='Validation loss')  # 検証データセットの損失をプロット\n",
        "    plt.xlabel('Epoch')  # x軸のラベルを設定\n",
        "    plt.ylabel('Loss')  # y軸のラベルを設定\n",
        "    plt.xlim(left=0)  # x軸の表示範囲を設定\n",
        "    plt.ylim(bottom=0)  # y軸の表示範囲を設定\n",
        "    plt.legend()  # 凡例を表示\n",
        "    plt.show()  # グラフを表示\n",
        "\n",
        "    # エポックごとの損失を出力\n",
        "    print(\"Epoch:\", epoch + 1)\n",
        "    print(\"Train loss     : \", train_loss)\n",
        "    print(\"Validation loss: \", val_loss)\n",
        "\n",
        "    # 検証データセットに対する損失が改善しない場合の処理\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss  # 最良の検証データセット損失を更新\n",
        "        trigger_times = 0  # 改善が見られないエポック数をリセット\n",
        "    else:\n",
        "        trigger_times += 1  # 改善が見られないエポック数をカウントアップ\n",
        "        if trigger_times >= patience:  # 許容回数を超えた場合の処理\n",
        "            print(f\"{epoch + 1} エポックで早期終了\")  # 早期終了のメッセージを出力\n",
        "            break  # 学習を終了\n",
        "\n",
        "print('学習終了')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **課題３**\n",
        "\n",
        "- ミニバッチとは？\n",
        "- ミニバッチに含まれる画像をネットワークに入力すると、なんらかの出力が得られます。\n",
        " - 10 クラスの分類問題なので ＿＿ 個の値が出力されます。それぞれ、画像がそのクラスに属する ＿＿＿＿＿＿ を意味します。\n",
        " - それを ＿＿＿＿＿＿＿ することで損失（誤差）を計算します。\n",
        " - その損失をなるべく ＿＿ くするようにネットワークの中身を更新していくことで学習を進めます。\n",
        "- グラフで２本の損失関数が描かれました\n",
        " - Train loss は学習データセットを用いて計算されるので、これを ＿＿＿＿ するように学習が進められている。\n",
        " - Validation loss は ＿＿＿ データセットを用いて計算されており、学習に直接的には使われていないが、＿＿＿＿＿＿ に用いることで ＿＿＿＿＿ を防いでいる。\n"
      ],
      "metadata": {
        "id": "_7JJVyyeXqfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テスト\n",
        "訓練されたモデルを使用してテストデータセット上でのパフォーマンスを評価します。\n",
        "\n",
        "最終的な精度（正確に分類された画像の割合）を求めることができます。"
      ],
      "metadata": {
        "id": "9HBS1ndOBr4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yyy-1rmURR5s"
      },
      "outputs": [],
      "source": [
        "correct = 0  # 正解数のカウンターを初期化\n",
        "total = 0  # 全体の画像数のカウンターを初期化\n",
        "\n",
        "# 訓練モードでの計算を停止（推論モードに切り替える）\n",
        "with torch.no_grad():\n",
        "    # テストデータセットに対してループ\n",
        "    for data in testloader:\n",
        "        images, labels = data  # ミニバッチごとの画像とラベルを取得\n",
        "        images = images.to(device)  # GPUに画像を送る\n",
        "        labels = labels.to(device)  # GPUにラベルを送る\n",
        "\n",
        "        outputs = net(images)  # ネットワークに画像を入力し、出力を取得\n",
        "        _, predicted = torch.max(outputs.data, 1)  # 出力の中で最大の値を持つクラスを予測として取得\n",
        "        total += labels.size(0)  # ミニバッチ内の画像数を全体の画像数に加算\n",
        "        correct += (predicted == labels).sum().item()  # 予測が正しい場合、正解数をカウントアップ\n",
        "\n",
        "# 精度を計算して表示\n",
        "print('テスト画像における精度 %d %%' % (100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "個々の結果も見てみましょう。"
      ],
      "metadata": {
        "id": "DeuYZUrODa8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlN_iIbvRR5s"
      },
      "outputs": [],
      "source": [
        "# テストデータの画像を表示し、正解と推定値を表示する関数\n",
        "def visualize_test_predictions(start_idx, end_idx):\n",
        "    # テストデータローダーを作成\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    # 指定したインデックス範囲の画像とラベルを取得\n",
        "    images, labels = zip(*[(data[0], data[1]) for i, data in enumerate(testloader) if start_idx <= i < end_idx])\n",
        "\n",
        "    # モデルの予測結果を格納するリストを初期化\n",
        "    predicted_labels = []\n",
        "\n",
        "    # 推論モードで計算を停止\n",
        "    with torch.no_grad():\n",
        "        # 画像ごとに予測を行う\n",
        "        for image in images:\n",
        "            image = image.to(device)  # GPUに画像を送る\n",
        "            outputs = net(image)  # ネットワークに画像を入力し、出力を得る\n",
        "            _, predicted = torch.max(outputs, 1)  # 出力の中で最大の値を持つクラスを予測として取得\n",
        "            predicted_labels.append(predicted.item())  # 予測結果をリストに追加\n",
        "\n",
        "    # 画像とラベルを表示\n",
        "    for i in range(len(images)):\n",
        "        print(f\"画像 {start_idx + i}\")\n",
        "        imshow(torchvision.utils.make_grid(images[i]))  # 画像をグリッド形式で表示\n",
        "        print(f\"正解： {classes[labels[i]]}\")  # 正解ラベルを表示\n",
        "        print(f\"推定： {classes[predicted_labels[i]]}\\n\")  # 予測ラベルを表示\n",
        "\n",
        "# 例: インデックス範囲 0 から 5 の画像を表示\n",
        "visualize_test_predictions(0, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 何が学習されたのかを見る\n",
        "\n",
        "前回は自分で畳み込みを練習して、フィルタを考えてもらいました。\n",
        "\n",
        "ここでは、全結合層が画像分類を行えるよう、画像から意味のある特徴量を抽出**（特徴抽出）** するために用いています。しかも、それに適するようにカーネルの値を自動で調整してくれます。この **カーネルの調整こそが CNN の特徴抽出の学習である** といってもいいでしょう。もちろん、全結合層の重みも一緒に調節され、高精度な画像分類ができるようになります。\n",
        "\n",
        "それに対して、（説明を省いていますが）ReLU Activation や、先に述べた Max Pooling といった層では、学習されるものはありません。"
      ],
      "metadata": {
        "id": "UOk8_h5jG2jR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_img = 3\n",
        "\n",
        "def fig_title(layer_name):\n",
        "  print(layer_name)\n",
        "  if layer_name.find(\"onv\") >= 0:\n",
        "    return \"Convolution \" + layer_name.replace(\"conv\",\"\") + f\" ({layer_name})\"\n",
        "  elif layer_name.find(\"pool\") >= 0:\n",
        "    return \"Max-Pooling \" + layer_name.replace(\"pool\",\"\") + f\" ({layer_name})\"\n",
        "  elif layer_name.find(\"relu\") >= 0:\n",
        "    return \"ReLU Activation \" + layer_name.replace(\"relu\",\"\") + f\" ({layer_name})\"\n",
        "  else:\n",
        "    return f\"({layer_name})\"\n",
        "\n",
        "\n",
        "class VisualizeActivations:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.activation = {}\n",
        "        self.hooks = []\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        self.hooks.append(self.model.conv1.register_forward_hook(self._get_activation('conv1')))\n",
        "        self.hooks.append(self.model.conv2.register_forward_hook(self._get_activation('conv2')))\n",
        "\n",
        "    def _get_activation(self, name):\n",
        "        def hook(model, input, output):\n",
        "            self.activation[name] = output.detach()\n",
        "        return hook\n",
        "\n",
        "    def visualize_layer(self, layer_name, title, num_images=4, cmap='gray'):\n",
        "        act = self.activation[layer_name].squeeze().cpu().detach().numpy()\n",
        "        num_images = min(num_images, act.shape[0])\n",
        "        fig, axarr = plt.subplots(1, num_images + 1, figsize=(num_images * 1.2, 1.2), gridspec_kw={'width_ratios': [0.2] + [1] * num_images})\n",
        "\n",
        "        axarr[0].text(0.5, 0.5, title, rotation='vertical', verticalalignment='center', horizontalalignment='center')\n",
        "        axarr[0].axis('off')\n",
        "\n",
        "        for idx in range(num_images):\n",
        "            axarr[idx + 1].imshow(act[idx], cmap=cmap)\n",
        "            axarr[idx + 1].axis('off')\n",
        "        # fig.suptitle(f\"[{layer_name}]\")\n",
        "        fig.suptitle(fig_title(layer_name))\n",
        "        plt.show()\n",
        "\n",
        "    def display_conv_kernels(self, layer_name, conv_layer, title, num_images=4, cmap='gray'):\n",
        "        kernels = conv_layer.weight.detach().cpu().numpy()\n",
        "        num_kernels = min(num_images, kernels.shape[0])\n",
        "        act = self.activation[layer_name].squeeze().cpu().detach().numpy()\n",
        "        fig, axarr = plt.subplots(2, num_kernels + 1, figsize=(num_kernels * 1.2, 2.4), gridspec_kw={'width_ratios': [0.2] + [1] * num_kernels})\n",
        "\n",
        "        axarr[0, 0].text(0.5, 0.5, \"Kernels\", rotation='vertical', verticalalignment='center', horizontalalignment='center')\n",
        "        axarr[0, 0].axis('off')\n",
        "        axarr[1, 0].text(0.5, 0.5, title, rotation='vertical', verticalalignment='center', horizontalalignment='center')\n",
        "        axarr[1, 0].axis('off')\n",
        "\n",
        "        for idx in range(num_kernels):\n",
        "            axarr[0, idx + 1].imshow(kernels[idx, 0], cmap=cmap)\n",
        "            axarr[0, idx + 1].axis('off')\n",
        "            axarr[1, idx + 1].imshow(act[idx], cmap=cmap)\n",
        "            axarr[1, idx + 1].axis('off')\n",
        "        fig.suptitle(fig_title(layer_name))\n",
        "        # fig.suptitle(f\"[{layer_name}]\")\n",
        "        plt.show()\n",
        "\n",
        "    def clear_hooks(self):\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "        self.hooks = []\n",
        "\n",
        "# VisualizeActivations インスタンス作成\n",
        "visualizer = VisualizeActivations(net)\n",
        "\n",
        "# テスト画像を取得し、指定された層の出力を可視化\n",
        "def visualize_test_image(testloader, index):\n",
        "    dataiter = iter(testloader)\n",
        "    images, labels = next(dataiter)\n",
        "    img = images[index].unsqueeze(0).to(device)\n",
        "\n",
        "    net(img)\n",
        "\n",
        "    visualizer.display_conv_kernels('conv1', net.conv1, 'Outputs', num_images=6)\n",
        "\n",
        "    relued = F.relu(net.conv1(img))\n",
        "    visualizer.activation['relu1'] = relued\n",
        "    visualizer.visualize_layer('relu1', 'Outputs', num_images=6)\n",
        "\n",
        "    pooled = net.pool(F.relu(net.conv1(img)))\n",
        "    visualizer.activation['pool1'] = pooled\n",
        "    visualizer.visualize_layer('pool1', 'Outputs', num_images=6)\n",
        "\n",
        "    visualizer.display_conv_kernels('conv2', net.conv2, 'Outputs', num_images=16)\n",
        "\n",
        "    relued = F.relu(net.conv2(pooled))\n",
        "    visualizer.activation['relu2'] = relued\n",
        "    visualizer.visualize_layer('relu2', 'Outputs', num_images=16)\n",
        "\n",
        "    pooled = net.pool(F.relu(net.conv2(pooled)))\n",
        "    visualizer.activation['pool2'] = pooled\n",
        "    visualizer.visualize_layer('pool2', 'Outputs', num_images=16)\n",
        "\n",
        "\n",
        "# テスト用データセットをロード\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# テスト画像の可視化\n",
        "visualize_test_image(testloader, selected_img)\n",
        "\n",
        "# フックをクリア\n",
        "visualizer.clear_hooks()"
      ],
      "metadata": {
        "id": "x3TTkaFjCHYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題３**\n",
        "- iris の演習ではガクの幅・花弁の長さなどの値を特徴量として最初から入力し、それらをもとにアヤメの種類を分類しました。<br>\n",
        "今回、特徴量として全結合層の入力となるのは、どの層の出力ですか？\n",
        "- 今回は 2 つの畳み込み層がありました。学習中に調節される値の数は何個ありますか？\n",
        " - `conv1` 縦＿＿＿ × 横＿＿＿ × フィルタ数＿＿＿＿＿ ＝ ＿＿＿＿個\n",
        " - `conv2` 縦＿＿＿ × 横＿＿＿ × フィルタ数＿＿＿＿＿ ＝ ＿＿＿＿個\n"
      ],
      "metadata": {
        "id": "ZvSdSDTGSVtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " ---\n",
        "\n",
        "### **課題４（発展）**\n",
        "\n",
        "このデータセットを用いて、分類精度を上げる研究計画を考えてみましょう。<br>\n",
        "正解はありませんので自由に考えてください。\n",
        "\n",
        "- 問題点\n",
        " - 今回の手法で、精度が十分でない技術的な理由や、改善できそうな項目は見つかりますか？\n",
        "- 目的\n",
        " - どのように改善して精度向上を目指しますか？"
      ],
      "metadata": {
        "id": "UwuVf_RpXjN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        " ### **課題５（発展）**\n",
        "\n",
        "[Kaggle](https://www.kaggle.com)（カグル）は、いろいろな人たちが機械学習を練習したり、データセットを共有したり、高性能な手法を開発すべく競うコンペを開催する場です。\n",
        "\n",
        "[Kaggleとは？機械学習初心者が知っておくべき3つの使い方](https://www.codexa.net/what-is-kaggle/) (codexa)\n",
        "\n",
        "<a title=\"Kaggle, Public domain, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Kaggle_Logo.svg\"><img width=\"150\" alt=\"Kaggle Logo\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f4/Kaggle_Logo.svg/512px-Kaggle_Logo.svg.png?20240209024103\"></a>\n",
        "\n",
        "- どんな画像分類のデータセットがありますか？\n",
        " - [Datasets](https://www.kaggle.com/datasets) で **image classification** と検索して、どれか１つ開いてみてください\n",
        "- [Competitions](https://www.kaggle.com/competitions)（コンペ）も見てみましょう\n",
        " - [このコンペ](https://www.kaggle.com/competitions/rsna-miccai-brain-tumor-radiogenomic-classification/overview) は、何の画像を、どんなクラスに分類するものでしょうか？"
      ],
      "metadata": {
        "id": "96b7ALcmeuu8"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBxaNSG7O+yiCl8K6C1qZW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/cnn/beginner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎨 AIは「計算」する：CNNによる画像分類と学習の可視化\n",
        "\n",
        "こんにちは。このノートブックでは、AI（人工知能）がどのようにして画像を見分ける（**クラス分類**）ようになるのかを、一緒に体験していきます。\n",
        "\n",
        "## 🤖 1. AIは「思考」ではなく「計算」をしている\n",
        "\n",
        "AIというと、まるで人間のように「考えて」いるように思えますが、現在のAI（特にディープラーニング）の正体は、膨大な **「計算」** です。\n",
        "\n",
        "* **入力：** 画像（たくさんのピクセルの集まり＝数値）\n",
        "* **処理：** モデル（計算のルールを詰め込んだもの）が、入力された数値を次々と計算\n",
        "* **出力：** 計算結果（例：「この画像が『ネコ』である確率は80%」といった数値）\n",
        "\n",
        "AIの「学習」とは、この**計算ルール（モデルの中身）を、目的に合わせて少しずつ調整していく作業**のことです。"
      ],
      "metadata": {
        "id": "SojDzdRXX6Y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📁 2. データの準備と歴史：「特徴」をどう見つけるか\n",
        "\n",
        "画像を分類するためには、画像の中の「特徴」を見つける必要があります。\n",
        "\n",
        "### 📜 AIの歴史：特徴抽出の自動化\n",
        "\n",
        "* **昔（～2010年頃）：**\n",
        "    人間が「特徴とは何か」を必死に考えていました。「ネコの特徴は、三角の耳と丸い目だ」といったルール（**特徴量**）を人間が手動で設計し、AI（当時はサポートベクターマシンなどが主流）に教えていました。\n",
        "* **今（ディープラーニングの時代）：**\n",
        "    人間が特徴を設計するのではなく、AI自身がデータ（大量の画像）から「分類に役立つ特徴」を自動で見つけ出すようになりました。その代表的な手法が **CNN（畳み込みニューラルネットワーク）** です。\n",
        "\n",
        "### 📚 教師あり学習：お手本（教師）データで学ぶ\n",
        "\n",
        "今回は「**教師あり学習**」という方法を使います。これは、AIに「問題（画像）」と「正解（ラベル）」のペアをたくさん見せて学習させる方法です。\n",
        "\n",
        "データは以下の3種類に分けて使います。\n",
        "1.  **教師データ (Training data):**\n",
        "    学習に使うメインのデータ。AIはこれを見て計算ルールを調整します。\n",
        "2.  **検証データ (Validation data):**\n",
        "    学習の途中で「今の実力」を試すためのデータ。学習の「やりすぎ」（過学習）を防ぐために使います。\n",
        "3.  **テストデータ (Test data):**\n",
        "    学習には一切使わない、最後の実力テスト用のデータ。これでAIの最終的な性能を評価します。\n",
        "\n",
        "ここでは「CIFAR-10」という、10種類の画像（飛行機、車、鳥、猫など）を集めたデータセットを使います。"
      ],
      "metadata": {
        "id": "NjXj3WpZX8O6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🛠️ 3. 準備：ライブラリと関数の読み込み\n",
        "\n",
        "（このセルは、Pythonやプログラミングに詳しくない方は、中身を見ずに実行だけしてください。「準備のおまじない」のようなものです）\n",
        "\n",
        "ここでは、AIを作るための道具（ライブラリ）を読み込みます。AIの計算、データの可視化（Matplotlib）、Colabのフォーム機能など、様々な機能を使います。\n",
        "\n",
        "### 🤖 AIの計算エンジン：代表的なライブラリ\n",
        "\n",
        "AI（特にディープラーニング）を作るには、複雑な計算を高速に行うための専門のライブラリ（フレームワーク）が必要です。代表的なものとして、**TensorFlow**、**PyTorch**、**scikit-learn** があります。\n",
        "\n",
        "\n",
        "* **scikit-learn (サイキット・ラーン):**\n",
        "    * ディープラーニングが登場する前から広く使われている、伝統的な機械学習ライブラリです。\n",
        "    * 今回のt-SNE（データの可視化）やデータの前処理のように、ディープラーニングの「補助」としても活躍します。\n",
        "\n",
        "\n",
        "* **TensorFlow (テンソルフロー):**\n",
        "    * Googleが開発したフレームワークで、非常に有名です。\n",
        "    * 特に、作ったAIを実際の製品やサービスに組み込む（デプロイ）ための機能が充実しています。\n",
        "\n",
        "* **PyTorch (パイトーチ):**\n",
        "    * Facebook (現Meta) が開発しました。\n",
        "    * Pythonの書き方に近く、プログラムがどう動いているかステップごとに確認しやすい（「Define-by-Run」という特徴）ため、学習や研究で人気があります。\n",
        "    * モデルの構造を柔軟に変更しやすいのも特徴です。\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 今回使用する「PyTorch」\n",
        "\n",
        "このノートブックでは、上記の中から **PyTorch** を使います。\n",
        "研究や学習の分野で人気が高く、AIがどのような計算を行っているかを直感的に理解するのに適しているためです。\n",
        "\n",
        "<a title=\"PyTorch, BSD &lt;http://opensource.org/licenses/bsd-license.php&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:PyTorch_logo_black.svg\"><img width=\"256\" alt=\"PyTorch logo black\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/PyTorch_logo_black.svg/256px-PyTorch_logo_black.svg.png?20200318230141\"></a>\n"
      ],
      "metadata": {
        "id": "LaDK8ErIX98s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 演習 3-0-1: PyTorch をインポートしてください\n",
        "# @markdown `torch` をインポートする、と書きます。`PyTorch` や `pytorch` ではありません\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DhQKk9mBTRfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 📊 （補足）t-SNEとは？\n",
        "\n",
        "「畳み込み」などで取り出された「特徴（たくさんの数値）」は、そのままでは多すぎて人間には理解できません。t-SNEは、そうした高次元のデータを、お互いの「近さ」を保ったまま、2次元のグラフ（散布図）に圧縮してくれる技術です。"
      ],
      "metadata": {
        "id": "icCfbMMRTbhK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ_4b6h7X52s"
      },
      "outputs": [],
      "source": [
        "#@title 3-1. 準備コード（設定変更せず実行してください） { display-mode: \"code\" }\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# PyTorch (AIの計算フレームワーク)\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# scikit-learn (t-SNE, 決定境界, データ分割, 標準化)\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 警告を非表示\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --- PixelsInImage から追加 ---\n",
        "import cv2\n",
        "import urllib.request\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "# ------------------------------\n",
        "\n",
        "# デバイス設定 (GPUが使えるか確認)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"使用するデバイス: {device}\")\n",
        "\n",
        "# CIFAR-10のクラス名\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# --- データの前処理と読み込み ---\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# 学習用データセットをダウンロード\n",
        "try:\n",
        "    trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                 download=True, transform=transform)\n",
        "    # テストデータセットをダウンロード\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                          download=True, transform=transform)\n",
        "except Exception as e:\n",
        "    print(f\"データセットのダウンロードに失敗しました: {e}\")\n",
        "    print(\"ネットワーク接続を確認するか、ランタイムを再起動してください。\")\n",
        "\n",
        "# 学習用データセットを「教師データ」と「検証データ」に分割\n",
        "# 50000件のうち、40000件を教師データ、10000件を検証データに\n",
        "train_indices, val_indices = train_test_split(\n",
        "    list(range(len(trainset_full))),\n",
        "    test_size=0.2, # 20%を検証用\n",
        "    random_state=42\n",
        ")\n",
        "train_dataset = Subset(trainset_full, train_indices)\n",
        "validation_dataset = Subset(trainset_full, val_indices)\n",
        "\n",
        "print(f\"全学習データ: {len(trainset_full)}件\")\n",
        "print(f\"  教師データ: {len(train_dataset)}件\")\n",
        "print(f\"  検証データ: {len(validation_dataset)}件\")\n",
        "print(f\"テストデータ: {len(testset)}件\")\n",
        "\n",
        "# --- t-SNE と 決定境界のための関数 ---\n",
        "\n",
        "# 特徴抽出関数\n",
        "def extract_features(model, dataloader, device):\n",
        "    model.eval() # 評価モード\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device)\n",
        "            # fc2層の出力（84次元）を特徴として抽出\n",
        "            features = model.extract_features(images)\n",
        "            features_list.append(features.cpu().numpy())\n",
        "            labels_list.append(labels.cpu().numpy())\n",
        "\n",
        "    all_features = np.concatenate(features_list, axis=0)\n",
        "    all_labels = np.concatenate(labels_list, axis=0)\n",
        "    return all_features, all_labels\n",
        "\n",
        "# t-SNE実行関数\n",
        "def run_tsne(features, random_state=42):\n",
        "    # データを標準化\n",
        "    scaler = StandardScaler()\n",
        "    features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "    # t-SNE\n",
        "    # perplexityはデータの密度に依存。サンプル数が多いので40程度に設定\n",
        "    tsne = TSNE(n_components=2, perplexity=40, random_state=random_state, max_iter=300, verbose=0)\n",
        "    features_tsne = tsne.fit_transform(features_scaled)\n",
        "    return features_tsne\n",
        "\n",
        "# t-SNE可視化関数\n",
        "def plot_tsne(features_tsne, labels, title):\n",
        "    df_vis = pd.DataFrame({\n",
        "        'tsne_x': features_tsne[:, 0],\n",
        "        'tsne_y': features_tsne[:, 1],\n",
        "        'label': labels\n",
        "    })\n",
        "    df_vis['class_name'] = df_vis['label'].apply(lambda x: classes[x])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    palette = sns.color_palette(\"tab10\", 10)\n",
        "    sns.scatterplot(\n",
        "        data=df_vis.sample(frac=1, random_state=42), # 描画順をシャッフル\n",
        "        x='tsne_x', y='tsne_y',\n",
        "        hue='class_name',\n",
        "        palette=palette,\n",
        "        s=10,\n",
        "        alpha=0.7,\n",
        "        legend='full'\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"t-SNE Component 1\")\n",
        "    plt.ylabel(\"t-SNE Component 2\")\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "    plt.show()\n",
        "\n",
        "# 決定境界の可視化関数\n",
        "def plot_decision_boundary(X_2d, y_2d, mlp_2d, title):\n",
        "    print(\"決定境界をプロット中...\")\n",
        "\n",
        "    x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
        "    y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
        "    h = (x_max - x_min) / 100 # メッシュの解像度\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "\n",
        "    Z = mlp_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.get_cmap(\"tab10\", 10))\n",
        "\n",
        "    # データポイントの散布図 (t-SNEと同じ)\n",
        "    palette = sns.color_palette(\"tab10\", 10)\n",
        "    sns.scatterplot(\n",
        "        x=X_2d[:, 0],\n",
        "        y=X_2d[:, 1],\n",
        "        hue=y_2d,\n",
        "        palette=palette,\n",
        "        s=5,\n",
        "        alpha=0.5,\n",
        "        legend='full'\n",
        "    )\n",
        "\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    new_labels = [classes[int(label)] for label in labels]\n",
        "    plt.legend(handles=handles, labels=new_labels, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"t-SNE Component 1\")\n",
        "    plt.ylabel(\"t-SNE Component 2\")\n",
        "    plt.show()\n",
        "\n",
        "# --- PixelsInImage から追加 ---\n",
        "# ピクセル値を文字として表示し、格子を描画する汎用関数\n",
        "def show_channel_with_grid_and_values(ax, data, title, cmap='gray', vmin=0, vmax=255):\n",
        "    ax.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(title)\n",
        "    ax.axis(\"off\")\n",
        "    # 格子を描画\n",
        "    for i in range(data.shape[0] + 1):\n",
        "        ax.axhline(i - 0.5, color='white', lw=1)\n",
        "    for j in range(data.shape[1] + 1):\n",
        "        ax.axvline(j - 0.5, color='white', lw=1)\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            # テキストの色を濃度値に応じて変更\n",
        "            val = data[i, j]\n",
        "            text_color = \"white\"\n",
        "            if cmap == 'gray' and val > 128:\n",
        "                text_color = 'black'\n",
        "            elif cmap == 'Red_cmap' and val > 200:\n",
        "                 text_color = 'black'\n",
        "            elif cmap == 'Green_cmap' and val > 180:\n",
        "                text_color = 'black'\n",
        "            elif cmap == 'Blue_cmap' and val > 180:\n",
        "                text_color = 'black'\n",
        "\n",
        "            ax.text(j, i, f\"{val:3}\", ha=\"center\", va=\"center\", color=text_color, fontsize=10)\n",
        "\n",
        "# カスタムカラーマップを定義\n",
        "cdict_red = {'red':   ((0.0, 0.0, 0.0), (1.0, 1.0, 1.0)), 'green': ((0.0, 0.0, 0.0), (1.0, 0.0, 0.0)), 'blue':  ((0.0, 0.0, 0.0), (1.0, 0.0, 0.0))}\n",
        "Red_cmap = LinearSegmentedColormap('Red_cmap', segmentdata=cdict_red, N=256)\n",
        "cdict_green = {'red':   ((0.0, 0.0, 0.0), (1.0, 0.0, 0.0)), 'green': ((0.0, 0.0, 0.0), (1.0, 1.0, 1.0)), 'blue':  ((0.0, 0.0, 0.0), (1.0, 0.0, 0.0))}\n",
        "Green_cmap = LinearSegmentedColormap('Green_cmap', segmentdata=cdict_green, N=256)\n",
        "cdict_blue = {'red':   ((0.0, 0.0, 0.0), (1.0, 0.0, 0.0)), 'green': ((0.0, 0.0, 0.0), (1.0, 0.0, 0.0)), 'blue':  ((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))}\n",
        "Blue_cmap = LinearSegmentedColormap('Blue_cmap', segmentdata=cdict_blue, N=256)\n",
        "# ------------------------------\n",
        "\n",
        "\n",
        "print(\"\\n--- CIFAR-10 クラスの例 ---\")\n",
        "\n",
        "# trainset_full から各クラスの最初の画像を収集\n",
        "class_images = {}\n",
        "for img, label in trainset_full:\n",
        "    if label not in class_images:\n",
        "        class_images[label] = img\n",
        "    if len(class_images) == 10:\n",
        "        break\n",
        "\n",
        "# 画像の正規化を元に戻す関数 (plt.axis('off') を削除)\n",
        "def imshow_example(img_tensor):\n",
        "    img = img_tensor / 2 + 0.5  # 正規化解除\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    # plt.axis('off') # <- これを削除し、呼び出し側で制御\n",
        "\n",
        "# 10クラスの画像を表示 (ソートして 0: plane, 1: car ... の順にする)\n",
        "plt.figure(figsize=(15, 3))\n",
        "sorted_labels = sorted(class_images.keys())\n",
        "for i in sorted_labels:\n",
        "    ax = plt.subplot(1, 10, i + 1)\n",
        "    imshow_example(class_images[i])\n",
        "    # タイトルの代わりに X軸ラベルをキャプションとして使用\n",
        "    ax.set_xlabel(f\"{i}: {classes[i]}\")\n",
        "    # 軸の目盛りを非表示にする\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# testset と classes は 3-1 で定義済みです\n",
        "\n",
        "# 画像の正規化を元に戻して表示する関数 (中身は読まなくてOK)\n",
        "def imshow_for_exercise(img_tensor):\n",
        "    try:\n",
        "        img = img_tensor / 2 + 0.5  # 正規化解除\n",
        "        npimg = img.numpy()\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"画像表示エラー: {e}\")\n",
        "\n",
        "# 取得したデータを表示する関数 (中身は読まなくてOK)\n",
        "def show_selected_data(selected_data):\n",
        "    try:\n",
        "        # 1. データをアンパック（画像とラベルに分離）\n",
        "        img_tensor, label_id = selected_data\n",
        "\n",
        "        # 2. ラベル番号をクラス名（文字列）に変換\n",
        "        class_name = classes[label_id]\n",
        "\n",
        "        # 3. 結果を表示\n",
        "        print(f\"--- 取得したデータを表示 ---\")\n",
        "        print(f\"  ラベル番号: {label_id}\")\n",
        "        print(f\"  クラス名: {class_name}\")\n",
        "\n",
        "        # 4. 画像を表示\n",
        "        imshow_for_exercise(img_tensor)\n",
        "\n",
        "    except NameError:\n",
        "        print(\"エラー: 'testset' または 'classes' が見つかりません。\")\n",
        "        print(\"このセルの前に「3-1. 準備コード」セルを実行してください。\")\n",
        "    except TypeError:\n",
        "         print(\"エラー: データが正しく取得できませんでした。\")\n",
        "         print(\"`testset[番号]` の形式を確認してください。\")\n",
        "    except Exception as e:\n",
        "        print(f\"エラーが発生しました: {e}\")\n",
        "\n",
        "max_epochs = 15 #@param {type:\"integer\"}\n",
        "learning_rate = 0.001 #@param {type:\"number\"}\n",
        "batch_size = 64 #@param {type:\"integer\"}\n",
        "\n",
        "# --- データローダーの作成 ---\n",
        "# 教師データ (学習中はバラバラに混ぜる shuffle=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "# 検証データ (混ぜる必要なし shuffle=False)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "# テストデータ (混ぜる必要なし shuffle=False)\n",
        "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "import torch.nn.functional as F # 確率に変換(softmax)するために必要\n",
        "\n",
        "# 3-2, 3-1, 6-1, 6-2 で定義済みのものを使用します\n",
        "# (show_selected_data, testset, classes, net_to_train, device)\n",
        "\n",
        "def analyze_image_prediction(index, target=testset):\n",
        "    \"\"\"\n",
        "    指定されたインデックスの画像を表示し、\n",
        "    net_to_train の予測確率と最終結果を表示する関数\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1. データの準備と正解の表示 ---\n",
        "    print(f\"--- インデックス {index} のデータを分析 ---\")\n",
        "\n",
        "    # 3-2で定義した関数 (show_selected_data) が必要\n",
        "    try:\n",
        "        # target からデータを取得\n",
        "        selected_data = target[index]\n",
        "        # 画像と正解ラベルを表示\n",
        "        show_selected_data(selected_data)\n",
        "\n",
        "        # ネットワーク入力用にテンソルを準備\n",
        "        img_tensor, label_id = selected_data\n",
        "        # (C, H, W) -> (B, C, H, W) に変換 (B=1) し、GPUへ\n",
        "        img_batch = img_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    except NameError as e:\n",
        "        print(\"NameError\", e)\n",
        "        return\n",
        "\n",
        "    # --- 2. AIによる予測の実行 ---\n",
        "    try:\n",
        "        net_to_train.eval() # 評価モードに設定\n",
        "\n",
        "        with torch.no_grad(): # 勾配計算をオフ\n",
        "            # ネットワークが10個の数値 (logits) を出力\n",
        "            logits = net_to_train(img_batch)\n",
        "\n",
        "            # 10個の数値を「確率」(合計100%)に変換 (Softmax)\n",
        "            # [0] はバッチ(1枚)からデータを取り出すため\n",
        "            probabilities = F.softmax(logits, dim=1)[0]\n",
        "\n",
        "            # 最も確率が高いクラスのインデックスを取得\n",
        "            _, predicted_index = torch.max(logits, 1)\n",
        "            predicted_index = predicted_index[0].item()\n",
        "\n",
        "        # --- 3. 結果の表示 ---\n",
        "        print(\"\\n--- AIによる予測結果 ---\")\n",
        "\n",
        "        # 10クラスすべての確率を表示\n",
        "        print(\"  [クラス別 確率]:\")\n",
        "        for i in range(len(classes)):\n",
        "            prob_percent = probabilities[i].item() * 100\n",
        "            # 正解ラベルと予測ラベルに目印を付ける\n",
        "            mark = \"\"\n",
        "            if i == label_id:\n",
        "                mark += \" (正解)\"\n",
        "            if i == predicted_index:\n",
        "                mark += \" <--- [AIの選択]\"\n",
        "\n",
        "            print(f\"    {classes[i]:<10} : {prob_percent:6.2f} % {mark}\")\n",
        "\n",
        "        # 最終的な答えを表示\n",
        "        predicted_class_name = classes[predicted_index]\n",
        "        true_class_name = classes[label_id]\n",
        "\n",
        "        color = \"✅\" if predicted_class_name == true_class_name else \"❌\"\n",
        "        print(f\"\\n  [AIの最終結論]: {predicted_class_name} {color}\")\n",
        "\n",
        "    except NameError:\n",
        "        print(\"エラー: 'net_to_train' または 'device' が見つかりません。\")\n",
        "        print(\"「6-1」「6-2」セルが実行されているか確認してください。\")\n",
        "    except Exception as e:\n",
        "        print(f\"予測中にエラーが発生しました: {e}\")\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1. 畳み込み層 (入力3ch, 出力6ch, カーネル5x5)\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        # 2. プーリング層 (2x2)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # 3. 畳み込み層 (入力6ch, 出力16ch, カーネル5x5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        # 4. 全結合層 (入力 16*5*5 = 400, 出力 120)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        # 5. 全結合層 (入力 120, 出力 84)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        # 6. 全結合層 (入力 84, 出力 10クラス)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    # 計算の流れ (forward)\n",
        "    def forward(self, x):\n",
        "        # 畳み込み1 -> ReLU活性化 -> プーリング\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        # 畳み込み2 -> ReLU活性化 -> プーリング\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "\n",
        "        # 平坦化 (1列の数値にする)\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "\n",
        "        # 全結合1 -> ReLU活性化\n",
        "        x = F.relu(self.fc1(x))\n",
        "        # 全結合2 -> ReLU活性化\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # 全結合3 (最終出力層)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "    # 特徴抽出用のメソッド (fc2の出力を取り出す)\n",
        "    def extract_features(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        features = F.relu(self.fc2(x)) # fc3の手前(84次元)\n",
        "        return features\n",
        "\n",
        "print(\"\\n準備完了。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🐍 3-1-1. (演習) Pythonの基本 (3)：len() とインデックス\n",
        "\n",
        "`3-1`のセルで、`train_dataset`（教師データ全体）を用意しました。\n",
        "「全部でいくつのペアが学習に使われるのか？」を調べるために、`len()` 関数でこのデータセットの「総数」を調べます。\n",
        "\n",
        "さらに、その総数を使って「**最後のデータ**」にアクセスしてみましょう。\n",
        "\n",
        "---\n",
        "### ⚙️ 演習3：総数を調べて、最後のデータを表示する\n",
        "\n",
        "この演習には2つのステップがあります。\n",
        "\n",
        "1.  **総数を調べる:**\n",
        "    `len( リストやデータセット )` を使うと、その総数を計算できます。\n",
        "2.  **最後のデータにアクセスする:**\n",
        "    Pythonのインデックスは **0番** から始まります。\n",
        "    もし総数が 40000 個なら、インデックスは `0, 1, 2, ...` と続き、最後のインデックスは `39999`（`総数 - 1`）になります。\n",
        "\n",
        "以下のコードセルの2つの課題（`0` となっている場所）を正しく書き換えて、`train_dataset` の総数（40000）を計算し、最後のデータ（39999番目）を表示させてください。"
      ],
      "metadata": {
        "id": "-TCemBhwATAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-1-1. 演習：データセットの総データ数と最後のデータの確認 { display-mode: \"code\" }\n",
        "\n",
        "#@markdown ### 課題1：`len()` 関数を使って総数を計算\n",
        "#@markdown 以下の `total_count = 0` の `0` の部分を、\n",
        "#@markdown `train_dataset` の総数を計算するコード（`len(train_dataset)`）に書き換えてください。\n",
        "\n",
        "print(\"--- 演習1：train_dataset の総データ数を計算 ---\")\n",
        "\n",
        "# 1. 以下の 999 を書き換えて、train_dataset の総数を total_count に代入します。\n",
        "total_count = 999\n",
        "\n",
        "# 2. 計算した総データ数を表示します。\n",
        "print(f\"train_dataset の総データ数: {total_count}\")\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 課題2：最後のデータを表示\n",
        "#@markdown `index_to_show` を、（上記で求めた総数 - 1） に変えてください。\n",
        "\n",
        "print(\"\\n--- 演習2：指定したインデックスのデータを表示 ---\")\n",
        "\n",
        "# 3. 以下の 0 を書き換えて、最後のインデックスを指定します。\n",
        "index_to_show = 999\n",
        "\n",
        "# 4. 指定したインデックスのデータを表示します\n",
        "print(f\"インデックス {index_to_show} のデータを表示します...\")\n",
        "\n",
        "show_selected_data(train_dataset[index_to_show])"
      ],
      "metadata": {
        "id": "keLaI3DkAXGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🖼️ 3-2. 画像とピクセル値\n",
        "\n",
        "AIが画像を「計算」するとはどういうことか、より具体的に理解するために、画像が「数値の集まり（ピクセル値）」であることを確認します。\n",
        "\n",
        "以下のセルは、CIFAR-10の画像（32x32ピクセル）を読み込み、スライダーで指定した「関心領域（ROI）」のピクセル値を、R (赤), G (緑), B (青) の各色チャンネルごとに表示するものです。\n",
        "\n",
        "* `image_index` で画像の番号を指定します。\n",
        "* `manual_x`, `manual_y` でROIの左上の座標を決めます。\n",
        "* `manual_width`, `manual_height` でROIのサイズを決めます。"
      ],
      "metadata": {
        "id": "fN5J-7viI1eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3-2. ピクセル値の可視化（CIFAR-10画像） { display-mode: \"code\" }\n",
        "#@markdown ### 画像インデックスとROIの指定\n",
        "image_index = 1441 #@param {type:\"slider\", min:0, max:9999, step:1}\n",
        "roi_x = 10 #@param {type:\"slider\", min:0, max:31, step:1}\n",
        "roi_y = 10 #@param {type:\"slider\", min:0, max:31, step:1}\n",
        "roi_size = 7 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "\n",
        "# --- CIFAR-10画像データの取得 ---\n",
        "# testsetは transform (正規化) 済みなので、ピクセル値確認用に\n",
        "# transformなしのデータセットを再ロード（ダウンロードはキャッシュされる）\n",
        "transform_vis = transforms.Compose([transforms.ToTensor()])\n",
        "testset_vis = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                           download=True, transform=transform_vis)\n",
        "\n",
        "# 0-9999 の範囲に補正\n",
        "image_index = max(0, min(image_index, len(testset_vis)-1))\n",
        "\n",
        "# (C, H, W) 形式のテンソルを取得\n",
        "img_tensor, label = testset_vis[image_index]\n",
        "\n",
        "# (H, W, C) 形式のNumpy配列に変換 (0-1の値)\n",
        "img_np = img_tensor.permute(1, 2, 0).numpy()\n",
        "# 0-255 の整数値に変換\n",
        "img_rgb = (img_np * 255).astype(np.uint8)\n",
        "\n",
        "# グレースケール変換\n",
        "gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# --- ROIの決定 ---\n",
        "final_x, final_y, final_w, final_h = roi_x, roi_y, roi_size, roi_size\n",
        "print(f\"CIFAR-10 画像 {image_index} ({classes[label]}) の ROI (X={final_x}, Y={final_y}, Size={roi_size}x{roi_size}) を表示します。\")\n",
        "\n",
        "# 座標が画像の範囲外にならないように補正\n",
        "final_x = max(0, min(final_x, img_rgb.shape[1] - final_w))\n",
        "final_y = max(0, min(final_y, img_rgb.shape[0] - final_h))\n",
        "# サイズが画像サイズを超えないように補正\n",
        "final_w = min(final_w, img_rgb.shape[1] - final_x)\n",
        "final_h = min(final_h, img_rgb.shape[0] - final_y)\n",
        "\n",
        "\n",
        "# 見つかった領域を矩形で囲む\n",
        "image_with_rect = img_rgb.copy()\n",
        "cv2.rectangle(image_with_rect, (final_x, final_y), (final_x + final_w, final_y + final_h), (0, 0, 255), 1)\n",
        "\n",
        "# 拡大する部分の座標\n",
        "roi_gray = gray[final_y:final_y+final_h, final_x:final_x+final_w]\n",
        "roi_r = img_rgb[final_y:final_y+final_h, final_x:final_x+final_w, 0]\n",
        "roi_g = img_rgb[final_y:final_y+final_h, final_x:final_x+final_w, 1]\n",
        "roi_b = img_rgb[final_y:final_y+final_h, final_x:final_x+final_w, 2]\n",
        "\n",
        "# --- グラフ描画 ---\n",
        "# 1. 全体像のプロット\n",
        "fig = plt.figure(figsize=(10, 4))\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.imshow(img_rgb)\n",
        "ax1.set_title(f\"Image {image_index} ({classes[label]})\")\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax2.imshow(image_with_rect)\n",
        "ax2.set_title(\"Region of Interest\")\n",
        "ax2.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# 2. ROIの詳細プロット\n",
        "fig, axes = plt.subplots(1, 4, figsize=(max(9, final_w*1.5), max(4, final_h*0.6)))\n",
        "show_channel_with_grid_and_values(axes[3], roi_b, \"Blue Channel (ROI)\", cmap=Blue_cmap)\n",
        "show_channel_with_grid_and_values(axes[2], roi_g, \"Green Channel (ROI)\", cmap=Green_cmap)\n",
        "show_channel_with_grid_and_values(axes[1], roi_r, \"Red Channel (ROI)\", cmap=Red_cmap)\n",
        "show_channel_with_grid_and_values(axes[0], roi_gray, \"Grayscale (ROI)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pnKc8QeGJGTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 4. AIの脳（CNNモデル）を設計する\n",
        "\n",
        "CNN（畳み込みニューラルネットワーク）の計算ルール（構造）を設計します。\n",
        "\n",
        "### 🌀 「畳み込み(Convolution)」とは？\n",
        "\n",
        "「畳み込み」とは、画像から「特徴」を抜き出すための計算です。\n",
        "虫眼鏡（**カーネル**または**フィルタ**と呼ばれる）で画像の一部を拡大し、その部分が「縦線」っぽいか、「丸」っぽいか、「赤」っぽいかなどを調べ、その結果（**特徴マップ**）を新しい画像として出力します。\n",
        "\n",
        "\n",
        "\n",
        "この虫眼鏡（カーネル）自体が、AIが学習によって自動で調整する「計算ルールの重み」です。学習が進むと、AIは「ネコを分類するには、この部分に注目する虫眼鏡が必要だ」ということを自動で見つけ出します。\n",
        "\n",
        "### 📜 モデルの構造\n",
        "\n",
        "今回は、以下のような流れで計算するモデルを設計します。\n",
        "（`->` は計算の流れを示します）\n",
        "\n",
        "1.  **入力** (32x32ピクセルのカラー画像)\n",
        "2.  **畳み込み 1 (conv1)**: 虫眼鏡で特徴を抜き出す\n",
        "3.  **プーリング 1 (pool)**: 画像を粗く（小さく）して、大まかな特徴だけ残す\n",
        "4.  **畳み込み 2 (conv2)**: もう一度、別の虫眼鏡で特徴を抜き出す\n",
        "5.  **プーリング 2 (pool)**: 再び画像を粗くする\n",
        "6.  **平坦化 (view)**: 2次元の画像データを、1列の数値データに変換する\n",
        "7.  **全結合 1 (fc1)**: 1列のデータを元に、より複雑な計算\n",
        "8.  **全結合 2 (fc2)**: さらにもう一段計算（ここで84個の数値＝**特徴ベクトル**が完成）\n",
        "9.  **全結合 3 (fc3)**: 84個の数値を、最終的な10クラスの「確率」に変換\n",
        "10. **出力** (10個の数値)"
      ],
      "metadata": {
        "id": "GY_bIoDbYCeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CNNモデルの定義と構造の表示（書き換えずに実行してください） { display-mode: \"code\" }\n",
        "\n",
        "# モデルの実体を作成し、GPU(device)に送る\n",
        "net_to_train = Net().to(device)\n",
        "\n",
        "# モデルの構造をテキストで出力\n",
        "print(\"--- AI (CNN) の構造 ---\")\n",
        "print(net_to_train)\n",
        "print(\"--------------------\")"
      ],
      "metadata": {
        "id": "zh16QO5nRwZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 5. 学習前のAIの実力（t-SNE可視化）\n",
        "\n",
        "今、AI（`net_before_learning`）は「生まれたて」の状態で、中身の計算ルール（重み）はランダム（でたらめ）です。\n",
        "この状態で画像を見せても、まともな特徴は取り出せません。\n",
        "\n",
        "「検証データ」を使って、この「生まれたてのAI」が画像をどう見ているか、t-SNEで可視化してみましょう。\n",
        "（t-SNEの計算には少し時間がかかります）"
      ],
      "metadata": {
        "id": "QKjdsSpEYHUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 学習前のt-SNE可視化 (実行に1〜2分かかります) { display-mode: \"code\" }\n",
        "\n",
        "# 検証データセット用のDataLoader\n",
        "validation_loader_vis = DataLoader(validation_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# 学習前の特徴抽出\n",
        "print(\"学習前のモデルで特徴を抽出中...\")\n",
        "features_before, labels_before = extract_features(net_to_train, validation_loader_vis, device)\n",
        "\n",
        "# t-SNE実行\n",
        "print(\"t-SNEで2次元に圧縮中...\")\n",
        "tsne_before = run_tsne(features_before)\n",
        "\n",
        "# 可視化\n",
        "print(\"可視化中...\")\n",
        "plot_tsne(tsne_before, labels_before, \"Feature Space (Before Learning)\")"
      ],
      "metadata": {
        "id": "M7vOVeOwYIhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔬 5-1. (演習) AIの「確率」を個別に確認する\n",
        "\n",
        "AIは具体的にどのような「確率」を出力しているのでしょうか？\n",
        "\n",
        "AI（`net_to_train`）の最終的な出力は、「10クラスそれぞれが、どのくらいの確率か」を示す10個の数値です。AIは、この中で最も確率が高いものを「答え」として選びます。\n",
        "\n",
        "---\n",
        "### ⚙️ 演習5-1-1：AI の出力を確認\n",
        "以下のセルは、`testset` のインデックス番号（`index_to_analyze`）を指定すると、\n",
        "1.  その画像と「正解（Ground-truth）」を表示し（`show_selected_data` を使用）、\n",
        "2.  AI（`net_to_train`）が計算した10クラスすべての「確率（Prediction）」\n",
        "3.  AIが選んだ「最終的な答え」\n",
        "を表示する関数（`analyze_image_prediction`）を定義し、実行します。\n",
        "\n",
        "`index_to_analyze = 10` の `10` の部分を、好きな数値（0〜9999）に変えて、AIが間違えやすい画像（例：`cat` と `dog`）などを探してみてください。"
      ],
      "metadata": {
        "id": "TQEyHvGsMr6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 5-1-1. 演習：個別の画像に対するAIの確率を表示 { display-mode: \"code\" }\n",
        "\n",
        "# 分析したいインデックスを指定\n",
        "# 以下の `index_to_analyze = 10` の `10` を変更して実行してください。\n",
        "\n",
        "index_to_analyze = 13\n",
        "\n",
        "# 定義した関数を実行\n",
        "analyze_image_prediction(index_to_analyze)"
      ],
      "metadata": {
        "id": "sfTjVYJTMyov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⚙️ 6. AIの学習：計算ルールを調整する\n",
        "\n",
        "AIの学習プロセスを設定します。\n",
        "\n",
        "### 🎛️ ハイパーパラメータの設定（フォーム）\n",
        "\n",
        "学習の「やり方」を決める重要な設定値（**ハイパーパラメータ**）を決めます。\n",
        "（Pythonが分からない方も、ここで数値を変更して学習の様子を変えられます）\n",
        "\n",
        "* **max_epochs:** 教師データを何回繰り返し学習するか（最大回数）。\n",
        "* **learning_rate (学習率):** 誤差をどれだけ強く計算ルールに反映するか。大きすぎると学習が不安定になり、小さすぎると時間がかかりすぎます。\n",
        "* **batch_size (バッチサイズ):** 一度に何枚の画像をまとめて処理するか。"
      ],
      "metadata": {
        "id": "iCbH3gutYPdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 6-2. 学習の実行：誤差逆伝播法\n",
        "\n",
        "いよいよ学習を実行します。AIは以下のサイクルを高速で繰り返します。\n",
        "\n",
        "1.  **入力:** 教師データをいくつか取り出す（バッチ処理）。\n",
        "2.  **順伝播 (計算):** モデル（AI）が画像を計算し、確率（予測）を出す。\n",
        "3.  **誤差の計算:** AIの予測と「正解」を比べる。\n",
        "    * **理想:** 正解クラス（例：ネコ）の確率が 1.0、他が 0.0。\n",
        "    * **予測:**「ネコ 0.6、イヌ 0.3、車 0.1 」...\n",
        "    * この **「ズレ」が「誤差（損失）」** です。\n",
        "4.  **誤差逆伝播法 (Backpropagation):**\n",
        "    計算された「誤差」を、計算ルールの調整に使うため、 **AI（モデル）の中を逆向きに伝播** させます。「この誤差は、さっきの計算（fc3）のせいだ」「じゃあ、その前のfc2のせいだ」...と遡っていきます。\n",
        "5.  **更新:**\n",
        "    逆伝播でわかった「誤差の原因」に基づき、計算ルール（重み）を **「誤差が少し小さくなる」方向にほんの少しだけ** 調整します（学習率 `learning_rate` がこの「少し」の量を決めます）。\n",
        "\n",
        "このサイクルを、教師データ全体で「1エポック (epoch)」と呼びます。今回はこれを `max_epochs` 回繰り返します。"
      ],
      "metadata": {
        "id": "E18tfSlsYS5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 6-2. 学習の実行 (数分かかります) { display-mode: \"code\" }\n",
        "\n",
        "# --- モデル、損失関数、最適化手法の定義 ---\n",
        "# 新しく「学習させるため」のモデルを用意\n",
        "\n",
        "\n",
        "# 損失関数 (誤差を計算するルール)\n",
        "# 「クロスエントロピー誤差」という、分類問題で一般的な計算尺を使います\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 最適化手法 (誤差をどう反映させるか)\n",
        "# 「Adam」という、効率よく計算ルールを調整してくれる手法を使います\n",
        "optimizer = optim.Adam(net_to_train.parameters(), lr=learning_rate)\n",
        "\n",
        "print(\"学習の準備完了。\")\n",
        "# 各エポックの損失と精度を記録するリスト\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "print(f\"学習を開始します... (最大{max_epochs}エポック)\")\n",
        "\n",
        "# tqdmを使って進捗バーを表示\n",
        "for epoch in tqdm(range(max_epochs), desc=\"Epochs\"):\n",
        "\n",
        "    # --- 1. 教師データでの学習 (Train) ---\n",
        "    net_to_train.train() # 学習モード\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # train_loader からバッチごとにデータを取り出す\n",
        "    for data in train_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # (1) 勾配をリセット (前の計算結果を消す)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # (2) 順伝播 (計算)\n",
        "        outputs = net_to_train(inputs)\n",
        "\n",
        "        # (3) 誤差の計算\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # (4) 誤差逆伝播\n",
        "        loss.backward()\n",
        "\n",
        "        # (5) 更新\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- 学習状況の記録 ---\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100 * correct_train / total_train\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "\n",
        "    # --- 2. 検証データでの評価 (Validation) ---\n",
        "    net_to_train.eval() # 評価モード\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad(): # 勾配計算をオフ\n",
        "        # validation_loader からバッチごとにデータを取り出す\n",
        "        for data in validation_loader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = net_to_train(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_loss = val_loss / len(validation_loader)\n",
        "    val_acc = 100 * correct_val / total_val\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    # エポックごとの結果を表示 (tqdmの進捗バーに表示)\n",
        "    tqdm.write(f\"Epoch [{epoch+1}/{max_epochs}] | \"\n",
        "             f\"Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}% | \"\n",
        "             f\"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"\\n学習が終了しました。\")\n",
        "\n",
        "# --- 学習曲線のプロット ---\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# 損失 (Loss)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss ')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# 精度 (Accuracy)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QQDjdz0zYURK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 可視化結果の考察（学習曲線）\n",
        "\n",
        "* **誤差 (Loss)**:\n",
        "    学習が進むにつれて、教師データ（青線）と検証データ（オレンジ線）の両方で、誤差（ズレ）が小さくなっていくのが分かります。AIが正解に近づいている証拠です。\n",
        "* **正解率 (Accuracy)**:\n",
        "    逆に、正解率はどんどん上がっていきます。\n",
        "\n",
        "もし、教師データの線だけが良くなり、検証データの線が悪化し始めたら、それは「教師データを丸暗記しすぎた（**過学習**）」状態です。検証データは、それを見抜くために使われます。"
      ],
      "metadata": {
        "id": "K3jIcvRMYkFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📊 7. 学習後のAIの実力\n",
        "\n",
        "さて、学習によって「賢く」なったAI（`net_to_train`）は、画像の「特徴」をうまく捉えられるようになったでしょうか？\n",
        "学習前と同じように、検証データを使ってt-SNEで可視化してみましょう。"
      ],
      "metadata": {
        "id": "uxUR577WYmSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 学習後のt-SNE可視化 (実行に1〜2分かかります) { display-mode: \"code\" }\n",
        "\n",
        "# 学習後の特徴抽出 (検証データローダーは6-1で定義した validation_loader を再利用)\n",
        "print(\"学習後のモデルで特徴を抽出中...\")\n",
        "features_after, labels_after = extract_features(net_to_train, validation_loader, device)\n",
        "\n",
        "# t-SNE実行\n",
        "print(\"t-SNEで2次元に圧縮中...\")\n",
        "tsne_after = run_tsne(features_after)\n",
        "\n",
        "# 可視化\n",
        "print(\"可視化中...\")\n",
        "plot_tsne(tsne_after, labels_after, \"Feature Space (After Learning)\")"
      ],
      "metadata": {
        "id": "jsGuJ5xHYkt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 可視化結果の考察（学習後）\n",
        "\n",
        "学習前のぐちゃぐちゃだったグラフと比べてみてください。\n",
        "**同じ色の点（同じクラス）が近くに集まり、異なる色の点が離れて、「クラスタ（塊）」が形成されている**のが分かります。\n",
        "\n",
        "これは、AIが学習によって、3072次元（32x32x3）の画像データを、**「クラスごとに分類しやすい」84次元の特徴空間**に変換する計算ルール（畳み込みカーネルや重み）を、自動で獲得したことを意味します。"
      ],
      "metadata": {
        "id": "ewgDtnt0Ypfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🗺️ 8. 最終テストと「決定境界」の可視化\n",
        "\n",
        "AIの学習は「分類しやすい特徴空間」を作ることでした。\n",
        "この空間（t-SNEで2次元にした空間）を使えば、クラスを分類する「仕切り線（**決定境界**）」を引くことができます。\n",
        "\n",
        "学習後のt-SNE空間（`tsne_after`）を「新しいデータ」とみなして、この2次元空間上でクラスの仕切り線を学習させ、可視化してみましょう。"
      ],
      "metadata": {
        "id": "HZ3G8qRSYrBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8-1. 決定境界の学習と可視化 (実行に1〜2分かかります) { display-mode: \"code\" }\n",
        "\n",
        "# t-SNEの2次元データ (X) とラベル (y)\n",
        "X_2d = tsne_after\n",
        "y_2d = labels_after\n",
        "\n",
        "# 決定境界を学習するための、別の単純なAI (MLPClassifier) を用意\n",
        "# (元のCNNとは別物です。2次元データを分類するためだけに使います)\n",
        "print(\"2次元空間で決定境界を学習中...\")\n",
        "mlp_2d = MLPClassifier(\n",
        "    hidden_layer_sizes=[50, 50],\n",
        "    max_iter=500,\n",
        "    random_state=42,\n",
        "    alpha=0.01,\n",
        "    batch_size=200,\n",
        "    early_stopping=True,\n",
        "    verbose=False\n",
        ")\n",
        "mlp_2d.fit(X_2d, y_2d)\n",
        "print(f\"2D 分類器の訓練データ精度: {mlp_2d.score(X_2d, y_2d):.4f}\")\n",
        "\n",
        "# 決定境界のプロット\n",
        "plot_decision_boundary(X_2d, y_2d, mlp_2d, \"Decision Boundaries (After Learning)\")"
      ],
      "metadata": {
        "id": "4KQhCCZJYsLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 可視化結果の考察（決定境界）\n",
        "\n",
        "背景の色が「仕切り線（決定境界）」です。\n",
        "AIが作った特徴空間（点）が、クラスごとにきれいに分かれているため、この2次元空間でもうまく仕切り線が引けていることがわかります。\n",
        "\n",
        "（t-SNEの特性上、クラスタ間の「距離」は必ずしも正確な意味を持ちませんが、「近さ」は意味を持ちます。そのため、いくつかのクラスが混ざって見える領域もありますが、全体としてはうまく分離できています。）\n",
        "\n",
        "### 💯 最終実力テスト（クラスごと）\n",
        "\n",
        "最後に、学習に一切使っていない「テストデータ」を使って、学習済みAI（`net_to_train`）の最終的な実力を評価します。\n",
        "全体の正解率だけでなく、どのクラスが得意で、どのクラスが苦手か、クラスごとの正解率も見てみましょう。"
      ],
      "metadata": {
        "id": "jJlRfJ3_YvaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8-2. 最終テスト（テストデータでの精度評価） { display-mode: \"code\" }\n",
        "\n",
        "net_to_train.eval() # 評価モード\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "\n",
        "# クラスごとの正解数をカウントする準備\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "with torch.no_grad():\n",
        "    # test_loader は 6-1 で定義済み\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # 計算\n",
        "        outputs = net_to_train(images)\n",
        "\n",
        "        # 確率が最大のクラスを予測結果とする\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # 全体の正解率を計算\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "        # クラスごとの正解率を計算\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(len(labels)): # バッチサイズ（通常64）でループ\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "# --- 全体の正解率を表示 ---\n",
        "final_accuracy = 100 * correct_test / total_test\n",
        "print(f\"--- 最終実力テスト結果 ---\")\n",
        "print(f\"テストデータ {total_test} 件に対する全体の正解率: {final_accuracy:.2f} %\")\n",
        "print(\"--------------------\")\n",
        "\n",
        "# --- クラスごとの正解率を表示 ---\n",
        "print(\"クラスごとの正解率:\")\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print(f'  {classes[i]:<10} : {100 * class_correct[i] / class_total[i]:.2f} %')\n",
        "    else:\n",
        "        print(f'  {classes[i]:<10} : データなし')"
      ],
      "metadata": {
        "id": "T1qbxAoeMzDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🖼️ 8-3. 個別の推定結果の確認\n",
        "\n",
        "テスト画像のうち、いくつかのサンプルを実際に表示して、AIの「正解」と「推定」がどうなっているか確認してみましょう。\n",
        "（実行するたびに、別の画像が表示されます）"
      ],
      "metadata": {
        "id": "vph8iIITM2xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8-3. 個別の推定結果を表示 { display-mode: \"code\" }\n",
        "\n",
        "# test_loader をシャッフルして再作成（毎回違う画像を見るため）\n",
        "vis_loader = DataLoader(testset, batch_size=8, shuffle=True)\n",
        "\n",
        "# 1バッチ分のデータを取得\n",
        "images, labels = next(iter(vis_loader))\n",
        "images_gpu, labels_gpu = images.to(device), labels.to(device)\n",
        "\n",
        "# 推定\n",
        "net_to_train.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = net_to_train(images_gpu)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# CPUに戻す\n",
        "predicted = predicted.cpu()\n",
        "\n",
        "# 画像の正規化を元に戻す（表示のため）\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # 正規化解除 ( (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) )\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "\n",
        "# 結果の表示\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(8):\n",
        "    ax = plt.subplot(2, 4, i + 1)\n",
        "    imshow(images[i])\n",
        "\n",
        "    # 正解と推定のラベル\n",
        "    true_label = classes[labels[i]]\n",
        "    pred_label = classes[predicted[i]]\n",
        "\n",
        "    # 色分け\n",
        "    color = \"green\" if true_label == pred_label else \"red\"\n",
        "\n",
        "    ax.set_title(f\"Ground-truth: {true_label}\\nPrediction: {pred_label}\", color=color)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bAP3XnT_M3uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 8-3-1. 演習：個別の画像に対するAIの確率を表示 { display-mode: \"code\" }\n",
        "\n",
        "# 分析したいインデックスを指定\n",
        "# 以下の `index_to_analyze = 10` の `10` を `3` に変更して実行してください。\n",
        "#\n",
        "# （余裕のある方は） `for` を使って、0, 1, 2, 3, 4 番の5枚について実行してください。\n",
        "\n",
        "index_to_analyze = 3\n",
        "\n",
        "# 定義した関数を実行\n",
        "analyze_image_prediction(index_to_analyze)"
      ],
      "metadata": {
        "id": "VzpzwfwAO5Mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏁 9. まとめ\n",
        "\n",
        "このノートブックでは、AI（CNN）の学習と分類のプロセスを可視化しました。\n",
        "\n",
        "1.  **AIは「計算」:**\n",
        "    AIは高次元の「数値（画像）」を、計算ルール（モデル）に従って処理し、低次元の「数値（確率や特徴）」に変換する計算機です。\n",
        "2.  **学習とは「計算ルールの調整」:**\n",
        "    学習前のAI（ランダムなルール）では、特徴空間はぐちゃぐちゃでした。\n",
        "3.  **教師あり学習と誤差:**\n",
        "    「正解」と「予測」の「ズレ（誤差）」を計算し、その誤差が小さくなるように計算ルールを少しずつ調整（**誤差逆伝播法**）しました。\n",
        "4.  **CNNと特徴空間:**\n",
        "    学習後のAIは、画像から「分類に役立つ特徴（特徴ベクトル）」を自動で抽出し、クラスごとに集まった「クラスタ」を形成できるようになりました。\n",
        "5.  **分類とは「確率の最大化」:**\n",
        "    AIは、入力された画像が、学習によって作られた特徴空間の「どのクラスタ（クラス）に最も近いか」を「確率」として計算し、最も高い確率のクラスを「答え」として出力します。\n",
        "\n",
        "AIの「学習」とは、このように、入力データを「**答えやすい（分類しやすい）形に変換する計算ルール**」を、大量のデータと誤差のフィードバックによって自動で見つけ出すプロセスである、と視覚的に理解できます。"
      ],
      "metadata": {
        "id": "aUtqFh7aY0vY"
      }
    }
  ]
}
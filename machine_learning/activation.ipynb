{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmKaU2bkY+tHm+DpgSXjEu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/activation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 活性化関数\n",
        "\n",
        "`ReLU` のようなものがコード中に書かれているのを見かけていることあると思います。これは何のためにあるのでしょう？\n",
        "\n",
        "これらは活性化関数 (activation function) といい、ニューラルネットワークにおいて **非線形性** を導入するために使用される関数です。活性化関数を使うことで、モデルが入力と出力の間に存在する複雑な関係を学習することが可能になります。\n",
        "\n",
        "一般的な活性化関数には以下のものがあります：\n",
        "\n",
        "- **ReLU（Rectified Linear Unit）**  \n",
        " $$\n",
        "  f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "- **Sigmoid**  \n",
        "$$\n",
        "  f(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "- **tanh**  \n",
        "$$\n",
        "  f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "どんな関数なのか見てみましょう。\n",
        "**Sigmoid と tanh は自分で実装してください**"
      ],
      "metadata": {
        "id": "dr0cq7hoHDi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# x軸の値を生成\n",
        "x = np.linspace(-3, 3, 100)\n",
        "\n",
        "# 活性化関数の定義\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return np.zeros(len(x)) # 自分で実装してね\n",
        "\n",
        "def tanh(x):\n",
        "    return np.zeros(len(x)) # 自分で実装してね\n",
        "\n",
        "# 活性化関数の出力を計算\n",
        "y_relu = relu(x)\n",
        "y_sigmoid = sigmoid(x)\n",
        "y_tanh = tanh(x)\n",
        "\n",
        "# グラフの描画\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, y_relu, label=\"ReLU\", color=\"blue\", linewidth=2)\n",
        "plt.plot(x, y_sigmoid, label=\"Sigmoid\", color=\"green\", linewidth=2)\n",
        "plt.plot(x, y_tanh, label=\"Tanh\", color=\"red\", linewidth=2)\n",
        "\n",
        "# 軸範囲の設定\n",
        "plt.xlim(-3, 3)\n",
        "plt.ylim(-3, 3)\n",
        "\n",
        "# グリッド、軸、タイトル\n",
        "plt.axhline(0, color='black', linewidth=0.5, linestyle=\"--\")\n",
        "plt.axvline(0, color='black', linewidth=0.5, linestyle=\"--\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.title(\"Activation Functions\", fontsize=14)\n",
        "plt.xlabel(\"Input (x)\", fontsize=12)\n",
        "plt.ylabel(\"Output (f(x))\", fontsize=12)\n",
        "\n",
        "# 凡例\n",
        "plt.legend(fontsize=10)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C8NRNPu0JjVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 活性化関数の必要性\n",
        "\n",
        "活性化関数がない場合、ニューラルネットワークは線形変換しか行えなくなります。つまり、各層で行われる処理がすべて線形であるため、層をどれだけ重ねても最終的には1回の線形変換で表現できてしまいます。すると\n",
        "\n",
        "- 複雑なデータ（非線形なデータパターン）を学習できない\n",
        "- 画像分類や音声認識、自然言語処理のような高度なタスクに対応できない\n",
        "\n",
        "といったことになります。\n",
        "\n",
        "## 活性化関数を挿入する箇所\n",
        "\n",
        "活性化関数は、各隠れ層の **線形変換（全結合層や畳み込み層）** の直後に挿入するのが一般的です。\n",
        "\n",
        "- 入力 → 全結合層（線形変換） → 活性化関数 → 次の層\n",
        "- 入力 → 畳み込み層（線形変換） → 活性化関数 → プーリング層"
      ],
      "metadata": {
        "id": "XOOC7IyJHonv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 線形と非線形\n",
        "\n",
        "#### 線形とは？\n",
        "線形とは、データの変換が一次方程式（直線または平面）で表現できることを意味します。  \n",
        "数学的には、以下の形の方程式で表されます：\n",
        "\n",
        "$$\n",
        "y = {\\mathbf W} {\\mathbf x} + {\\mathbf b}\n",
        "$$\n",
        "\n",
        "ここで：\n",
        "- $ {\\mathbf W} $ は重み（係数）行列\n",
        "- $ {\\mathbf x} $ は入力データ\n",
        "- $ {\\mathbf b} $ はバイアス項（定数）\n",
        "\n",
        "線形モデルの特徴：\n",
        "1. データが直線や平面で分離可能な場合に適している。\n",
        "2. モデルがシンプルで計算効率が高い。\n",
        "3. しかし、直線的なデータの関係しか学習できない。\n",
        "\n",
        "線形モデルの限界：\n",
        "\n",
        "たとえば、クラスAとクラスBが円形状に分布している場合、直線では分けられません（非線形な分離が必要）。このような場合、線形モデルでは対応できず、非線形性が必要となります。\n",
        "\n",
        "\n",
        "#### 非線形とは？\n",
        "\n",
        "非線形とは、データの変換が一次方程式では表現できない、より複雑な関係を持つことを意味します。  \n",
        "数学的には、次のような関数が含まれる場合を指します：\n",
        "\n",
        "$$\n",
        "y = f({\\mathbf W} {\\mathbf x} + {\\mathbf b})\n",
        "$$\n",
        "\n",
        "ここで $ f $ は活性化関数（非線形関数）です。\n",
        "\n",
        "非線形モデルの特徴：\n",
        "1. 複雑なデータ構造を学習可能。\n",
        "2. データの分離が直線では難しい場合に適用可能。\n",
        "3. 層を増やすことで高度な特徴を抽出できる。\n"
      ],
      "metadata": {
        "id": "YjDxqx1TJkmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "### 課題：$y=x^3$ の近似\n",
        "\n",
        "単純な非線形関数 $$y=x^3$$ をニューラルネットワークで近似する回帰問題です。\n",
        "\n",
        "#### 実験の流れ\n",
        "\n",
        "- データの生成\n",
        "\n",
        "入力データ $x$ を [-1, 1] の範囲で均等に分布する100点とする。\n",
        "出力データ $y$ は $y=x^3$ を用いて計算する。\n",
        "\n",
        "- モデルの構築\n",
        "\n",
        "活性化関数を使わない線形モデルは実装済みです。これを用いて結果を確認します。<br />\n",
        "活性化関数（ReLU）を使った非線形モデルは、以下のコードを参考に自分で実装してください。関連する行はコメントアウトされているので、外して使用してください。\n",
        "\n",
        "- トレーニング\n",
        "\n",
        "それぞれのモデルを 10,000 エポックトレーニングする。\n",
        "\n",
        "- 結果の可視化\n",
        "\n",
        "活性化関数がある場合とない場合の出力を比較し、真の関数 $y=x^3$ にどれだけ近いかを確認する。"
      ],
      "metadata": {
        "id": "9CnyzKgAH5sE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2BdgIdsFbR2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データの生成\n",
        "x = np.linspace(-1, 1, 100).reshape(-1, 1)  # 入力 (100点のデータ)\n",
        "y = x ** 3  # 出力 y = x^3\n",
        "\n",
        "# PyTorchのテンソルに変換\n",
        "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# データの可視化\n",
        "plt.scatter(x, y, label='True function')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-55QiuZwFeHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearModel, self).__init__()\n",
        "        self.linear = nn.Linear(1, 100)  # 1次元入力 -> 100次元隠れ層\n",
        "        self.output = nn.Linear(100, 1)  # 100次元 -> 1次元出力\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)  # 線形変換のみ\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-zbce_SwFf8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ここに NonLinearModel クラスを自分で実装してください\n",
        "\n"
      ],
      "metadata": {
        "id": "jdUurDm_Fgx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル初期化\n",
        "linear_model = LinearModel()\n",
        "# nonlinear_model = NonLinearModel()\n",
        "\n",
        "# 損失関数と最適化アルゴリズム\n",
        "criterion = nn.MSELoss()  # 平均二乗誤差\n",
        "optimizer_linear = optim.SGD(linear_model.parameters(), lr=0.01)\n",
        "# optimizer_nonlinear = optim.SGD(nonlinear_model.parameters(), lr=0.01)\n",
        "\n",
        "# トレーニング関数\n",
        "def train_model(model, optimizer, epochs=10000):\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(x_tensor)\n",
        "        loss = criterion(predictions, y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# トレーニング実行\n",
        "train_model(linear_model, optimizer_linear)\n",
        "# train_model(nonlinear_model, optimizer_nonlinear)\n"
      ],
      "metadata": {
        "id": "AyvcieGeFjJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの予測\n",
        "linear_predictions = linear_model(x_tensor).detach().numpy()\n",
        "# nonlinear_predictions = nonlinear_model(x_tensor).detach().numpy()\n",
        "\n",
        "# 結果をプロット\n",
        "plt.scatter(x, y, label='True function', color='lightblue', alpha=0.8)  # Thin light blue scatter\n",
        "plt.plot(x, linear_predictions, label='Linear model (no activation)', color='red', linewidth=2)  # Red line, thicker\n",
        "# plt.plot(x, nonlinear_predictions, label='Nonlinear model (with ReLU)', color='green', linewidth=2)  # Green line, thicker\n",
        "plt.legend()\n",
        "plt.title(\"Comparison of Linear and Nonlinear Models\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ppmLpzC1FkLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 追加の課題\n",
        "\n",
        "ReLU だけでなく、別の活性化関数に差し替えて試してみてください。特にReLU の亜種にあたるものを自分で探してください。実装方法については PyTorch のドキュメントを参照してください。"
      ],
      "metadata": {
        "id": "dnWV70HoNhIE"
      }
    }
  ]
}
# 自然言語処理 (NLP)

画像処理においても、近年の深層学習では Transformer という技術が広く用いられています。
Transformer はもともと、コンピュータにおいて人間の言葉を扱う自然言語処理 (natural language processing; NLP) の世界で登場した技術です。
自然言語処理の分野を軽く知っておくことが、Transformer のイメージを理解する近道となります。

## 自然言語処理のタスク

この分野で具体的にどのようなタスクが扱われているのか見ていきましょう。これらのタスクは、言語の多義性、[長距離依存関係](#長距離依存関係の捉え方)の捉え方など、いくつか共通の課題をもっています。

#### テキスト生成
正確かつ文章として自然なテキスト生成することです。たとえばChatGPTのようなモデルが行うテキスト生成では、入力されたプロンプトに対して意味のある、文脈に沿った応答を生成する必要があります。

#### 翻訳
言語間でのテキストの翻訳では、文脈の正確な理解と、異なる言語間の文法やニュアンスの違いを適切に扱う能力が求められます。
長距離依存関係の理解が特に重要です。

#### 要約
テキストを要約する際には、長い文書の主要なポイントを抽出し、簡潔にまとめる能力が必要です。
文脈全体を理解し、重要な情報を選別することが課題となります。

#### 感情分析
テキストに含まれる感情や意見を識別するには、細かいニュアンスや文脈の理解が必要です。
単語の使い方や文脈が感情のポジティブさやネガティブさに大きく影響を与えるため、これらを正確に捉えることが課題です。

#### 質問応答
特定の質問に対して正確な答えを提供するには、質問の意図と関連する情報源の文脈を深く理解する必要があります。
ここでは、関連する情報を特定し、それに基づいて適切な回答を生成する能力が求められます。

## 従来の技術

#### RNN (Recurrent Neural Networks)
文や音声データのようなシーケンシャルなデータ(時系列または順序立ったデータ)を処理するために設計されました。ただし、長距離の依存関係を捉えるのが得意ではありません。

#### LSTM (Long Short-Term Memory)
RNNの問題を解決するために導入されました。長期記憶と短期記憶の概念を用いて、長距離の依存関係をより効果的に捉えます。

#### GRU (Gated Recurrent Unit)
LSTM に似ていますが、よりシンプルな構造を持ち、計算効率が向上しています。

## 従来の技術の限界

RNN、LSTM、GRUといった従来の技術は、自然言語処理の多くの基本的なタスクに貢献しました。これらのモデルは、テキストのシーケンシャルな性質を捉えるために設計され、文脈を通じて情報を伝達する能力を持っています。しかし、これらの技術には様々な根本的な課題があります。ここでは、特に重要な課題として、長距離依存関係の同定の難しさについて説明します。

#### 長距離依存関係とその難しさ

長距離依存関係とは、前に出てきた文章と、そこから離れたところで登場する文章の依存関係を指します。

> 涼太はピザを注文したが、彼はチーズが苦手なので、トッピングにはトマトとペパロニを選んだ。彼が食べるとき、...

例えば上記の文章において、前半の文章に出てきた「涼太」が後半の文章の「彼」を指すことが、長距離依存関係に当たります。また、「涼太」と「彼」が同じ意味を持つと判断することを、長距離依存関係の同定といいます。

上記の文章で、「彼」という代名詞が「涼太」を指していることは、人間にとっては直感的に理解できます。しかし、RNNやLSTM、GRUを用いたモデルでは、テキストが長くなるにつれて、「彼」が文の初めの「涼太」を指しているという情報を保持し続けることが難しくなります。特に、「彼が食べるとき」という部分に至ると、モデルが「彼」が誰であるか、そして「チーズが苦手」であるという情報を正確に関連付けることはさらに困難になります。

このように、RNNやその派生形であるLSTM、GRUでも、非常に長いテキストにおける情報の伝達は困難で、長距離依存関係を効果的にモデル化することが難しいです。たとえば、小説や研究論文のような長い文書では、文の初めと終わりで言及される概念が密接に関連していることがよくあります。従来のモデルでは、これらの関連性を捉えることができず、結果として文脈の理解が不完全になりがちです。

## Attention 機構

**Attention 機構**は、情報処理の際に特定のデータポイントに「注目を当てる」技術です。これは、モデルが入力データの重要な部分を識別し、それに基づいて出力を生成するのに役立ちます。Attention 機構は、モデルがデータのどの部分に注目すべきかを学習することを可能にします。

「注目を当てる」というのは、モデルが入力シーケンス内の特定の情報に対して重点的に注目し、その情報の重要性を評価することを意味します。

> 涼太はピザを注文したが、彼はチーズが苦手なので、トッピングにはトマトとペパロニを選んだ。彼が食べるとき、...

上記の例では、モデルは文脈内で重要な要素である「涼太」とそれに関連する情報である「彼」の間の関係を特定し、適切な文脈理解を行うことが可能になります。

#### Self-Attention

**Self-Attention** は、Attentionメカニズムの一種で、特にTransformerモデル内で用いられる技術です。Self-Attention は、入力シーケンス内の各要素が、自身を含むシーケンス内の他の全ての要素との関連性を評価するプロセスです。このプロセスを通じて、モデルは文脈内での各要素の位置づけや重要性を把握し、文全体の意味をより深く理解することができます。

> 涼太はピザを注文したが、彼はチーズが苦手なので、トッピングにはトマトとペパロニを選んだ。彼が食べるとき、...

という文で、後半部分の「彼」が誰を指しているのかを理解するには、「涼太」と「彼」の間の関係を正確に捉える必要があります。文脈内で「涼太」と「彼」が同一人物を指す、ということを理解するのが目標です。ここでの「間の関係」とは、単語「涼太」と代名詞「彼」が同じ実体を指しているという関連性を指します。

<!--
#### Google 検索との対比

Self-Attention や Transformer を理解するうえで、Query クエリ / Key キー / Value バリュー という概念を理解するのが重要です。身近な Google 検索と対比しながら理解していきましょう。*本来 Google 検索ではこれらの語は用いませんが、Transformer の概念にあてはめて考えてみます。*
- **Query**: 利用者が検索バーに入力する検索語。利用者が知りたい情報や回答を求めるための問い合わせです。
- **Key**: Googleが把握するインターネット上の文書のメタデータやコンテンツのキーワードに相当します。これらは、利用者のクエリに対する回答の候補となる情報を示す指標です。
- **Value**: インターネット上の文書そのものが「値」に相当します。クエリに対して関連性が高いと評価された文書は、利用者に返される結果（情報）となります。

これを自然言語処理での Self-Attention に置き換えてみます。
-->

#### テキストの理解における Query, Key, Value

Self-Attention や Transformer を理解するうえで、Query クエリ / Key キー / Value バリュー という概念を理解するのが重要です。Attentionメカニズムにおいては、文中の各単語がQuery、Key、Valueの役割を持ちます。これらの要素を用いて、文中の単語間の関連性を計算し、文脈に基づいた情報の統合を行います。

- **Query**: 文中のある単語が、他の単語との関連性を評価するための「問い合わせ」となります。
- **Key**: 文中の他の単語が持つ「キーワード」や特徴で、Queryの単語との関連性の計算に用いられます。
- **Value**: Keyに対応する単語の情報で、Queryの単語と関連性が高いと評価された場合、その情報が統合されて新しい文脈の理解に寄与します。

> 昨日、公園で本を読んでいたら、友達に会った。彼は明日の試験のために勉強していると言っていた。

上記の文章の例では、Query/Key/Valueは以下のようになります。

- Query: 「彼」をQueryとします。モデルはこの代名詞が指す対象を特定しようと「問い合わせ」を行います。
- Key: 「友達」や「公園」など、文中の名詞や「昨日」のような時制を表す単語がKeyの役割を果たします。これらは、「彼」が指す対象の特定において潜在的に関連する文脈の手がかりとなります。
- Value: 「友達に会った」という情報がValueとなります。この情報は、「彼」が指す対象の特定において、「彼」が「友達」を指しているという文脈を提供します。

#### QueryからKey, Valueを得る方法

テキスト内の各単語（Query）が文内の他のすべての単語（Key）とどのように関連しているかを計算し、その結果に基づいて最も関連性の高い情報（Value）を集約する必要があります。実際には、以下の流れを通じて、文脈に基づく情報の統合が行われ、テキストの深い理解が可能になります。

1. **QueryとKeyの比較**: モデルは文中の各単語（Query）を取り出し、それを文内の他のすべての単語（Key）と比較します。この比較は、通常、内積や他の類似度計算方法によって行われます。

1. **Attentionスコアの計算**: Queryと各Keyの間の比較結果に基づき、**Attentionスコア**が計算されます。Attentionスコアは、Queryが各Keyにどれだけ「注目」しているかを示します。

1. **Valueの重み付けと集約**: 各Keyに紐づくValueは、対応するAttentionスコアによって重み付けされます。そして、これらの重み付けされたValueが集約され、Queryに対する新しい文脈上の情報が生成されます。

各Queryに対して最も関連性が高いと評価されたValueの情報が強調され、文内の単語間の関係性や文脈が反映された新しい単語の表現が得られます。これにより、テキストの意味を深く理解することができます。

#### 備考

Attentionメカニズムには、Self-Attention以外にも様々な形式が存在しますが、Query, Key, Valueという概念は、特にSelf-Attentionとその応用であるTransformerモデルにおいて中心的な役割を果たします。他のAttention形式では、同じ概念が異なる形で使用される場合もあります。

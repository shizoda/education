# 自然言語処理 (NLP)

画像処理においても、近年の深層学習では Transformer という技術が広く用いられています。
Transformer はもともと、コンピュータにおいて人間の言葉を扱う自然言語処理 (natural language processing; NLP) の世界で登場した技術です。
自然言語処理の分野を軽く知っておくことが、Transformer のイメージを理解する近道となります。

## 自然言語処理のタスク

この分野で具体的にどのようなタスクが扱われているのか見ていきましょう。これらのタスクは、言語の多義性、文脈の理解、長距離依存関係の捉え方など、いくつか共通の課題をもっています。

#### テキスト生成
正確かつ文章として自然なテキスト生成することです。たとえばChatGPTのようなモデルが行うテキスト生成では、入力されたプロンプトに対して意味のある、文脈に沿った応答を生成する必要があります。

#### 翻訳
言語間でのテキストの翻訳では、文脈の正確な理解と、異なる言語間の文法やニュアンスの違いを適切に扱う能力が求められます。
長距離依存関係の理解が特に重要です。

#### 要約
テキストを要約する際には、長い文書の主要なポイントを抽出し、簡潔にまとめる能力が必要です。
文脈全体を理解し、重要な情報を選別することが課題となります。

#### 感情分析
テキストに含まれる感情や意見を識別するには、細かいニュアンスや文脈の理解が必要です。
単語の使い方や文脈が感情のポジティブさやネガティブさに大きく影響を与えるため、これらを正確に捉えることが課題です。

#### 質問応答
特定の質問に対して正確な答えを提供するには、質問の意図と関連する情報源の文脈を深く理解する必要があります。
ここでは、関連する情報を特定し、それに基づいて適切な回答を生成する能力が求められます。

## 従来の技術

#### RNN (Recurrent Neural Networks)
文や音声データのようなシーケンシャルなデータを処理するために設計されました。ただし、長距離の依存関係を捉えるのが得意ではありません。

#### LSTM (Long Short-Term Memory)
RNNの問題を解決するために導入されました。長期記憶と短期記憶の概念を用いて、長距離の依存関係をより効果的に捉えます。

#### GRU (Gated Recurrent Unit)
LSTM に似ているが、よりシンプルな構造を持ち、計算効率が向上しています。

## 従来の技術の限界

RNN、LSTM、GRUといった従来の技術は、自然言語処理の多くの基本的なタスクに貢献しました。これらのモデルは、テキストのシーケンシャルな性質を捉えるために設計され、文脈を通じて情報を伝達する能力を持っています。しかし、これらの技術には根本的な課題があります。

#### 長距離依存関係の捉え方

RNNやその派生形であるLSTM、GRUでも、非常に長いテキストにおける情報の伝達は困難で、長距離依存関係を効果的にモデル化することが難しいです。たとえば、小説や研究論文のような長い文書では、文の初めと終わりで言及される概念が密接に関連していることがよくあります。従来のモデルでは、これらの関連性を捉えることができず、結果として文脈の理解が不完全になりがちです。

> 涼太はピザを注文したが、彼はチーズが苦手なので、トッピングにはトマトとペパロニを選んだ。彼が食べるとき、...

この文で、「彼」という代名詞が「涼太」を指していることは、人間にとっては直感的に理解できます。しかし、RNNやLSTM、GRUを用いたモデルでは、テキストが長くなるにつれて、「彼」が文の初めの「涼太」を指しているという情報を保持し続けることが難しくなります。特に、「彼が食べるとき」という部分に至ると、モデルが「彼」が誰であるか、そして「チーズが苦手」であるという情報を正確に関連付けることはさらに困難になります。

#### 文脈の理解

単語ひとつひとつの意味は、その使用されている文脈によって大きく変わることがあります。この文脈に基づく意味の変化を適切に理解し、処理できるかどうかが、NLPシステムの性能を大きく左右します。しかし、RNNやLSTM、GRUといった従来のシーケンシャル処理モデルでは、テキストが長い場合や複雑な文脈が絡む場合に、このような文脈の変化を捉えきれないことがあります。

従来のモデルは、テキストを順次処理していく過程で、情報を一定の「記憶」に圧縮して伝達する必要があります。その結果、入力シーケンスの先頭にある情報が、シーケンスの後方で適切に利用されることが難しくなります。特に、文脈によって意味が大きく変わる単語の処理において、これが顕著になります。

> 昨日、公園で本を読んでいたら、友達に会った。彼は明日の試験のために勉強していると言っていた。

「彼」という代名詞が誰を指しているかを理解する必要があります。人間であれば、この「彼」が直前に言及された「友達」を指していることを容易に理解できますが、RNNやLSTM、GRUを用いた従来のモデルでは、このような代名詞の参照関係を正確に解決することが困難です。特に、複数の人物が登場する長い物語や会話の文脈では、どの代名詞がどの名詞を指しているかを追跡することが非常に難しくなります。

この問題は、テキスト内で言及される様々なエンティティ間の関係性を適切にモデル化し、長いシーケンスを通じてそれらの関係性を維持する能力が不足しているために起こります。従来のモデルでは、特にシーケンスが長くなると、初期の部分で処理した情報が後の部分で十分に活用されず、文脈の理解が不完全になる傾向があります。

<!-- 
#### 計算効率の問題

RNN系のモデルはシーケンスの各要素を順番に処理するため、計算に時間がかかり、並列化が困難です。この計算の遅さは、大規模なデータセットやリアルタイムアプリケーションでの使用を制限する一因となっていました。
-->

## Attention -- 焦点を当てる

Attention機構における「焦点を当てる」というプロセスは、モデルが入力シーケンス内の特定の情報に対して重点的に注目し、その情報の重要性を評価することを意味します。このプロセスを通じて、モデルは文脈内で重要な要素（この例では「涼太」）とそれに関連する情報（「彼」が指す対象）の間の関係を特定し、適切な文脈理解を行うことが可能になります。

> 涼太はピザを注文したが、彼はチーズが苦手なので、トッピングにはトマトとペパロニを選んだ

という文において、後半部分の「彼」が誰を指しているのかを理解するためには、「涼太」と「彼」の間の関係を正確に捉える必要があります。文脈内で「涼太」と「彼」が同一人物を指す、ということを理解するのが目標です。ここでの「間の関係」とは、単語「涼太」と代名詞「彼」が同じ実体を指しているという関連性を指します。


#### Google 検索との対比

Attention や Transformer では、Query クエリ / Key キー / Value バリュー という概念を理解するのが重要です。身近な Google 検索と対比しながら理解していきましょう。*本来 Google 検索ではこれらの語は用いませんが、Transformer の概念にあてはめて考えてみます。*
- **Query**: 利用者が検索バーに入力する検索語。利用者が知りたい情報や回答を求めるための問い合わせです。
- **Key**: Googleが把握するインターネット上の文書のメタデータやコンテンツのキーワードに相当します。これらは、利用者のクエリに対する回答の候補となる情報を示す指標です。
- **Value**: インターネット上の文書そのものが「値」に相当します。クエリに対して関連性が高いと評価された文書は、利用者に返される結果（情報）となります。

これを自然言語処理、特にTransformerモデルの話題に置き換えてみます。

#### Transformer モデルへ

TransformerモデルのAttentionメカニズムにおいては、文中の各単語がQuery、Key、Valueの役割を持ちます。これらの要素を用いて、文中の単語間の関連性を計算し、文脈に基づいた情報の統合を行います。

- **Query**: 文中のある単語が、他の単語との関連性を評価するための「問い合わせ」となります。
- **Key**: 文中の他の単語が持つ「キーワード」や特徴で、Queryの単語との関連性の計算に用いられます。
- **Value**: Keyに対応する単語の情報で、Queryの単語と関連性が高いと評価された場合、その情報が統合されて新しい文脈の理解に寄与します。

#### 例文より

> 昨日、公園で本を読んでいたら、友達に会った。彼は明日の試験のために勉強していると言っていた。

- Query: 「彼」をQueryとします。この単語に対して、モデルは「彼は誰を指しているのか？」という問い合わせを行います。
- Key: 文中の各単語、特に「涼太」がKeyに相当します。モデルは「彼」を指している可能性がある文中の単語やフレーズをKeyとして扱い、Queryとの関連性を評価します。
- Value: 「涼太」に関連する情報、つまり「涼太はピザを注文したが、チーズが苦手なので、トッピングにはトマトとペパロニを選んだ」という文脈全体がValueになります。Query（「彼」）とKey（「涼太」）の関連性が高いと判断された場合、このValueの情報が「彼」の参照解決に用いられ、「彼」が「涼太」を指していることが文脈上理解されます。

> 昨日、公園で本を読んでいたら、友達に会った。彼は明日の試験のために勉強していると言っていた。
- Query: ここでも「彼」をQueryとします。モデルはこの代名詞が指す対象を特定しようと「問い合わせ」を行います。
- Key: 「友達」や「公園」など、文中の名詞や「昨日」のような時制を表す単語がKeyの役割を果たします。これらは、「彼」が指す対象の特定において潜在的に関連する文脈の手がかりとなります。
- Value: 「友達に会った」という情報がValueとなります。この情報は、「彼」が指す対象の特定において、「彼」が「友達」を指しているという文脈を提供します。

# 自然言語処理 (NLP)

画像処理においても、近年の深層学習では Transformer という技術が広く用いられています。
Transformer はもともと、コンピュータにおいて人間の言葉を扱う自然言語処理 (natural language processing; NLP) の世界で登場した技術です。
自然言語処理の分野を軽く知っておくことが、Transformer のイメージを理解する近道となります。

## 自然言語処理のタスク

この分野で具体的にどのようなタスクが扱われているのか見ていきましょう。これらのタスクは、言語の多義性、文脈の理解、長距離依存関係の捉え方など、いくつか共通の課題をもっています。

#### テキスト生成
正確かつ文章として自然なテキスト生成することです。たとえばChatGPTのようなモデルが行うテキスト生成では、入力されたプロンプトに対して意味のある、文脈に沿った応答を生成する必要があります。

#### 翻訳
言語間でのテキストの翻訳では、文脈の正確な理解と、異なる言語間の文法やニュアンスの違いを適切に扱う能力が求められます。
長距離依存関係の理解が特に重要です。

#### 要約
テキストを要約する際には、長い文書の主要なポイントを抽出し、簡潔にまとめる能力が必要です。
文脈全体を理解し、重要な情報を選別することが課題となります。

#### 感情分析
テキストに含まれる感情や意見を識別するには、細かいニュアンスや文脈の理解が必要です。
単語の使い方や文脈が感情のポジティブさやネガティブさに大きく影響を与えるため、これらを正確に捉えることが課題です。

#### 質問応答
特定の質問に対して正確な答えを提供するには、質問の意図と関連する情報源の文脈を深く理解する必要があります。
ここでは、関連する情報を特定し、それに基づいて適切な回答を生成する能力が求められます。

## 従来の技術

#### RNN (Recurrent Neural Networks)
文や音声データのようなシーケンシャルなデータを処理するために設計されました。ただし、長距離の依存関係を捉えるのが得意ではありません。

#### LSTM (Long Short-Term Memory)
RNNの問題を解決するために導入されました。長期記憶と短期記憶の概念を用いて、長距離の依存関係をより効果的に捉えます。

#### GRU (Gated Recurrent Unit)
LSTM に似ているが、よりシンプルな構造を持ち、計算効率が向上しています。

## 従来の技術の限界

RNN、LSTM、GRUといった従来の技術は、自然言語処理の多くの基本的なタスクに貢献しました。これらのモデルは、テキストのシーケンシャルな性質を捉えるために設計され、文脈を通じて情報を伝達する能力を持っています。しかし、これらの技術には根本的な課題があります。

#### 長距離依存関係の捉え方

RNNやその派生形であるLSTM、GRUでも、非常に長いテキストにおける情報の伝達は困難で、長距離依存関係を効果的にモデル化することが難しいです。たとえば、小説や研究論文のような長い文書では、文の初めと終わりで言及される概念が密接に関連していることがよくあります。従来のモデルでは、これらの関連性を捉えることができず、結果として文脈の理解が不完全になりがちです。

> 涼太はピザを注文したが、彼はチーズが苦手なので、トッピングにはトマトとペパロニを選んだ。彼が食べるとき、...

この文で、「彼」という代名詞が「涼太」を指していることは、人間にとっては直感的に理解できます。しかし、RNNやLSTM、GRUを用いたモデルでは、テキストが長くなるにつれて、「彼」が文の初めの「涼太」を指しているという情報を保持し続けることが難しくなります。特に、「彼が食べるとき」という部分に至ると、モデルが「彼」が誰であるか、そして「チーズが苦手」であるという情報を正確に関連付けることはさらに困難になります。

#### 文脈の理解

単語ひとつひとつの意味は、その使用されている文脈によって大きく変わることがあります。この文脈に基づく意味の変化を適切に理解し、処理できるかどうかが、NLPシステムの性能を大きく左右します。しかし、RNNやLSTM、GRUといった従来のシーケンシャル処理モデルでは、テキストが長い場合や複雑な文脈が絡む場合に、このような文脈の変化を捉えきれないことがあります。

従来のモデルは、テキストを順次処理していく過程で、情報を一定の「記憶」に圧縮して伝達する必要があります。その結果、入力シーケンスの先頭にある情報が、シーケンスの後方で適切に利用されることが難しくなります。特に、文脈によって意味が大きく変わる単語の処理において、これが顕著になります。

> 昨日、公園で本を読んでいたら、友達に会った。彼は明日の試験のために勉強していると言っていた。

「彼」という代名詞が誰を指しているかを理解する必要があります。人間であれば、この「彼」が直前に言及された「友達」を指していることを容易に理解できますが、RNNやLSTM、GRUを用いた従来のモデルでは、このような代名詞の参照関係を正確に解決することが困難です。特に、複数の人物が登場する長い物語や会話の文脈では、どの代名詞がどの名詞を指しているかを追跡することが非常に難しくなります。

この問題は、テキスト内で言及される様々なエンティティ間の関係性を適切にモデル化し、長いシーケンスを通じてそれらの関係性を維持する能力が不足しているために起こります。従来のモデルでは、特にシーケンスが長くなると、初期の部分で処理した情報が後の部分で十分に活用されず、文脈の理解が不完全になる傾向があります。

<!-- 
#### 計算効率の問題

RNN系のモデルはシーケンスの各要素を順番に処理するため、計算に時間がかかり、並列化が困難です。この計算の遅さは、大規模なデータセットやリアルタイムアプリケーションでの使用を制限する一因となっていました。
-->

## Attention 機構へ

これらの課題を克服するために、Attention 機構が導入されました。Attentionは、モデルが入力シーケンスの特定の部分に「焦点を当てる」ことを可能にし、重要な情報に対してより高い重みを付けることができます。これにより、長距離依存関係の問題を効果的に解決し、シーケンス内の任意の位置間で直接的な関係をモデル化できます。

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "history_visible": true,
      "authorship_tag": "ABX9TyNlLTjWDqqntsAtNImToLQP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/transformer/cifar10_pytorch_vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT) による画像分類\n",
        "\n",
        "### はじめに\n",
        "\n",
        "前回、CNN（畳み込みニューラルネットワーク）を使用してCIFAR-10データセットの分類を行いました。CNNでは、畳み込み層やプーリング層を使用して画像から特徴量を抽出し、最終的に分類を行います。しかし、今回の課題では畳み込み層やプーリング層を使わずに、同様のタスクを Vision Transformer (ViT) を使って実行してみましょう。\n",
        "\n",
        "### ViTとは\n",
        "\n",
        "<a title=\"Davide Coccomini, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Vision_Transformer.gif\"><img width=\"512\" alt=\"Vision Transformer\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/Vision_Transformer.gif/512px-Vision_Transformer.gif?20230818142429\"></a>\n",
        "\n",
        "Vision Transformer (ViT) は、自然言語処理で成功を収めたTransformerモデルを画像認識に適用したものです。TransformerはAttentionメカニズムを使用して、入力データの重要な部分に焦点を当てることで特徴量を抽出します。これにより、ViTは画像全体を処理するのではなく、部分的な情報からも高い特徴抽出能力を発揮します。\n",
        "\n",
        "### CNN と ViT\n",
        "\n",
        "CNNの場合、以下の2種類の層を繰り返し用いて特徴量を抽出します。\n",
        "\n",
        "- **畳み込み層**<br>\n",
        "異なるフィルタを画像に適用して、エッジやテクスチャなどの低レベルの特徴を検出します。\n",
        "- **プーリング層**<br>\n",
        "空間的な次元を削減し、計算量を減らしながら重要な特徴を保持します。\n",
        "\n",
        "CNN におけるこれらの処理では、**固定のカーネル** を使用し、局所的な特徴を抽出します。局所的なパターン認識に優れていますが、画像全体の文脈を直接捉えることは難しいです。\n",
        "畳み込み層により、画像全体に同じフィルタが適用されます。\n",
        "\n",
        "ViTは、特徴量のうち識別に有効な部分だけを重み付けします。自己注意機構を使用して、各パッチのうち、分類に有効そうなものに高い重みを与えるようになっています。これにより、もし\n",
        "\n",
        "### 特徴抽出の流れ\n",
        "\n",
        "ViT の場合は、以下の手順で特徴量を抽出します。\n",
        "\n",
        "<img src=\"https://github.com/shizoda/education/assets/34496702/f4d8612f-66a6-4302-8163-dd5d8d63f27d\" width=60%>\n",
        "\n",
        "*[原論文 [Dosovitskiy21]](https://openreview.net/forum?id=YicbFdNTTy) Fig. 1 の一部。全体構成を表す*\n",
        "\n",
        "- **パッチ分割**<br>\n",
        "画像を小さな固定サイズのパッチに分割します。例えば、32x32のCIFAR-10の画像を4x4のパッチに分割すると、各パッチのサイズは8x8となり、合計64個のパッチが得られます。\n",
        "- **パッチ埋め込み** <br>\n",
        "各パッチを固定長のベクトルに変換します。各パッチをフラットにして線形層に通すことで行います。例えば、8x8のパッチは64次元のベクトルに変換され、それを指定した次元の埋め込みベクトル（例えば128次元）へ線形層により変換します。\n",
        "- **位置エンコーディング** <br>\n",
        "上で得られた埋め込みベクトルにはパッチの位置情報が与えられていないため、モデルは各パッチがどの位置にあるかを認識できません。位置エンコーディングは、固定長のベクトルを加算することで、各パッチに位置情報を持たせます。モデルが画像の構造を理解するのに役立ちます。\n",
        "- **クラストークン**<br>\n",
        "画像全体の特徴を表すために導入される特別なトークンです。このトークンは、全てのパッチ埋め込みと同じ次元を持ち、最初に入力として追加されます。最終的な分類結果は、このクラストークンの表現から得られます。\n",
        "- **Transformerエンコーダ** <br>\n",
        "パッチ埋め込みと位置エンコーディングを行った後、これらのベクトルはTransformerエンコーダに入力されます。エンコーダは複数の層からなり、各層は自己注意機構とフィードフォワードネットワークから構成されます。自己注意機構により、各パッチが他の全てのパッチとの関係を学習します。これにより、画像全体の文脈を考慮した特徴量を抽出できます。\n",
        "\n",
        "### ViT のメリット\n",
        "\n",
        "本来はもっと幅広いメリットがあるのですが、今回は Attention（注意）メカニズムに初めて触れる意味で、その名からわかりやすい以下の２点に着目してもらえれば十分と思います。\n",
        "\n",
        "- **部分的な情報からの特徴抽出**<br>\n",
        "ViTは、画像の一部にしか対象物が映っていない場合でも、その部分から有効な特徴量を抽出できます。これは自己注意機構 (Self-Attention Mechanism) が、画像中の重要な部分のパッチに大きな重みを与え、その部分からの特徴量を主として画像を表すからです。\n",
        "- **自己注意機構による可視化**<br>\n",
        "モデルがどの部分に注目するかを可視化することもできます。これにより、判断根拠をある程度可視化することが可能です。\n",
        "\n",
        "### パッチ間の相互関係と画像理解\n",
        "\n",
        "Transformerの自己注意機構では、各パッチが他の全てのパッチに対して注意を向けることができます。これにより、画像の一部分の情報が他の部分の情報とどのように関係しているかを学習できます。例えば、画像中のある物体の一部が他の部分とどのように連携しているかを理解することが可能です。これにより、画像全体のコンテキストを捉えることができ、より精度の高い特徴抽出と分類が可能になります。\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DMkjc9LfKW92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch 関連のライブラリをインポートします\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# GPU が利用可能であることを確認\n",
        "assert torch.cuda.is_available(), \"GPU が使えません。ランタイムの設定を確認してください。\""
      ],
      "metadata": {
        "id": "ySI-pKgrRPxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViTクラスの定義\n",
        "\n",
        "ViTクラスは、ViTモデル全体を定義するクラスです。画像をパッチに分割し、各パッチを埋め込んだ後、Transformerエンコーダを通じて特徴量を抽出し、最終的にクラスを予測します。\n",
        "\n",
        "今回の実装では、以下の表のようにサイズ (shape) が変わっていきます。\n",
        "\n",
        "| ステージ | 形状　　　　　　　　　　　　. |\n",
        "|----------|-------------------|\n",
        "| 入力画像 | $(B, 3, 32, 32)$ |\n",
        "| パッチ分割 | $(B, 64, 48)$ |\n",
        "| パッチ埋め込み | $(B, 64, 512)$ |\n",
        "| クラストークン追加後 | $(B, 65, 512)$ |\n",
        "| 位置エンコーディング追加後 | $(B, 65, 512)$ |\n",
        "| Transformerエンコーダ後 | $(B, 65, 512)$ |\n",
        "| 最終分類 (クラストークンの抽出) | $(B, 512)$ |\n",
        "| 線形層通過後 | $(B, クラス数)$ |\n",
        "\n",
        "#### 入力画像サイズ\n",
        "- 入力画像のサイズは $(C, H, W)$ です。<br>\n",
        "ここでは $C = 3$、$H = W = 32$ です。\n",
        "\n",
        "#### パッチ分割\n",
        "- パッチサイズを $P\\times P$ とすると、ここでは $P=4$ です。\n",
        "- したがって、パッチの数は\n",
        "$$(H \\times W)/(P \\times P)=64$$\n",
        "となります\n",
        "\n",
        "#### パッチ埋め込み\n",
        "- 各パッチの次元は $ 3 \\times P^2 = 3 \\times 4^2 = 48 $ です。\n",
        "- パッチ埋め込み層では、これを埋め込み次元 $ D = 512 $ のベクトルに変換します。つまり、パッチの埋め込みベクトルの形状は$(B, 64, 512)$ になります。ここで $B$ はバッチサイズです。\n",
        "\n",
        "#### クラストークンと位置エンコーディング\n",
        "- **クラストークン（class token）** は、クラスごとに画像全体の特徴を表現するために使用されます。これは、パッチ埋め込みベクトルの先頭に追加され、形状は $ (B, 1, 512)$ となります。\n",
        "- **位置エンコーディング（position embedding）** は、各パッチの位置情報を保持するために使用されます。これにより、モデルはパッチの順序を認識できます。位置エンコーディングを加えた後の入力ベクトルの形状は  $ (B, 65, 512)  $ となります。\n",
        "\n",
        "#### Transformerエンコーダ\n",
        "- **Transformerエンコーダ**は、自己注意機構を用いて各パッチ間の関係を学習します。入力の形状は $ (B, 65, 512) $ です。\n",
        "- 各エンコーダ層では、入力ベクトルが自己注意機構とフィードフォワードネットワークを通過し、最終的に LayerNorm によって正規化されます。\n",
        "\n",
        "#### 最終分類\n",
        "- 最後に、クラストークンの表現が最終的な特徴ベクトルとして取り出され、線形層を通じてクラス分類が行われます。出力形状は $ (B, クラス数) $ です。"
      ],
      "metadata": {
        "id": "WFfFljAaMwzx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZ2u9l4YKR1G"
      },
      "outputs": [],
      "source": [
        "# ViTクラスの定義\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, num_classes=10, img_size=32, patch_size=4, dim_hidden=512, num_heads=8, dim_feedforward=512, num_layers=6):\n",
        "        super().__init__()\n",
        "        assert img_size % patch_size == 0  # 画像サイズがパッチサイズで割り切れるか確認\n",
        "\n",
        "        # 初期化パラメータの設定\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        num_patches = (img_size // patch_size) ** 2  # パッチの数を計算\n",
        "        dim_patch = 3 * patch_size ** 2  # 各パッチの次元数\n",
        "\n",
        "        # パッチ埋め込み層\n",
        "        self.patch_embed = nn.Linear(dim_patch, dim_hidden)  # パッチを埋め込みベクトルに変換\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, dim_hidden))  # 位置エンコーディングの初期化\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, dim_hidden))  # クラストークンの初期化\n",
        "\n",
        "        # Transformerエンコーダ層のスタック\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer(dim_hidden, num_heads, dim_feedforward) for _ in range(num_layers)])\n",
        "        self.norm = nn.LayerNorm(dim_hidden)  # 最後のLayerNorm層\n",
        "        self.linear = nn.Linear(dim_hidden, num_classes)  # 最終分類用の線形層\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, c, h, w = x.shape  # バッチサイズ、チャンネル、高さ、幅\n",
        "        assert h == self.img_size and w == self.img_size  # 入力画像サイズが一致するか確認\n",
        "\n",
        "        # 画像をパッチに分割し、各パッチを埋め込みベクトルに変換\n",
        "        x = x.view(bs, c, h // self.patch_size, self.patch_size, w // self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 4, 1, 3, 5).reshape(bs, -1, 3 * self.patch_size ** 2)\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # クラストークンを追加し、位置エンコーディングを適用\n",
        "        class_token = self.class_token.expand(bs, -1, -1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "        x += self.pos_embed\n",
        "\n",
        "        # Transformerエンコーダ層を通過\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # attention_weightsを保存\n",
        "        self.attention_weights = self.layers[0].attention.attn_weights\n",
        "\n",
        "        # 最終分類\n",
        "        x = self.norm(x)[:, 0]\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題１**\n",
        "全体的なネットワークの図を描いてください。\n",
        "<br> 各ステップごとに shape がどうなるかも書き込んでください。"
      ],
      "metadata": {
        "id": "M-7aaycvFiaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TransformerEncoderLayer クラスの定義\n",
        "\n",
        "TransformerEncoderLayer クラスは、1つのTransformerエンコーダ層を定義します。各エンコーダ層は自己注意機構とフィードフォワードネットワーク（FNN）から構成され、入力データの相互関係を学習します。\n",
        "\n",
        "エンコーダ層の役割は、入力シーケンス内の異なる要素間の関係を学習することです。これにより、画像内の異なるパッチ間の関係を理解し、より豊かな特徴表現を得ることができます。\n",
        "\n",
        "具体的には、エンコーダ層では以下の手順で処理が行われます：\n",
        "\n",
        "- **自己注意機構** <br>\n",
        "入力データの各部分の重要度を計算し、重要な部分に焦点を当てます。これにより、入力データの相互関係を学習します。\n",
        "- **フィードフォワードネットワーク（FNN）**<br>\n",
        "自己注意機構の出力に対して、2層の全結合ネットワークを適用します。これにより、特徴表現がさらに強化されます。\n",
        "- **正規化と残差接続**<br>\n",
        "各サブレイヤーの出力を正規化し、入力に加算することで、学習を安定させます。"
      ],
      "metadata": {
        "id": "TUYR-ysNNphX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TransformerEncoderLayerクラスの定義\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, dim_hidden, num_heads, dim_feedforward):\n",
        "        super().__init__()\n",
        "        self.attention = SelfAttention(dim_hidden, num_heads)  # 自己注意機構\n",
        "        self.fnn = FNN(dim_hidden, dim_feedforward)  # フィードフォワードネットワーク\n",
        "        self.norm1 = nn.LayerNorm(dim_hidden)  # 最初のLayerNorm\n",
        "        self.norm2 = nn.LayerNorm(dim_hidden)  # 二番目のLayerNorm\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 自己注意機構の出力に残差接続を追加\n",
        "        x = x + self.attention(self.norm1(x))\n",
        "        # フィードフォワードネットワークの出力に残差接続を追加\n",
        "        x = x + self.fnn(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "GsgO-DFwN7iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SelfAttention クラスの定義\n",
        "\n",
        "SelfAttention クラスは、自己注意機構を定義します。\n",
        "\n",
        "CNN（Convolutional Neural Network）では、固定のカーネルを画像全体に対して施します。CNNの畳み込み層では、カーネル（フィルタ）が画像の各位置に対して適用され、特徴マップが生成されます。これにより、局所的な特徴を抽出することができますが、画像全体の文脈を考慮することは困難です。\n",
        "\n",
        "ViTは、特徴量のうち **識別に有効な部分だけを重み付け** する効果があります。これを実現するのが自己注意機構（Self-Attention Mechanism）です。自己注意機構では、クエリとキーの内積を計算し、ソフトマックス関数を用いて重み付けを行います。この重みをバリューに掛けることで、重要な特徴量に対して高い重みを与え、そうでない部分には低い重みを与えることができます。\n",
        "\n",
        "自己注意機構では、以下の手順で処理が行われます：\n",
        "\n",
        "#### **クエリ（Query）、キー（Key）、バリュー（Value）** の計算\n",
        "\n",
        "ViTでは、画像を小さなパッチに分割し、それぞれのパッチを特徴ベクトルに変換します。この特徴ベクトルを使って、クエリ（Query）、キー（Key）、バリュー（Value）という3つの新しい行列を計算します。クエリは、そのパッチが他のパッチとどれだけ関連があるかを測るための行列です。キーは、他のパッチと関連性を評価するための行列です。バリューは、パッチが持つ情報そのものを表す行列です。これらの行列を計算することで、ViTは画像全体の文脈を捉えることができます。\n",
        "\n",
        "数式では、入力パッチの特徴ベクトルを$\\mathbf{X}$とし、それに重み行列$\\mathbf{W_Q}$、$\\mathbf{W_K}$、$\\mathbf{W_V}$を掛けることで、クエリ$\\mathbf{Q}$、キー$\\mathbf{K}$、バリュー$\\mathbf{V}$を得ます。具体的には次のようになります：\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbf{Q} &= \\mathbf{X} \\mathbf{W_Q}, \\\\\n",
        "\\mathbf{K} &= \\mathbf{X} \\mathbf{W_K}, \\\\\n",
        "\\mathbf{V} &= \\mathbf{X} \\mathbf{W_V}\n",
        "\\end{align*}\n",
        "\n",
        "ここで、$\\mathbf{W_Q}$、$\\mathbf{W_K}$、$\\mathbf{W_V}$は学習可能なパラメータであり、訓練データを通じて最適化されます。これにより、各パッチが他のパッチとどのように関連しているかを学習することができます。結果として、画像全体の意味や文脈を理解するための重要な情報が得られます。\n",
        "\n",
        "#### **注意スコア** の計算\n",
        "\n",
        "次に、クエリとキーを使って、各パッチの関連性を計算します。これを注意スコアと呼びます。注意スコアは、各パッチが他のパッチとどれだけ関連しているかを示す値です。具体的には、クエリ$\\mathbf{Q}$とキー$\\mathbf{K}$の内積を計算し、それをキーの次元数$\\sqrt{D}$で割って正規化します。この結果にソフトマックス関数を適用して、注意スコアを得ます。ソフトマックス関数を使うことで、注意スコアが確率分布となり、全てのパッチのスコアの合計が1になります。\n",
        "\n",
        "数式では、注意スコアは次のように計算されます：\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{D}}\\right)\\mathbf{V}\n",
        "\\end{align*}\n",
        "\n",
        "ここで、$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})$は注意機構の出力を表し、$\\mathbf{Q}\\mathbf{K}^T$はクエリとキーの内積を取った行列です。この内積は、各パッチが他のパッチにどれだけ注意を払うべきかを示します。次に、これを$\\sqrt{D}$で割ることで、数値のスケールを調整します。\n",
        "\n",
        "#### ソフトマックスおよびバリューの計算\n",
        "ソフトマックス関数を適用することで、各パッチの重要度が確率として表現されます。最後に、この注意スコアをバリュー $\\mathbf{V}$ に掛けることで、各パッチの情報を重み付けして集約します。これにより、ViTは画像全体の情報を効果的に集約し、より良い特徴量を得ることができます。"
      ],
      "metadata": {
        "id": "DyLxy0jEOD2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SelfAttentionクラスの定義\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_hidden, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert dim_hidden % num_heads == 0  # 隠れ次元がヘッドの数で割り切れるか確認\n",
        "        self.num_heads = num_heads  # 注意機構のヘッドの数\n",
        "        dim_head = dim_hidden // num_heads  # 各ヘッドの次元数\n",
        "        self.scale = dim_head ** -0.5  # スケーリング係数\n",
        "\n",
        "        # 入力をQ、K、Vに投影する線形層\n",
        "        self.proj_in = nn.Linear(dim_hidden, dim_hidden * 3, bias=qkv_bias)\n",
        "        self.proj_out = nn.Linear(dim_hidden, dim_hidden)  # 出力の線形層\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, n, _ = x.shape  # バッチサイズとシーケンス長\n",
        "        # Q、K、Vに分割して計算\n",
        "        qkv = self.proj_in(x).view(bs, n, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)  # クエリ、キー、バリューに分割\n",
        "\n",
        "        # 注意スコアを計算\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)  # ソフトマックスで正規化\n",
        "\n",
        "        # コンテキストベクトルを計算\n",
        "        x = (attn @ v).transpose(1, 2).reshape(bs, n, -1)\n",
        "        self.attn_weights = attn  # Attention weightsを保存\n",
        "        return self.proj_out(x)"
      ],
      "metadata": {
        "id": "bZSWmEEJOYa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FNNクラスの定義\n",
        "\n",
        "FNNクラスは、フィードフォワードネットワーク（FNN）を定義します。FNNは、自己注意機構の出力に対して、さらに特徴を抽出するために使用されます。具体的には、2層の全結合ネットワークを使用します。\n",
        "\n",
        "FNNでは、以下の手順で処理が行われます：\n",
        "\n",
        "- 線形変換と活性化関数：入力データに対して線形変換を行い、その後GELU活性化関数を適用します。これにより、非線形性が導入されます。\n",
        "- もう一つの線形変換：活性化関数の出力に対して、もう一度線形変換を行います。"
      ],
      "metadata": {
        "id": "Dgj6f4CGOa4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FNNクラスの定義\n",
        "class FNN(nn.Module):\n",
        "    def __init__(self, dim_hidden, dim_feedforward):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(dim_hidden, dim_feedforward)  # 最初の線形変換\n",
        "        self.linear2 = nn.Linear(dim_feedforward, dim_hidden)  # 次の線形変換\n",
        "        self.activation = nn.GELU()  # 活性化関数\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 線形変換 -> 活性化関数 -> 線形変換の順に適用\n",
        "        return self.linear2(self.activation(self.linear1(x)))"
      ],
      "metadata": {
        "id": "PPzI-BcoOabG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題２**\n",
        "\n",
        "- Query, Key, Value のうち、のちの処理に使うために出力される特徴量はどれですか？\n",
        " - その出力のために、上記以外の２つはどのような役割を果たしますか？\n",
        "- 畳み込み層とは違って、ここでは特徴量の重み付けが行われます。そのメリットは何ですか？"
      ],
      "metadata": {
        "id": "SpA2E92mF2dR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 学習\n",
        "\n",
        "CNN が ViT に置き換わったものの、それに対する学習処理は CNN の場合とまったく同じです。"
      ],
      "metadata": {
        "id": "C5tYvTHFOoQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViT クラスのインスタンスを net として得る\n",
        "net = ViT()\n",
        "print(net)\n",
        "\n",
        "# GPUが利用可能な場合はGPUにモデルを移動させる\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device)\n",
        "\n",
        "initial_lr = 0.01 # 初期学習率\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=initial_lr) # Adam オプティマイザ\n",
        "criterion = nn.CrossEntropyLoss() # クロスエントロピー損失関数"
      ],
      "metadata": {
        "id": "tMUwaKebO-Di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データも準備します。"
      ],
      "metadata": {
        "id": "I-ey8EhCRatR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# 学習用データセットをロードし、検証用データセットに分割\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "train_size = int(0.8 * len(trainset))\n",
        "validation_size = len(trainset) - train_size\n",
        "train_dataset, validation_dataset = random_split(trainset, [train_size, validation_size])\n",
        "\n",
        "# ミニバッチ (mini-batch) サイズを100とし、学習用データローダと検証用データローダを定義\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "validationloader = torch.utils.data.DataLoader(validation_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# テスト用データセットをロード\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "S-H5ILtSRdLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここから学習ループに入ります。\n",
        "\n",
        "ViT は CNN よりも処理が重いため、エポック数は少なめにしています。他は CNN の課題とほぼ同一です。\n",
        "\n"
      ],
      "metadata": {
        "id": "-zvOoo5cPITg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epoch = 15     # 最大エポック数\n",
        "patience = 5       # 改善が見られないエポック数の許容回数\n",
        "trigger_times = 0  # 改善が見られないエポック数のカウンター\n",
        "\n",
        "best_val_loss = float(\"inf\")  # 初期値として無限大を設定\n",
        "train_losses = []  # 学習データセットの損失を保存するリスト\n",
        "val_losses = []  # 検証データセットの損失を保存するリスト\n",
        "\n",
        "# エポック数のループ\n",
        "for epoch in range(max_epoch):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # 学習データセットからミニバッチを得るたびに…\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)  # 入力データをGPUに送る\n",
        "        labels = labels.to(device)  # ラベルをGPUに送る\n",
        "\n",
        "        optimizer.zero_grad()  # 勾配の初期化\n",
        "\n",
        "        outputs = net(inputs)  # ネットワークに入力データを渡して出力を取得\n",
        "        loss = criterion(outputs, labels)  # 損失を計算\n",
        "        loss.backward()  # 逆伝播を行い、勾配を計算\n",
        "        optimizer.step()  # パラメータを更新\n",
        "\n",
        "        running_loss += loss.item()  # ミニバッチの損失を累積\n",
        "\n",
        "    # エポックごとの訓練データセットに対する平均損失を計算\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 検証データセットに対する損失を計算\n",
        "    val_loss = 0.0\n",
        "    net.eval()  # モデルを評価モードに切り替える\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # 検証データセットからミニバッチを得るたびに…\n",
        "        for data in validationloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)  # 入力データをGPUに送る\n",
        "            labels = labels.to(device)  # ラベルをGPUに送る\n",
        "\n",
        "            outputs = net(images)  # ネットワークに入力データを渡して出力を取得\n",
        "            loss = criterion(outputs, labels)  # 損失を計算\n",
        "            val_loss += loss.item()  # 損失を累積\n",
        "\n",
        "    # エポックごとの検証データセットに対する平均損失を計算\n",
        "    val_loss = val_loss / len(validationloader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # 損失の値をグラフで表示\n",
        "    plt.clf()  # 前のグラフをクリア\n",
        "    plt.plot(train_losses, label='Training loss')  # 訓練データセットの損失\n",
        "    plt.plot(val_losses, label='Validation loss')  # 検証データセットの損失をプロット\n",
        "    plt.xlabel('Epoch')  # x軸のラベルを設定\n",
        "    plt.ylabel('Loss')  # y軸のラベルを設定\n",
        "    plt.xlim(left=0)  # x軸の表示範囲を設定\n",
        "    plt.ylim(bottom=0)  # y軸の表示範囲を設定\n",
        "    plt.legend()  # 凡例を表示\n",
        "    plt.show()  # グラフを表示\n",
        "\n",
        "    # エポックごとの損失を出力\n",
        "    print(\"Epoch:\", epoch + 1)\n",
        "    print(\"Train loss     : \", train_loss)\n",
        "    print(\"Validation loss: \", val_loss)\n",
        "\n",
        "    # 検証データセットに対する損失が改善しない場合の処理\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss  # 最良の検証データセット損失を更新\n",
        "        trigger_times = 0  # 改善が見られないエポック数をリセット\n",
        "    else:\n",
        "        trigger_times += 1  # 改善が見られないエポック数をカウントアップ\n",
        "        if trigger_times >= patience:  # 許容回数を超えた場合の処理\n",
        "            print(f\"{epoch + 1} エポックで早期終了\")  # 早期終了のメッセージを出力\n",
        "            break  # 学習を終了\n",
        "\n",
        "print('学習終了')"
      ],
      "metadata": {
        "id": "BPjY_J_MPGdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テスト\n",
        "\n",
        "精度を求める処理もまったく同じです。\n",
        "\n",
        "CIFAR-10 は小規模なデータのため、残念ながら CNN を超える精度は出ないかもしれません。"
      ],
      "metadata": {
        "id": "i8orq5KBPOU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0  # 正解数のカウンターを初期化\n",
        "total = 0  # 全体の画像数のカウンターを初期化\n",
        "\n",
        "# 訓練モードでの計算を停止（推論モードに切り替える）\n",
        "with torch.no_grad():\n",
        "    # テストデータセットに対してループ\n",
        "    for data in testloader:\n",
        "        images, labels = data  # ミニバッチごとの画像とラベルを取得\n",
        "        images = images.to(device)  # GPUに画像を送る\n",
        "        labels = labels.to(device)  # GPUにラベルを送る\n",
        "\n",
        "        outputs = net(images)  # ネットワークに画像を入力し、出力を取得\n",
        "        _, predicted = torch.max(outputs.data, 1)  # 出力の中で最大の値を持つクラスを予測として取得\n",
        "        total += labels.size(0)  # ミニバッチ内の画像数を全体の画像数に加算\n",
        "        correct += (predicted == labels).sum().item()  # 予測が正しい場合、正解数をカウントアップ\n",
        "\n",
        "print('テスト画像における精度 %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "id": "FM_sO79uPPj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クラスベクトル\n",
        "\n",
        "クラスベクトルは、**クラストークン** がTransformerエンコーダを通過した後の最終的な表現を指します。言い換えれば、クラストークンがエンコーダを通じて得た情報を含むベクトルです。このクラスベクトルは、画像全体の特徴を捉えており、最終的な分類層に入力され、各クラスへの所属確率を出力します。まずそれ自体を表で確認しましょう。"
      ],
      "metadata": {
        "id": "EnKgHvVaVaiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# CIFAR-10のクラス名\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# 学習済みモデルのクラスベクトルを取得\n",
        "class_vectors = net.linear.weight.data.cpu().numpy()\n",
        "\n",
        "# クラス番号、クラス名、クラスベクトルをデータフレームに格納\n",
        "df = pd.DataFrame(class_vectors, columns=[f'Feature_{i+1}' for i in range(class_vectors.shape[1])])\n",
        "df.insert(0, 'Class Name', classes)\n",
        "df.insert(0, 'Class Number', range(10))\n",
        "\n",
        "# データフレームを表示\n",
        "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Class Vectors\", dataframe=df)\n",
        "df"
      ],
      "metadata": {
        "id": "VyUrGCeIVjlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### クラスベクトルによる可視化\n",
        "\n",
        "ここではクラスベクトルを用いて、入力画像のどの部分がモデルの予測に重要だったか（注意されていたか）をヒートマップで示します。セグメンテーションのように事前に注目箇所を与えて学習するわけではありませんが、Transformerの自己注意機構により、各パッチ間の関係性を学習し、自然に注目箇所が形成されます。各パッチが予測にどの程度寄与しているかを示す注意スコアを計算し、そのスコアを基にヒートマップを生成します。\n",
        "\n",
        "ここで、ViTにおける自己注意機構をおさらいします：\n",
        "\n",
        "**クエリ（Query）、キー（Key）、バリュー（Value）の計算**\n",
        "\n",
        "\\begin{align*}\n",
        "\\mathbf{Q} &= \\mathbf{X} \\mathbf{W_Q}, \\\n",
        "\\mathbf{K} &= \\mathbf{X} \\mathbf{W_K}, \\\n",
        "\\mathbf{V} &= \\mathbf{X} \\mathbf{W_V}\n",
        "\\end{align*}\n",
        "\n",
        "ここで、$\\mathbf{W_Q}$、$\\mathbf{W_K}$、$\\mathbf{W_V}$\n",
        "$X$は入力パッチの特徴ベクトル、$\\mathbf{W_Q}$、$\\mathbf{W_K}$、$\\mathbf{W_V}$はそれぞれの重み行列です。\n",
        "\n",
        "**注意スコアの計算**\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) &= \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{D}}\\right)\\mathbf{V}\n",
        "\\end{align*}\n",
        "$D$はキーの次元数です。\n",
        "\n",
        "**クラスベクトルと注意スコアを用いたヒートマップの生成**\n",
        "\n",
        "クラスベクトルは、各クラスに対応する特徴ベクトルであり、最終的なクラス分類に使用されます。注意スコアとバリュー行列を用いて、画像内の各パッチの重要性を示すヒートマップを生成することができます。具体的には、注意スコア$\\alpha_i$と対応するバリューベクトル$\\mathbf{V}_i$を掛け合わせて、各パッチの重要度を計算します。\n",
        "\n",
        "数式では、ヒートマップは次のように計算されます：\n",
        "\n",
        "\\begin{align*}\n",
        "\\text{Heatmap} &= \\sum_{i} \\alpha_i \\mathbf{V}_i\n",
        "\\end{align*}\n",
        "\n",
        "ここで、$\\alpha_i$は注意スコアであり、$\\mathbf{V}_i$はバリュー行列$\\mathbf{V}$の$i$列目のベクトルです。ヒートマップは、モデルが注目している各パッチの重要性を示します。このヒートマップを視覚化することで、モデルがどの部分に注目しているかを理解することができます。"
      ],
      "metadata": {
        "id": "_WcS0bfKPU2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "# テストデータの画像を表示し、正解と推定値を表示する関数\n",
        "def visualize_test_predictions(start_idx, end_idx):\n",
        "    # テストデータローダーを作成\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 指定したインデックス範囲の画像とラベルを取得\n",
        "    images, labels = zip(*[(data[0], data[1]) for i, data in enumerate(testloader) if start_idx <= i < end_idx])\n",
        "\n",
        "    # モデルの予測結果を格納するリストを初期化\n",
        "    predicted_labels = []\n",
        "\n",
        "    # 推論モードで計算を停止\n",
        "    with torch.no_grad():\n",
        "        # 画像ごとに予測を行う\n",
        "        for idx, image in enumerate(images):\n",
        "            image_tensor = image.clone().detach().to(device)  # NumPy配列をテンソルに変換してGPUに送る\n",
        "            outputs = net(image_tensor)  # ネットワークに画像を入力し、出力を得る\n",
        "            _, predicted = torch.max(outputs, 1)  # 出力の中で最大の値を持つクラスを予測として取得\n",
        "            predicted_labels.append(predicted.item())  # 予測結果をリストに追加\n",
        "\n",
        "            # ヒートマップの計算\n",
        "            class_vector = net.linear.weight[predicted].cpu().detach().numpy()\n",
        "            attn = net.layers[0].attention.attn_weights.cpu().detach().numpy()  # attention weights\n",
        "\n",
        "            # 各ヘッドの注意重みを平均化\n",
        "            avg_attn = np.mean(attn[0,...], axis=0)\n",
        "\n",
        "            # クラスベクトルを除いたパッチ数を計算\n",
        "            num_patches = avg_attn.shape[-1] - 1  # クラスベクトルを除く\n",
        "            patch_size = int(np.sqrt(num_patches))\n",
        "\n",
        "            if patch_size * patch_size != num_patches:\n",
        "                raise ValueError(f\"Attention weights shape {num_patches} is not a perfect square\")\n",
        "\n",
        "            # クラスベクトルを除いてリシェイプ\n",
        "            heatmap = avg_attn[1:, 1:]  # (64, 64) の形状\n",
        "            heatmap = np.mean(heatmap, axis=0).reshape(patch_size, patch_size)  # (8, 8) の形状に平均化してリシェイプ\n",
        "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())  # 正規化\n",
        "\n",
        "            # ヒートマップを元の画像サイズに補間\n",
        "            heatmap_resized = np.array(Image.fromarray(np.uint8(255 * heatmap)).resize((32, 32)))\n",
        "\n",
        "            # オリジナル画像とヒートマップの重畳表示\n",
        "            image_np = image.cpu().detach().squeeze().permute(1, 2, 0).numpy()\n",
        "            fig, ax = plt.subplots(1, 2)\n",
        "            ax[0].imshow(image_np)\n",
        "            ax[0].set_title(f'Original Image\\nClass: {classes[labels[idx]]}')\n",
        "            ax[0].axis('off')\n",
        "\n",
        "            ax[1].imshow(image_np)\n",
        "            ax[1].imshow(heatmap_resized, alpha=0.7)\n",
        "            ax[1].set_title(f'Heatmap Overlay\\nPredicted: {classes[predicted]}')\n",
        "            ax[1].axis('off')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "# 例: インデックス範囲 0 から 5 の画像を表示\n",
        "visualize_test_predictions(0, 5)"
      ],
      "metadata": {
        "id": "6OYHPNOhVLz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "こちらも、あまり期待する結果ではなかったかもしれません。CIFAR-10 は ViT の真価を発揮するにはデータが小さく、Google Colab での学習もあまり多く行うことはできません。十分な計算資源がある場合は、ImageNet のような大規模データセットを用いて事前学習してから、この学習を行うといった案も考えられます。"
      ],
      "metadata": {
        "id": "Jnalo4G8E7gG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題３**\n",
        "\n",
        "- ここで可視化されているヒートマップのうち，値の高いところ（赤や黄色）は何を意味しますか？\n",
        " - これを求めるために行われている計算を説明してください\n",
        " - **Grad-CAM** （グラッドカム）も今回と似たヒートマップを出力する手法で、CNN でも使用できます。[ふんわり理解するGrad-CAM -- Zenn](https://zenn.dev/iq108uni/articles/7269a1b72f42be)<br>\n",
        " CNN に対して Grad-CAM で行っていることと、ViT に対してここで行っていることは何が違いますか？"
      ],
      "metadata": {
        "id": "pzDqXOuYHDEF"
      }
    }
  ]
}
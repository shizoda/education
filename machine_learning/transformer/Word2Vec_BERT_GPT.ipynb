{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPfVstscWOHujevUfTUKDVP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/transformer/Word2Vec_BERT_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 GPTはどのようにテキストを生成するのか？\n",
        "\n",
        "この課題では、ChatGPT で使われている「GPT（Generative Pre-trained Transformer）」が、どのようにして自然な文章を作り出すのか、その基本的な仕組みを4つのステップで学んでいきます。\n",
        "\n",
        "**この課題のゴール:**\n",
        "-  単語の意味を数値で表現する「埋め込みベクトル」を理解する。\n",
        "-  多くの単語の意味を重み付けした「文脈ベクトル」を理解する。\n",
        "-  BERT と GPT の違いを理解する。\n",
        "-  GPTが「次の単語」をどうやって予測しているか理解する。\n"
      ],
      "metadata": {
        "id": "F0VMOpPcQ5hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 【まず実行】必要なライブラリのインストール\n",
        "# Transformers ライブラリ（BERTやGPTを使うためのもの）\n",
        "# tqdm は進捗バーを表示するため\n",
        "!pip install -q transformers torch tqdm gensim\n",
        "print(\"✅ ライブラリのインストールが完了しました。\")\n",
        "\n",
        "#@title GPUの確認\n",
        "import torch\n",
        "import tqdm # tqdm のインポート（永続的指示）\n",
        "\n",
        "# GPUが利用可能か確認し、デバイスを設定します\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"利用中のデバイス: {device}\")\n",
        "\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "import sys\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# モデル名\n",
        "model_name = \"glove-wiki-gigaword-50\"\n",
        "\n",
        "try:\n",
        "    # グローバル変数としてモデルをロード\n",
        "    # (もし 'model_wv' が未定義の場合のみロード)\n",
        "    if 'model_wv' not in globals():\n",
        "        print(f\"軽量モデル '{model_name}' をダウンロード・ロードしています...\")\n",
        "        print(\"（初回はダウンロードに時間がかかる場合があります）\")\n",
        "\n",
        "        # gensim.downloader を使ってロード\n",
        "        # KeyedVectors オブジェクトが返る\n",
        "        model_wv = api.load(model_name)\n",
        "\n",
        "        print(\"\\n✅ GloVeモデルのロードが完了しました。\")\n",
        "        print(f\"  - 次元数: {model_wv.vector_size}\")\n",
        "        print(f\"  - 語彙数: {len(model_wv.key_to_index)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"エラー: モデルのロードに失敗しました。{e}\", file=sys.stderr)"
      ],
      "metadata": {
        "id": "hu9oBZJWRxT2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 ステップ 0: Word2Vec — 「意味」の計算とその学習方法\n",
        "\n",
        "GPTやBERTといったAIが登場する前に活躍した「Word2Vec（ワード・トゥ・ベック）」という技術があります。これは、単語を「意味」を持つベクトル（数値のリスト）に変換する技術です。\n",
        "\n",
        "このステップでは、まずWord2Vecが**どのようにしてそのベクトルを学習するのか**（CBOWモデル）を見てから、その結果として**なぜ「意味の計算」が可能になるのか**を学びます。\n",
        "\n",
        "---\n",
        "\n",
        "## 🤔 どうやってベクトルを学習するのか？\n",
        "\n",
        "Word2Vecは、AIに「**あるタスク**」を解かせることで、副産物として単語のベクトルを獲得します。\n",
        "\n",
        "### 🧠 1. 中核理論: 分布仮説\n",
        "\n",
        "Word2Vecの根底には「**分布仮説**」という考え方があります。\n",
        "\n",
        "> **分布仮説**: ある単語の意味は、その単語の「**周囲に出現する単語**」によって決まる。\n",
        "\n",
        "例えば、「銀行」という単語は、「お金」「預金」「引き出す」「ローン」といった単語の近くで使われることが多いです。\n",
        "\n",
        "### 📖 2. 学習タスク (CBOWモデル)\n",
        "\n",
        "この仮説を実現するため、AI（シンプルなニューラルネットワーク）に「**予測タスク**」を解かせます。代表的なモデルが **CBOW (Continuous Bag-of-Words)** です。\n",
        "\n",
        "* **CBOWのタスク**: **「周囲の単語」から「中心の単語」を予測（穴埋め）する。**\n",
        "* **例文**: `... went to the [ ? ] to deposit money`\n",
        "\n",
        "AIは、この文脈（`deposit`, `money` など）を入力として受け取り、中心の `[ ? ]` が `bank` であることを予測するように学習します。\n",
        "\n",
        "### 🛠️ 3. 埋め込みベクトルが作られる仕組み\n",
        "\n",
        "CBOWタスクを解くニューラルネットワークは、以下の仕組みで動作します。\n",
        "\n",
        "1.  **辞書（埋め込み層 $W_{in}$）**:\n",
        "    まず、全単語（例: 5万語）分の「辞書テーブル」（$W_{in}$）を用意します。この時点では、各単語のベクトル（例: 50次元）はランダムな数値です。\n",
        "2.  **入力と平均化**:\n",
        "    「周囲の単語」（`to`, `the`, `to`, `deposit`など）のベクトルを $W_{in}$ から引き出し、それらを**平均**します。この平均ベクトルが「文脈」を表します。\n",
        "    （数式: $\\mathbf{\\hat{v}} = \\frac{1}{2m} \\sum \\mathbf{v}_{context}$）\n",
        "3.  **予測**:\n",
        "    この平均文脈ベクトル $\\mathbf{\\hat{v}}$ を、もう一つの重み行列 $W_{out}$（出力層）に入力し、中心の単語 `[ ? ]` が辞書のどの単語であるかの確率を予測します。\n",
        "4.  **辞書の更新**:\n",
        "    「正解は `bank` だった」とAIに教え、予測が正解に近づくようにネットワークを更新します。この時、**$W_{in}$（入力辞書）と $W_{out}$（出力辞書）の両方が、誤差逆伝播によって少しだけ修正されます。**\n",
        "5.  **繰り返し**:\n",
        "    この「予測→更新」を、大量のテキスト全体で何億回も繰り返します。\n",
        "\n",
        "学習が完了すると、$W_{in}$（入力辞書）には、文脈予測タスクを解くために最適化されたベクトル群が残ります。私たちが「単語の埋め込みベクトル」として使うのは、主にこの $W_{in}$ です。\n",
        "\n",
        "以下の図は、このCBOWモデルの学習の模式図です。"
      ],
      "metadata": {
        "id": "afF7Yf7m051T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🤖 (SVG修正版 v7) Word2Vec (CBOW) の学習模式図\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# CBOWモデルの学習プロセスを示すSVG\n",
        "# 誤差逆伝播の破線を、曲線(Q)から直線(L)に変更\n",
        "# 矢印が(4)と(2)のブロックの下部中央に上向きで入るように修正\n",
        "svg_cbow_string = \"\"\"\n",
        "<svg width=\"900px\" viewBox=\"0 0 850 450\" xmlns=\"http://www.w3.org/2000/svg\" font-family=\"Arial, sans-serif\">\n",
        "\n",
        "    <defs>\n",
        "        <marker id=\"arrowhead\" markerWidth=\"10\" markerHeight=\"7\" refX=\"0\" refY=\"3.5\" orient=\"auto\">\n",
        "            <polygon points=\"0 0, 10 3.5, 0 7\" fill=\"#333\"/>\n",
        "        </marker>\n",
        "        <marker id=\"arrowhead-red-up\" markerWidth=\"7\" markerHeight=\"10\"\n",
        "                refX=\"3.5\" refY=\"10\" orient=\"auto\" markerUnits=\"strokeWidth\">\n",
        "            <polygon points=\"0 10, 3.5 0, 7 10\" fill=\"#D32F2F\"/>\n",
        "        </marker>\n",
        "    </defs>\n",
        "    <text x=\"425\" y=\"30\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">\n",
        "        Word2Vec (CBOWモデル) の学習の仕組み\n",
        "    </text>\n",
        "    <text x=\"425\" y=\"55\" font-size=\"16\" text-anchor=\"middle\" fill=\"#555\">\n",
        "        タスク: 「周囲の単語」から「中心の単語」を予測する\n",
        "    </text>\n",
        "\n",
        "    <g id=\"input-sentence\">\n",
        "        <text x=\"180\" y=\"100\" font-size=\"16\" text-anchor=\"middle\" fill=\"#666\">入力文の例:</text>\n",
        "        <rect x=\"250\" y=\"80\" width=\"50\" height=\"35\" rx=\"5\" fill=\"#F0F0F0\" stroke=\"#AAA\"/>\n",
        "        <text x=\"275\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to</text>\n",
        "        <rect x=\"310\" y=\"80\" width=\"50\" height=\"35\" rx=\"5\" fill=\"#F0F0F0\" stroke=\"#AAA\"/>\n",
        "        <text x=\"335\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">the</text>\n",
        "\n",
        "        <rect x=\"370\" y=\"80\" width=\"50\" height=\"35\" rx=\"5\" fill=\"#FF6B6B\" stroke=\"#D32F2F\" stroke-width=\"2\"/>\n",
        "        <text x=\"395\" y=\"102\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#FFF\">[ ? ]</text>\n",
        "\n",
        "        <rect x=\"430\" y=\"80\" width=\"50\" height=\"35\" rx=\"5\" fill=\"#F0F0F0\" stroke=\"#AAA\"/>\n",
        "        <text x=\"455\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to</text>\n",
        "        <rect x=\"490\" y=\"80\" width=\"65\" height=\"35\" rx=\"5\" fill=\"#F0F0F0\" stroke=\"#AAA\"/>\n",
        "        <text x=\"522.5\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">deposit</text>\n",
        "        <text x=\"595\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">...</text>\n",
        "    </g>\n",
        "\n",
        "    <g id=\"step1-input\">\n",
        "        <text x=\"100\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">1. 入力 (文脈語)</text>\n",
        "        <rect x=\"60\" y=\"170\" width=\"80\" height=\"30\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\"/>\n",
        "        <text x=\"100\" y=\"190\" font-size=\"14\" text-anchor=\"middle\">\"to\"</text>\n",
        "        <rect x=\"60\" y=\"210\" width=\"80\" height=\"30\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\"/>\n",
        "        <text x=\"100\" y=\"230\" font-size=\"14\" text-anchor=\"middle\">\"the\"</text>\n",
        "        <rect x=\"60\" y=\"250\" width=\"80\" height=\"30\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\"/>\n",
        "        <text x=\"100\" y=\"270\" font-size=\"14\" text-anchor=\"middle\">\"to\"</text>\n",
        "        <rect x=\"60\" y=\"290\" width=\"80\" height=\"30\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\"/>\n",
        "        <text x=\"100\" y=\"310\" font-size=\"14\" text-anchor=\"middle\">\"deposit\"</text>\n",
        "    </g>\n",
        "\n",
        "    <g id=\"step2-embedding\">\n",
        "        <text x=\"260\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">2. 埋め込み層 (辞書)</text>\n",
        "        <rect x=\"190\" y=\"170\" width=\"160\" height=\"150\" rx=\"8\" fill=\"#FFF9C4\" stroke=\"#FBC02D\"/>\n",
        "\n",
        "        <text x=\"270\" y=\"305\" font-size=\"14\" text-anchor=\"middle\">\n",
        "            重み行列 W<tspan baseline-shift=\"sub\" font-size=\"0.8em\">in</tspan>\n",
        "        </text>\n",
        "\n",
        "        <text x=\"200\" y=\"190\" font-size=\"12\" fill=\"#E65100\" font-weight=\"bold\">これが学習対象であり、</text>\n",
        "        <text x=\"200\" y=\"210\" font-size=\"12\" fill=\"#E65100\" font-weight=\"bold\">最終的な「単語ベクトル」</text>\n",
        "        <text x=\"200\" y=\"230\" font-size=\"12\" fill=\"#E65100\" font-weight=\"bold\">そのものになる。</text>\n",
        "    </g>\n",
        "\n",
        "    <path d=\"M 145 240 L 185 240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
        "    <text x=\"165\" y=\"260\" font-size=\"10\" fill=\"#333\" text-anchor=\"middle\">辞書引き</text>\n",
        "\n",
        "    <g id=\"step3-average\">\n",
        "        <text x=\"440\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">3. 中間層 (平均)</text>\n",
        "        <rect x=\"400\" y=\"220\" width=\"80\" height=\"40\" rx=\"20\" fill=\"#E8F5E9\" stroke=\"#388E3C\"/>\n",
        "        <text x=\"440\" y=\"245\" font-size=\"14\" text-anchor=\"middle\">平均化</text>\n",
        "\n",
        "        <path d=\"M 355 240 L 395 240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
        "        <text x=\"375\" y=\"225\" font-size=\"10\" fill=\"#333\" text-anchor=\"middle\">文脈ベクトル</text>\n",
        "    </g>\n",
        "\n",
        "    <g id=\"step4-output-layer\">\n",
        "        <text x=\"590\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">4. 出力層</text>\n",
        "        <rect x=\"520\" y=\"170\" width=\"140\" height=\"150\" rx=\"8\" fill=\"#FCE4EC\" stroke=\"#C2185B\"/>\n",
        "        <text x=\"590\" y=\"305\" font-size=\"14\" text-anchor=\"middle\">\n",
        "            重み行列 W<tspan baseline-shift=\"sub\" font-size=\"0.8em\">out</tspan>\n",
        "        </text>\n",
        "\n",
        "        <path d=\"M 485 240 L 515 240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
        "    </g>\n",
        "\n",
        "    <g id=\"step5-softmax\">\n",
        "        <text x=\"750\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">5. 予測 (Softmax)</text>\n",
        "        <rect x=\"700\" y=\"170\" width=\"100\" height=\"150\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\"/>\n",
        "        <text x=\"750\" y=\"190\" font-size=\"12\" text-anchor=\"middle\">全語彙の確率</text>\n",
        "        <text x=\"750\" y=\"210\" font-size=\"11\" text-anchor=\"middle\">\"apple\": 0.01%</text>\n",
        "        <text x=\"750\" y=\"230\" font-size=\"11\" text-anchor=\"middle\">\"river\": 0.03%</text>\n",
        "        <rect x=\"705\" y=\"240\" width=\"90\" height=\"20\" fill=\"#C8E6C9\" stroke=\"#2E7D32\"/>\n",
        "        <text x=\"750\" y=\"254\" font-size=\"12\" font-weight=\"bold\" text-anchor=\"middle\">\"bank\": 92%</text>\n",
        "        <text x=\"750\" y=\"275\" font-size=\"11\" text-anchor=\"middle\">...</text>\n",
        "\n",
        "        <path d=\"M 665 240 L 695 240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
        "    </g>\n",
        "\n",
        "    <g id=\"update-loop\">\n",
        "        <text x=\"750\" y=\"340\" font-size=\"14\" fill=\"#4CAF50\" text-anchor=\"middle\">正解: \"bank\"</text>\n",
        "        <text x=\"750\" y=\"360\" font-size=\"14\" fill=\"#D32F2F\" text-anchor=\"middle\">誤差を計算</text>\n",
        "\n",
        "        <path d=\"M 750 360 L 750 390 L 590 390 L 590 325\"\n",
        "              fill=\"none\" stroke=\"#D32F2F\" stroke-width=\"2\" stroke-dasharray=\"5,5\"/>\n",
        "\n",
        "        <path d=\"M 750 360 L 750 410 L 270 410 L 270 325\"\n",
        "              fill=\"none\" stroke=\"#D32F2F\" stroke-width=\"2\" stroke-dasharray=\"5,5\"\"/>\n",
        "\n",
        "        <text x=\"425\" y=\"375\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#D32F2F\">\n",
        "            誤差を逆伝播し、重み\n",
        "            <tspan x=\"425\" dy=\"1.2em\">\n",
        "                (W<tspan baseline-shift=\"sub\" font-size=\"0.8em\">in</tspan> と W<tspan baseline-shift=\"sub\" font-size=\"0.8em\">out</tspan>) を更新する\n",
        "            </tspan>\n",
        "        </text>\n",
        "        </g>\n",
        "</svg>\n",
        "\"\"\"\n",
        "\n",
        "# HTML() を使って表示します\n",
        "display(HTML(svg_cbow_string))"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "bDSAB-WD0938",
        "outputId": "470e5dff-2400-48cf-c39e-7f0b5c51e41d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<svg width=\"900px\" viewBox=\"0 0 850 450\" xmlns=\"http://www.w3.org/2000/svg\" font-family=\"Arial, sans-serif\">\n",
              "\n",
              "    <defs>\n",
              "        <marker id=\"arrowhead\" markerWidth=\"10\" markerHeight=\"7\" refX=\"0\" refY=\"3.5\" orient=\"auto\">\n",
              "            <polygon points=\"0 0, 10 3.5, 0 7\" fill=\"#333\"/>\n",
              "        </marker>\n",
              "        <marker id=\"arrowhead-red-up\" markerWidth=\"7\" markerHeight=\"10\" \n",
              "                refX=\"3.5\" refY=\"10\" orient=\"auto\" markerUnits=\"strokeWidth\">\n",
              "            <polygon points=\"0 10, 3.5 0, 7 10\" fill=\"#D32F2F\"/>\n",
              "        </marker>\n",
              "    </defs>\n",
              "    <text x=\"425\" y=\"30\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">\n",
              "        Word2Vec (CBOWモデル) の学習の仕組み\n",
              "    </text>\n",
              "    <text x=\"425\" y=\"55\" font-size=\"16\" text-anchor=\"middle\" fill=\"#555\">\n",
              "        タスク: 「周囲の単語」から「中心の単語」を予測する\n",
              "    </text>\n",
              "\n",
              "    <g id=\"input-sentence\">\n",
              "        <text x=\"180\" y=\"100\" font-size=\"16\" text-anchor=\"middle\" fill=\"#666\">入力文の例:</text>\n",
              "        <rect x=\"250\" y=\"80\" width=\"50\" height=\"35\" rx=\"5\" fill=\"#F0F0F0\" stroke=\"#AAA\"/>\n",
              "        <text x=\"275\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to</text>\n",
              "        <rect x=\"310\" y=\"80\" width=\"50\" height=\"35\" rx=\"5\" fill=\"#F0F0F0\" stroke=\"#AAA\"/>\n",
              "        <text x=\"335\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">the</text>\n",
              "        \n",
              "        <rect x=\"370\" y=\"80\" width=\"50\" height=\"35\" rx=\"5\" fill=\"#FF6B6B\" stroke=\"#D32F2F\" stroke-width=\"2\"/>\n",
              "        <text x=\"395\" y=\"102\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#FFF\">[ ? ]</text>\n",
              "        \n",
              "        <rect x=\"430\" y=\"80\" width=\"50\" height=\"35\" rx=\"5\" fill=\"#F0F0F0\" stroke=\"#AAA\"/>\n",
              "        <text x=\"455\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to</text>\n",
              "        <rect x=\"490\" y=\"80\" width=\"65\" height=\"35\" rx=\"5\" fill=\"#F0F0F0\" stroke=\"#AAA\"/>\n",
              "        <text x=\"522.5\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">deposit</text>\n",
              "        <text x=\"595\" y=\"102\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">...</text>\n",
              "    </g>\n",
              "    \n",
              "    <g id=\"step1-input\">\n",
              "        <text x=\"100\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">1. 入力 (文脈語)</text>\n",
              "        <rect x=\"60\" y=\"170\" width=\"80\" height=\"30\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\"/>\n",
              "        <text x=\"100\" y=\"190\" font-size=\"14\" text-anchor=\"middle\">\"to\"</text>\n",
              "        <rect x=\"60\" y=\"210\" width=\"80\" height=\"30\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\"/>\n",
              "        <text x=\"100\" y=\"230\" font-size=\"14\" text-anchor=\"middle\">\"the\"</text>\n",
              "        <rect x=\"60\" y=\"250\" width=\"80\" height=\"30\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\"/>\n",
              "        <text x=\"100\" y=\"270\" font-size=\"14\" text-anchor=\"middle\">\"to\"</text>\n",
              "        <rect x=\"60\" y=\"290\" width=\"80\" height=\"30\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\"/>\n",
              "        <text x=\"100\" y=\"310\" font-size=\"14\" text-anchor=\"middle\">\"deposit\"</text>\n",
              "    </g>\n",
              "\n",
              "    <g id=\"step2-embedding\">\n",
              "        <text x=\"260\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">2. 埋め込み層 (辞書)</text>\n",
              "        <rect x=\"190\" y=\"170\" width=\"160\" height=\"150\" rx=\"8\" fill=\"#FFF9C4\" stroke=\"#FBC02D\"/>\n",
              "        \n",
              "        <text x=\"270\" y=\"305\" font-size=\"14\" text-anchor=\"middle\">\n",
              "            重み行列 W<tspan baseline-shift=\"sub\" font-size=\"0.8em\">in</tspan>\n",
              "        </text>\n",
              "        \n",
              "        <text x=\"200\" y=\"190\" font-size=\"12\" fill=\"#E65100\" font-weight=\"bold\">これが学習対象であり、</text>\n",
              "        <text x=\"200\" y=\"210\" font-size=\"12\" fill=\"#E65100\" font-weight=\"bold\">最終的な「単語ベクトル」</text>\n",
              "        <text x=\"200\" y=\"230\" font-size=\"12\" fill=\"#E65100\" font-weight=\"bold\">そのものになる。</text>\n",
              "    </g>\n",
              "    \n",
              "    <path d=\"M 145 240 L 185 240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
              "    <text x=\"165\" y=\"260\" font-size=\"10\" fill=\"#333\" text-anchor=\"middle\">辞書引き</text>\n",
              "\n",
              "    <g id=\"step3-average\">\n",
              "        <text x=\"440\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">3. 中間層 (平均)</text>\n",
              "        <rect x=\"400\" y=\"220\" width=\"80\" height=\"40\" rx=\"20\" fill=\"#E8F5E9\" stroke=\"#388E3C\"/>\n",
              "        <text x=\"440\" y=\"245\" font-size=\"14\" text-anchor=\"middle\">平均化</text>\n",
              "        \n",
              "        <path d=\"M 355 240 L 395 240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
              "        <text x=\"375\" y=\"225\" font-size=\"10\" fill=\"#333\" text-anchor=\"middle\">文脈ベクトル</text>\n",
              "    </g>\n",
              "\n",
              "    <g id=\"step4-output-layer\">\n",
              "        <text x=\"590\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">4. 出力層</text>\n",
              "        <rect x=\"520\" y=\"170\" width=\"140\" height=\"150\" rx=\"8\" fill=\"#FCE4EC\" stroke=\"#C2185B\"/>\n",
              "        <text x=\"590\" y=\"305\" font-size=\"14\" text-anchor=\"middle\">\n",
              "            重み行列 W<tspan baseline-shift=\"sub\" font-size=\"0.8em\">out</tspan>\n",
              "        </text>\n",
              "        \n",
              "        <path d=\"M 485 240 L 515 240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
              "    </g>\n",
              "\n",
              "    <g id=\"step5-softmax\">\n",
              "        <text x=\"750\" y=\"150\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\">5. 予測 (Softmax)</text>\n",
              "        <rect x=\"700\" y=\"170\" width=\"100\" height=\"150\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\"/>\n",
              "        <text x=\"750\" y=\"190\" font-size=\"12\" text-anchor=\"middle\">全語彙の確率</text>\n",
              "        <text x=\"750\" y=\"210\" font-size=\"11\" text-anchor=\"middle\">\"apple\": 0.01%</text>\n",
              "        <text x=\"750\" y=\"230\" font-size=\"11\" text-anchor=\"middle\">\"river\": 0.03%</text>\n",
              "        <rect x=\"705\" y=\"240\" width=\"90\" height=\"20\" fill=\"#C8E6C9\" stroke=\"#2E7D32\"/>\n",
              "        <text x=\"750\" y=\"254\" font-size=\"12\" font-weight=\"bold\" text-anchor=\"middle\">\"bank\": 92%</text>\n",
              "        <text x=\"750\" y=\"275\" font-size=\"11\" text-anchor=\"middle\">...</text>\n",
              "        \n",
              "        <path d=\"M 665 240 L 695 240\" fill=\"none\" stroke=\"#333\" stroke-width=\"2\" marker-end=\"url(#arrowhead)\"/>\n",
              "    </g>\n",
              "\n",
              "    <g id=\"update-loop\">\n",
              "        <text x=\"750\" y=\"340\" font-size=\"14\" fill=\"#4CAF50\" text-anchor=\"middle\">正解: \"bank\"</text>\n",
              "        <text x=\"750\" y=\"360\" font-size=\"14\" fill=\"#D32F2F\" text-anchor=\"middle\">誤差を計算</text>\n",
              "        \n",
              "        <path d=\"M 750 360 L 750 390 L 590 390 L 590 325\" \n",
              "              fill=\"none\" stroke=\"#D32F2F\" stroke-width=\"2\" stroke-dasharray=\"5,5\"/>\n",
              "\n",
              "        <path d=\"M 750 360 L 750 410 L 270 410 L 270 325\"\n",
              "              fill=\"none\" stroke=\"#D32F2F\" stroke-width=\"2\" stroke-dasharray=\"5,5\"\"/>\n",
              "        \n",
              "        <text x=\"425\" y=\"375\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#D32F2F\">\n",
              "            誤差を逆伝播し、重み\n",
              "            <tspan x=\"425\" dy=\"1.2em\">\n",
              "                (W<tspan baseline-shift=\"sub\" font-size=\"0.8em\">in</tspan> と W<tspan baseline-shift=\"sub\" font-size=\"0.8em\">out</tspan>) を更新する\n",
              "            </tspan>\n",
              "        </text>\n",
              "        </g>\n",
              "</svg>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧮 学習の結果: 「意味」の計算\n",
        "\n",
        "この学習の結果、`bank`（銀行）という単語のベクトルは、`deposit`（預金）や `money`（お金）といった単語のベクトルと、「予測タスクを解く上で関連性が高い」と判断され、ベクトル空間上で近い位置に配置されるよう調整されます。\n",
        "\n",
        "Word2Vecのすごいところは、単なる近さだけでなく、**単語と単語の「関係性」も「方向」として学習する**点にあります。\n",
        "\n",
        "---\n",
        "\n",
        "### 🧭 有名な計算式: king - man + woman = queen\n",
        "\n",
        "Word2Vecの世界で最も有名なのが、この計算式です。\n",
        "\n",
        "> `(king) - (man) + (woman) ≈ (queen)`\n",
        "\n",
        "「`king`（王様）」が持つベクトルから「`man`（男性）」っぽさを引き算し、そこに「`woman`（女性）」っぽさを足し算すると、その計算結果は「`queen`（女王）」が持つベクトルと、ほぼ同じ値になります。\n",
        "\n",
        "なぜなら、Word2Vecは大量の文章を学習することで、\n",
        "* 「`man`」から「`king`」への「方向」（関係性）\n",
        "* 「`woman`」から「`queen`」への「方向」（関係性）\n",
        "\n",
        "これらがどちらも**「（その性別の）王族になる」**という意味の「関係性」であると気づき、この **2つの矢印がほぼ同じ向き・同じ長さ（＝平行）** になるように、各単語の場所（ベクトル）を調整するからです。\n",
        "\n",
        "\n",
        "\n",
        "### ✨ 重要性\n",
        "\n",
        "Word2Vecは、大量の文章から **「単語と単語の間の関係性」を自動的に見つけ出し、それを「ベクトル空間上の方向（数値の差分）」として学習** します。\n",
        "\n",
        "「性別」だけでなく、「国の首都（例: Japan → Tokyo、France → Paris）」や「動詞の時制（例: walk → walked）」といった様々な関係性も、同じように「方向」として数値に保存されます。\n",
        "\n",
        "この「**意味を位置や方向として捉え、計算できるようにする**」という考え方は、のちのGPTやBERTといった、より複雑なAIが文章の「文脈」を深く理解するための、大切な土台となっています。\n",
        "\n",
        "（※ただし、先ほどの `bank` の例のように、Word2Vecの段階では「銀行」と「土手」のベクトルは区別できず、混ざり合ってしまいます。この「文脈」の問題はステップ1で見ます。）"
      ],
      "metadata": {
        "id": "1KMOFBFmRDcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧮 計算のステップ（図の解説）\n",
        "\n",
        "この計算が成り立つ理由を、図を見ながらステップで追ってみましょう。\n",
        "\n",
        "1.  **「王族の方向」を求める:**\n",
        "    まず、「`man`」の場所から「`king`」の場所への矢印（図の赤い矢印）を計算します。これが「王族になる」という関係性を表す「方向」です。\n",
        "    （計算式: `king - man`）\n",
        "\n",
        "2.  **「`woman`」の場所に足す:**\n",
        "    次に、この「王族の方向」の矢印を、「`woman`」の場所からスタートさせます。\n",
        "    （計算式: `woman + (king - man)`）\n",
        "\n",
        "3.  **着地点は「`queen`」:**\n",
        "    図が示すように、この地図では関係性（矢印）が平行に保たれているため、「`woman`」の場所から「王族の方向」へ進んだ先には、ちょうど「`queen`」の場所が位置しています。\n",
        "\n",
        "これが「意味の計算」の正体です。\n",
        "\n",
        "---\n",
        "\n",
        "## ✨ 重要性\n",
        "\n",
        "Word2Vecは、ただ単語を暗記するのではなく、大量の文章から **「単語と単語の間の関係性」を自動的に見つけ出し、それを「地図上の方向（数値の差分）」として学習** します。\n",
        "\n",
        "「性別」だけでなく、「国の首都（例: Japan → Tokyo、France → Paris）」や「動詞の時制（例: walk → walked）」といった様々な関係性も、同じように「方向」として数値に保存されます。\n",
        "\n",
        "この「**意味を位置や方向として捉え、計算できるようにする**」という考え方は、のちのGPTやBERTといった、より複雑なAIが文章の「文脈」を深く理解するための、とても大切な土台となっています。"
      ],
      "metadata": {
        "id": "CUDr5Nxaw_EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ベクトル演算 `A - B + C` (計算経路の可視化)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### フォーム\n",
        "#@markdown ベクトル演算 `A - B + C` を実行します。\n",
        "#@markdown (例: A=king, B=man, C=woman)\n",
        "wordA = \"king\" #@param {\"type\":\"string\",\"placeholder\":\"king\"}\n",
        "wordB = \"man\" #@param {\"type\":\"string\",\"placeholder\":\"man\"}\n",
        "wordC = \"woman\" #@param {\"type\":\"string\",\"placeholder\":\"woman\"}\n",
        "#@markdown ---\n",
        "#@markdown ### 可視化オプション\n",
        "#@markdown 次元削減 (PCA) を用いて、計算結果を2Dプロットします。\n",
        "#@markdown 上位何件の類似単語をプロットに含めますか？\n",
        "enable_visualization = True\n",
        "top_k_to_plot = 7 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "#@markdown ---\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import sys # エラー表示用\n",
        "\n",
        "# 0. gensimモデル (model_wv) がロードされているか確認\n",
        "if 'model_wv' not in globals():\n",
        "    print(\"エラー: ステップ0のGloVeモデル (model_wv) がロードされていません。\", file=sys.stderr)\n",
        "    print(\"（このセルの前の「【実行】GloVeモデルのロード」セルを実行してください。）\", file=sys.stderr)\n",
        "else:\n",
        "    # 1. 埋め込み行列の取得 (numpy配列)\n",
        "    embedding_matrix = model_wv.vectors\n",
        "\n",
        "    print(f\"辞書の総単語数: {embedding_matrix.shape[0]}\")\n",
        "    print(f\"ベクトルの次元数: {embedding_matrix.shape[1]}\") # 50次元のはず\n",
        "\n",
        "    # 2. 単語のベクトルを取得\n",
        "    words = [wordA.lower(), wordB.lower(), wordC.lower()]\n",
        "    vectors = {}\n",
        "    all_words_found = True\n",
        "\n",
        "    for word in words:\n",
        "        if not word: # 空文字列チェック\n",
        "            print(f\"エラー: 単語が入力されていません。\", file=sys.stderr)\n",
        "            all_words_found = False\n",
        "            break\n",
        "        try:\n",
        "            # gensim (KeyedVectors) を使ってベクトルを取得\n",
        "            vectors[word] = model_wv[word]\n",
        "            print(f\"'{word}' のベクトル取得完了。\")\n",
        "\n",
        "        except KeyError:\n",
        "            print(f\"エラー: 単語 '{word}' がGloVe辞書にありません。\", file=sys.stderr)\n",
        "            all_words_found = False\n",
        "            break\n",
        "\n",
        "    # 3. ベクトル演算 (Numpy)\n",
        "    if all_words_found:\n",
        "        vecA = vectors[wordA.lower()]\n",
        "        vecB = vectors[wordB.lower()]\n",
        "        vecC = vectors[wordC.lower()]\n",
        "\n",
        "        # ▼▼▼ 要求された全6点のベクトルを計算 ▼▼▼\n",
        "        vec_A_minus_B = vecA - vecB   # A - B (関係性ベクトル)\n",
        "        vec_A_plus_C = vecA + vecC    # A + C (中間計算)\n",
        "        vec_Result = vecA - vecB + vecC # A - B + C (最終結果)\n",
        "\n",
        "        # 4. 最終結果 (A-B+C) と全単語の類似度を計算\n",
        "        # gensim の most_similar は positive=[A, C], negative=[B] で\n",
        "        # この計算 (A-B+C) を内部的に行ってくれます。\n",
        "\n",
        "        print(f\"\\n--- 計算結果 (vec({wordA}) - vec({wordB}) + vec({wordC})) に近い単語 Top {top_k_to_plot} ---\")\n",
        "\n",
        "        try:\n",
        "            similar_words = model_wv.most_similar(\n",
        "                positive=[wordA.lower(), wordC.lower()],\n",
        "                negative=[wordB.lower()],\n",
        "                topn=top_k_to_plot\n",
        "            )\n",
        "\n",
        "            # 結果の整形\n",
        "            results = []\n",
        "            top_result_labels = [] # 可視化用\n",
        "            top_result_vectors = [] # 可視化用\n",
        "\n",
        "            for token_str, similarity in similar_words:\n",
        "                results.append((token_str, similarity))\n",
        "\n",
        "                if token_str not in words:\n",
        "                    top_result_labels.append(token_str)\n",
        "                    top_result_vectors.append(model_wv[token_str])\n",
        "\n",
        "            df_results = pd.DataFrame(results, columns=[\"Word\", \"Cosine Similarity\"])\n",
        "            print(df_results.to_string(index=False))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"類似度計算中にエラーが発生しました: {e}\", file=sys.stderr)\n",
        "            enable_visualization = False # 可視化をスキップ\n",
        "\n",
        "        # --- 6. ▼▼▼ 可視化機能（要求された6点と4矢印） ▼▼▼ ---\n",
        "        if enable_visualization:\n",
        "            print(\"\\n--- 2D可視化 (PCA) ---\")\n",
        "\n",
        "            # 1. 収集するベクトルとラベル\n",
        "            labels_to_plot = [wordA, wordB, wordC]\n",
        "            vectors_to_plot = [vecA, vecB, vecC]\n",
        "\n",
        "            labels_to_plot.extend(top_result_labels)\n",
        "            vectors_to_plot.extend(top_result_vectors)\n",
        "\n",
        "            # ▼▼▼ A-B, A+C, Result もPCAの対象に含める ▼▼▼\n",
        "            ab_label = f\"{wordA}-{wordB}\"\n",
        "            ac_label = f\"{wordA}+{wordC}\"\n",
        "            result_label = f\"Result ({wordA}-{wordB}+{wordC})\"\n",
        "\n",
        "            labels_to_plot.extend([ab_label, ac_label, result_label])\n",
        "            vectors_to_plot.extend([vec_A_minus_B, vec_A_plus_C, vec_Result])\n",
        "\n",
        "            # 2. Numpy配列に (既にNumpy配列です)\n",
        "            vectors_np = np.array(vectors_to_plot)\n",
        "\n",
        "            # 3. PCAで2次元に削減\n",
        "            pca = PCA(n_components=2)\n",
        "            vectors_2d = pca.fit_transform(vectors_np)\n",
        "\n",
        "            # 4. プロット\n",
        "            plt.figure(figsize=(12, 9))\n",
        "\n",
        "            # 座標を辞書に\n",
        "            coords = {label: vectors_2d[i] for i, label in enumerate(labels_to_plot)}\n",
        "\n",
        "            # A, B, C をプロット\n",
        "            plt.scatter(coords[wordA][0], coords[wordA][1], c='blue', s=150, label=f\"A: {wordA}\", zorder=5)\n",
        "            plt.annotate(wordA, (coords[wordA][0], coords[wordA][1]), fontsize=12, zorder=6)\n",
        "\n",
        "            plt.scatter(coords[wordB][0], coords[wordB][1], c='red', s=150, label=f\"B: {wordB}\", zorder=5)\n",
        "            plt.annotate(wordB, (coords[wordB][0], coords[wordB][1]), fontsize=12, zorder=6)\n",
        "\n",
        "            plt.scatter(coords[wordC][0], coords[wordC][1], c='green', s=150, label=f\"C: {wordC}\", zorder=5)\n",
        "            plt.annotate(wordC, (coords[wordC][0], coords[wordC][1]), fontsize=12, zorder=6)\n",
        "\n",
        "            # 計算結果 (A-B) をプロット\n",
        "            plt.scatter(coords[ab_label][0], coords[ab_label][1], c='purple', s=150, marker='X', label=ab_label, zorder=10)\n",
        "            plt.annotate(ab_label, (coords[ab_label][0], coords[ab_label][1]), fontsize=14, zorder=11)\n",
        "\n",
        "            # 計算結果 (A+C) をプロット\n",
        "            plt.scatter(coords[ac_label][0], coords[ac_label][1], c='brown', s=150, marker='X', label=ac_label, zorder=10)\n",
        "            plt.annotate(ac_label, (coords[ac_label][0], coords[ac_label][1]), fontsize=14, zorder=11)\n",
        "\n",
        "            # 最終計算結果 (Result) をプロット\n",
        "            plt.scatter(coords[result_label][0], coords[result_label][1], c='orange', s=300, marker='*', label=result_label, zorder=10)\n",
        "            plt.annotate(result_label, (coords[result_label][0], coords[result_label][1]), fontsize=14, zorder=11)\n",
        "\n",
        "            # Top-k の結果をプロット\n",
        "            for label in top_result_labels:\n",
        "                plt.scatter(coords[label][0], coords[label][1], c='gray', s=50, alpha=0.7)\n",
        "                plt.annotate(label, (coords[label][0], coords[label][1]), alpha=0.7)\n",
        "\n",
        "            # 6. ▼▼▼ 要求された4つの矢印 ▼▼▼\n",
        "\n",
        "            # 矢印1: A -> A-B (赤色, -B)\n",
        "            plt.arrow(coords[wordA][0], coords[wordA][1],\n",
        "                      coords[ab_label][0] - coords[wordA][0],\n",
        "                      coords[ab_label][1] - coords[wordA][1],\n",
        "                      color='red', linestyle='--', head_width=0.01, length_includes_head=True, alpha=0.5, label=f\"- {wordB}\")\n",
        "\n",
        "            # 矢印2: A -> A+C (緑色, +C)\n",
        "            plt.arrow(coords[wordA][0], coords[wordA][1],\n",
        "                      coords[ac_label][0] - coords[wordA][0],\n",
        "                      coords[ac_label][1] - coords[wordA][1],\n",
        "                      color='green', linestyle='--', head_width=0.01, length_includes_head=True, alpha=0.5, label=f\"+ {wordC}\")\n",
        "\n",
        "            # 矢印3: A+C -> Result (赤色, -B)\n",
        "            plt.arrow(coords[ac_label][0], coords[ac_label][1],\n",
        "                      coords[result_label][0] - coords[ac_label][0],\n",
        "                      coords[result_label][1] - coords[ac_label][1],\n",
        "                      color='red', linestyle='-.', head_width=0.01, length_includes_head=True, alpha=0.5)\n",
        "\n",
        "            # 矢印4: A-B -> Result (緑色, +C)\n",
        "            plt.arrow(coords[ab_label][0], coords[ab_label][1],\n",
        "                      coords[result_label][0] - coords[ab_label][0],\n",
        "                      coords[result_label][1] - coords[ab_label][1],\n",
        "                      color='green', linestyle='-.', head_width=0.01, length_includes_head=True, alpha=0.5)\n",
        "\n",
        "            plt.title(f\"PCA Visualization of Vector Calculation: (A={wordA}, B={wordB}, C={wordC})\")\n",
        "            plt.xlabel(\"Principal Component 1\")\n",
        "            plt.ylabel(\"Principal Component 2\")\n",
        "            plt.legend()\n",
        "            plt.grid(True, linestyle='--', alpha=0.6)\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "r9-C0RHikwY6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 👩‍🎓 **課題 (ステップ0 考察・穴埋め)**\n",
        ">\n",
        "> 1.  上のセルのフォーム機能を使って、`king - man + woman` 以外のアナロジー（類推）計算を試してみましょう。\n",
        ">     * 例: `Paris - France + Japan` （首都の関係）\n",
        ">     * 例: `walking - walk + swam` （動詞の時制の関係）\n",
        "> 2.  （穴埋め） Word2Vec は、大量の文章を学習し、単語を「埋め込み＿＿＿＿＿」として表現します。このベクトルでは、単語の＿＿＿＿＿がベクトル空間上の「位置」や「向き（関係性）」として表現されます。\n",
        ">\n",
        ">     例えば、`king - man` が表す「男性から王様への関係性」のベクトルと、`queen - woman` が表す「＿＿＿＿＿から女王への関係性」のベクトルが、空間上でほぼ同じ向きになります。このように、単語を計算可能なベクトルに変換することで、コンピュータは単語の＿＿＿＿＿を扱えるようになります。\n",
        "> 3.  このグラフは、ベクトル演算 `A - B + C = Result` が `A - B = Result - C` という関係（平行四辺形）で視覚化できることを示しています。このことから、`A - B`（例: `king - man`）のベクトルは、どのような「意味」や「関係性」を持っていると解釈できるか、あなたの考えを説明してください。（自由記述）\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "NvSTrkJrxKKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 ステップ1: (BERT) 埋め込みベクトルと「文脈」の問題\n",
        "\n",
        "ステップ0では、Word2Vec (GloVe) が単語の意味をベクトル（空間上の位置）で表現できることを見ました。\n",
        "\n",
        "しかし、Word2Vecには大きな問題があります。それは「**多義語**」を区別できないことです。\n",
        "Word2Vecでは、`bank`（銀行）と `bank`（土手）は、辞書に1つの単語として登録されているため、**常に同じベクトル**になってしまいます。これでは、文脈を正しく理解できません。\n",
        "\n",
        "この「文脈」の問題を解決したのが、**Transformer** という技術です。\n",
        "\n",
        "---\n",
        "### 🧠 Transformer, BERT, GPT とは？\n",
        "\n",
        "この課題では `BERT` や `GPT` というモデルを使います。これらは全て `Transformer` という強力なAIモデル（設計図）をベースにしています。\n",
        "\n",
        "（RNN、LSTMといった技術を知らなくても、以下の説明で進められます。）\n",
        "\n",
        "* **Transformer (トランスフォーマー):**\n",
        "    * 2017年に登場した、AIモデルの「設計図」あるいは「アーキテクチャ」の名前です。\n",
        "    * 文章の中の「どの単語とどの単語が強く関係しているか」（＝文脈）を非常にうまく扱うことができます。この中核技術が「**Attention**（アテンション）」（ステップ2で学びます）です。\n",
        "\n",
        "* **BERT (バート):**\n",
        "    * `Transformer` の設計図を使って「**文脈を深く理解する**」ためにGoogleが訓練したモデルです。\n",
        "        * 「感情分析」や「質問応答」など様々なタスクに使われます（これは **ファインチューニング** と呼ばれます）。\n",
        "        * しかし、そのための「基礎体力」を作る訓練（**事前学習**）として、BERTが（下記のGPTとは異なる方法で）解くのが「**穴埋め問題**（Masked Language Model）」です。\n",
        "    * **なぜ穴埋め問題か？** それは、文の**両側**（左右）の文脈を同時に見るようにモデルを強制できるからです。\n",
        "        * 例えば、`I went to the [?] to deposit money.` (私はお金をおろしに [?] に行った) という穴埋めを解くには、左側の `went to the` だけでなく、右側の `deposit money` を見ることが不可欠です。\n",
        "        * このように両側の文脈を見る訓練（事前学習）を大量に行うことで、BERTは「単語の文脈上の深い意味」を理解するようになります。この「賢くなったBERT」をベースとして、感情分析などの応用タスクを（ファインチューニングで）解かせるわけです。\n",
        "\n",
        "* **GPT (ジーピーティー):**\n",
        "    * `Transformer` の設計図を使って「**次の単語を予測する**」ためにOpenAIが訓練したモデルです。\n",
        "    * 主な目的は、文章の続きを生成すること（例: \"I sat on the river\" → `bank`）です。\n",
        "    * 文の**左側**（すでに入力された部分）だけを参照して、次に来る単語を予測します。\n",
        "\n",
        "なお、今回の課題のゴールは、このGPTの仕組みを理解することです。\n",
        "\n"
      ],
      "metadata": {
        "id": "-zvvL_bzlsHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🤖 BERT vs GPT 模式図 (日本語解説 + 英語例文)\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# 埋め込みたいSVGコード（BERT vs GPT）\n",
        "# 解説文は日本語、例文（トークン）は英語\n",
        "svg_code_string_jp_en = \"\"\"\n",
        "<svg width=\"900px\" viewBox=\"0 0 800 500\" xmlns=\"http://www.w3.org/2000/svg\"\n",
        "     font-family=\"Arial, 'Hiragino Sans', 'Hiragino Kaku Gothic ProN', 'Meiryo', sans-serif\">\n",
        "\n",
        "    <text x=\"400\" y=\"30\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">\n",
        "        BERT vs GPT: Attention パターンの違い\n",
        "    </text>\n",
        "\n",
        "    <g id=\"bert-section\">\n",
        "        <text x=\"200\" y=\"80\" font-size=\"20\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1976D2\">\n",
        "            BERT (双方向)\n",
        "        </text>\n",
        "        <text x=\"200\" y=\"105\" font-size=\"14\" text-anchor=\"middle\" fill=\"#666\">\n",
        "            マスク言語モデル (穴埋め)\n",
        "        </text>\n",
        "\n",
        "        <g id=\"bert-words\">\n",
        "            <rect x=\"40\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "            <text x=\"70\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">I went</text>\n",
        "\n",
        "            <rect x=\"110\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "            <text x=\"140\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to the</text>\n",
        "\n",
        "            <rect x=\"180\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#FF6B6B\" stroke=\"#D32F2F\" stroke-width=\"3\"/>\n",
        "            <text x=\"210\" y=\"155\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#FFF\">[MASK]</text>\n",
        "\n",
        "            <rect x=\"250\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "            <text x=\"280\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to</text>\n",
        "\n",
        "            <rect x=\"320\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "            <text x=\"350\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">deposit</text>\n",
        "        </g>\n",
        "\n",
        "\n",
        "        <defs>\n",
        "            <marker id=\"arrowhead-blue\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\">\n",
        "                <polygon points=\"0 0, 10 3, 0 6\" fill=\"#1976D2\" />\n",
        "            </marker>\n",
        "        </defs>\n",
        "\n",
        "        <path d=\"M 100 145 Q 150 110 185 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
        "        <path d=\"M 170 145 Q 180 120 195 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
        "        <path d=\"M 280 145 Q 250 110 225 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
        "        <path d=\"M 350 145 Q 280 100 220 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
        "\n",
        "        <text x=\"210\" y=\"205\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#4CAF50\">↓</text>\n",
        "        <rect x=\"170\" y=\"210\" width=\"80\" height=\"35\" rx=\"5\" fill=\"#C8E6C9\" stroke=\"#4CAF50\" stroke-width=\"2\"/>\n",
        "        <text x=\"210\" y=\"232\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2E7D32\">bank</text>\n",
        "    </g>\n",
        "\n",
        "    <g id=\"gpt-section\">\n",
        "        <text x=\"600\" y=\"80\" font-size=\"20\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#7B1FA2\">\n",
        "            GPT (単一方向)\n",
        "        </text>\n",
        "        <text x=\"600\" y=\"105\" font-size=\"14\" text-anchor=\"middle\" fill=\"#666\">\n",
        "            次単語予測\n",
        "        </text>\n",
        "\n",
        "        <g id=\"gpt-words\">\n",
        "            <rect x=\"460\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "            <text x=\"485\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">I sat</text>\n",
        "\n",
        "            <rect x=\"520\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "            <text x=\"545\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">on</text>\n",
        "\n",
        "            <rect x=\"580\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "            <text x=\"605\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">the</text>\n",
        "\n",
        "            <rect x=\"640\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "            <text x=\"665\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">river</text>\n",
        "\n",
        "            <rect x=\"700\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#FFE0B2\" stroke=\"#F57C00\" stroke-width=\"3\"/>\n",
        "            <text x=\"725\" y=\"155\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#E65100\">?</text>\n",
        "        </g>\n",
        "\n",
        "        <defs>\n",
        "            <marker id=\"arrowhead-purple\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\">\n",
        "                <polygon points=\"0 0, 10 3, 0 6\" fill=\"#7B1FA2\" />\n",
        "            </marker>\n",
        "        </defs>\n",
        "\n",
        "        <path d=\"M 510 145 Q 600 110 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
        "        <path d=\"M 570 145 Q 640 120 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowah-purple)\" opacity=\"0.7\"/>\n",
        "        <path d=\"M 630 145 Q 670 130 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
        "        <path d=\"M 690 150 L 705 150\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
        "\n",
        "        <text x=\"725\" y=\"205\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#4CAF50\">↓</text>\n",
        "        <rect x=\"685\" y=\"210\" width=\"80\" height=\"35\" rx=\"5\" fill=\"#C8E6C9\" stroke=\"#4CAF50\" stroke-width=\"2\"/>\n",
        "        <text x=\"725\" y=\"232\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2E7D32\">bank</text>\n",
        "    </g>\n",
        "\n",
        "    <g id=\"explanations\">\n",
        "        <rect x=\"30\" y=\"280\" width=\"340\" height=\"180\" rx=\"8\" fill=\"#F5F5F5\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "        <text x=\"200\" y=\"305\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1976D2\">\n",
        "            双方向コンテキスト\n",
        "        </text>\n",
        "        <text x=\"40\" y=\"330\" font-size=\"13\" fill=\"#333\">\n",
        "            • 単語の<tspan font-weight=\"bold\">両方向</tspan>（左右）を参照する\n",
        "        </text>\n",
        "        <text x=\"40\" y=\"355\" font-size=\"13\" fill=\"#333\">\n",
        "            • [MASK] トークンが全ての単語に注目\n",
        "        </text>\n",
        "        <text x=\"40\" y=\"380\" font-size=\"13\" fill=\"#333\">\n",
        "            • 訓練: マスクされた単語を予測 (穴埋め)\n",
        "        </text>\n",
        "        <text x=\"40\" y=\"405\" font-size=\"13\" fill=\"#333\">\n",
        "            • 最適なタスク: <tspan font-weight=\"bold\">読解・理解系</tspan>タスク\n",
        "        </text>\n",
        "        <text x=\"40\" y=\"430\" font-size=\"13\" fill=\"#333\">\n",
        "            (分類, 質問応答, 固有表現抽出)\n",
        "        </text>\n",
        "\n",
        "        <rect x=\"430\" y=\"280\" width=\"340\" height=\"180\" rx=\"8\" fill=\"#F5F5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "        <text x=\"600\" y=\"305\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#7B1FA2\">\n",
        "            単一方向 (Causal)\n",
        "        </text>\n",
        "        <text x=\"440\" y=\"330\" font-size=\"13\" fill=\"#333\">\n",
        "            • 単語の<tspan font-weight=\"bold\">左側</tspan>（過去）のみ参照する\n",
        "        </text>\n",
        "        <text x=\"440\" y=\"355\" font-size=\"13\" fill=\"#333\">\n",
        "            • 予測トークン(?)は過去の単語のみ注目\n",
        "        </text>\n",
        "        <text x=\"440\" y=\"380\" font-size=\"13\" fill=\"#333\">\n",
        "            • 訓練: 次に来る単語を予測\n",
        "        </text>\n",
        "        <text x=\"440\" y=\"405\" font-size=\"13\" fill=\"#333\">\n",
        "            • 最適なタスク: <tspan font-weight=\"bold\">テキスト生成系</tspan>タスク\n",
        "        </text>\n",
        "        <text x=\"440\" y=\"430\" font-size=\"13\" fill=\"#333\">\n",
        "            (テキスト補完, 対話)\n",
        "        </text>\n",
        "    </g>\n",
        "\n",
        "    <rect x=\"150\" y=\"470\" width=\"500\" height=\"25\" rx=\"5\" fill=\"#FFF9C4\" stroke=\"#F57F17\" stroke-width=\"2\"/>\n",
        "    <text x=\"400\" y=\"488\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#F57F17\">\n",
        "        キー: BERTは文脈を理解し、GPTは続きを生成する\n",
        "    </text>\n",
        "</svg>\n",
        "\"\"\"\n",
        "\n",
        "# HTML() を使って日本語版SVGを表示します\n",
        "display(HTML(svg_code_string_jp_en))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1JiQIKzcsXWq",
        "outputId": "71cc4fb0-5a3c-4f85-e411-faca9680a0cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<svg width=\"900px\" viewBox=\"0 0 800 500\" xmlns=\"http://www.w3.org/2000/svg\" \n",
              "     font-family=\"Arial, 'Hiragino Sans', 'Hiragino Kaku Gothic ProN', 'Meiryo', sans-serif\">\n",
              "    \n",
              "    <text x=\"400\" y=\"30\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">\n",
              "        BERT vs GPT: Attention パターンの違い\n",
              "    </text>\n",
              "\n",
              "    <g id=\"bert-section\">\n",
              "        <text x=\"200\" y=\"80\" font-size=\"20\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1976D2\">\n",
              "            BERT (双方向)\n",
              "        </text>\n",
              "        <text x=\"200\" y=\"105\" font-size=\"14\" text-anchor=\"middle\" fill=\"#666\">\n",
              "            マスク言語モデル (穴埋め)\n",
              "        </text>\n",
              "\n",
              "        <g id=\"bert-words\">\n",
              "            <rect x=\"40\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
              "            <text x=\"70\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">I went</text>\n",
              "\n",
              "            <rect x=\"110\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
              "            <text x=\"140\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to the</text>\n",
              "\n",
              "            <rect x=\"180\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#FF6B6B\" stroke=\"#D32F2F\" stroke-width=\"3\"/>\n",
              "            <text x=\"210\" y=\"155\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#FFF\">[MASK]</text>\n",
              "\n",
              "            <rect x=\"250\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
              "            <text x=\"280\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to</text>\n",
              "\n",
              "            <rect x=\"320\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
              "            <text x=\"350\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">deposit</text>\n",
              "        </g>\n",
              "\n",
              "\n",
              "        <defs>\n",
              "            <marker id=\"arrowhead-blue\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\">\n",
              "                <polygon points=\"0 0, 10 3, 0 6\" fill=\"#1976D2\" />\n",
              "            </marker>\n",
              "        </defs>\n",
              "\n",
              "        <path d=\"M 100 145 Q 150 110 185 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
              "        <path d=\"M 170 145 Q 180 120 195 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
              "        <path d=\"M 280 145 Q 250 110 225 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
              "        <path d=\"M 350 145 Q 280 100 220 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
              "\n",
              "        <text x=\"210\" y=\"205\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#4CAF50\">↓</text>\n",
              "        <rect x=\"170\" y=\"210\" width=\"80\" height=\"35\" rx=\"5\" fill=\"#C8E6C9\" stroke=\"#4CAF50\" stroke-width=\"2\"/>\n",
              "        <text x=\"210\" y=\"232\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2E7D32\">bank</text>\n",
              "    </g>\n",
              "\n",
              "    <g id=\"gpt-section\">\n",
              "        <text x=\"600\" y=\"80\" font-size=\"20\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#7B1FA2\">\n",
              "            GPT (単一方向)\n",
              "        </text>\n",
              "        <text x=\"600\" y=\"105\" font-size=\"14\" text-anchor=\"middle\" fill=\"#666\">\n",
              "            次単語予測\n",
              "        </text>\n",
              "\n",
              "        <g id=\"gpt-words\">\n",
              "            <rect x=\"460\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
              "            <text x=\"485\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">I sat</text>\n",
              "\n",
              "            <rect x=\"520\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
              "            <text x=\"545\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">on</text>\n",
              "\n",
              "            <rect x=\"580\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
              "            <text x=\"605\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">the</text>\n",
              "\n",
              "            <rect x=\"640\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
              "            <text x=\"665\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">river</text>\n",
              "\n",
              "            <rect x=\"700\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#FFE0B2\" stroke=\"#F57C00\" stroke-width=\"3\"/>\n",
              "            <text x=\"725\" y=\"155\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#E65100\">?</text>\n",
              "        </g>\n",
              "\n",
              "        <defs>\n",
              "            <marker id=\"arrowhead-purple\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\">\n",
              "                <polygon points=\"0 0, 10 3, 0 6\" fill=\"#7B1FA2\" />\n",
              "            </marker>\n",
              "        </defs>\n",
              "\n",
              "        <path d=\"M 510 145 Q 600 110 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
              "        <path d=\"M 570 145 Q 640 120 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowah-purple)\" opacity=\"0.7\"/>\n",
              "        <path d=\"M 630 145 Q 670 130 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
              "        <path d=\"M 690 150 L 705 150\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
              "\n",
              "        <text x=\"725\" y=\"205\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#4CAF50\">↓</text>\n",
              "        <rect x=\"685\" y=\"210\" width=\"80\" height=\"35\" rx=\"5\" fill=\"#C8E6C9\" stroke=\"#4CAF50\" stroke-width=\"2\"/>\n",
              "        <text x=\"725\" y=\"232\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2E7D32\">bank</text>\n",
              "    </g>\n",
              "\n",
              "    <g id=\"explanations\">\n",
              "        <rect x=\"30\" y=\"280\" width=\"340\" height=\"180\" rx=\"8\" fill=\"#F5F5F5\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
              "        <text x=\"200\" y=\"305\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1976D2\">\n",
              "            双方向コンテキスト\n",
              "        </text>\n",
              "        <text x=\"40\" y=\"330\" font-size=\"13\" fill=\"#333\">\n",
              "            • 単語の<tspan font-weight=\"bold\">両方向</tspan>（左右）を参照する\n",
              "        </text>\n",
              "        <text x=\"40\" y=\"355\" font-size=\"13\" fill=\"#333\">\n",
              "            • [MASK] トークンが全ての単語に注目\n",
              "        </text>\n",
              "        <text x=\"40\" y=\"380\" font-size=\"13\" fill=\"#333\">\n",
              "            • 訓練: マスクされた単語を予測 (穴埋め)\n",
              "        </text>\n",
              "        <text x=\"40\" y=\"405\" font-size=\"13\" fill=\"#333\">\n",
              "            • 最適なタスク: <tspan font-weight=\"bold\">読解・理解系</tspan>タスク\n",
              "        </text>\n",
              "        <text x=\"40\" y=\"430\" font-size=\"13\" fill=\"#333\">\n",
              "            (分類, 質問応答, 固有表現抽出)\n",
              "        </text>\n",
              "\n",
              "        <rect x=\"430\" y=\"280\" width=\"340\" height=\"180\" rx=\"8\" fill=\"#F5F5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
              "        <text x=\"600\" y=\"305\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#7B1FA2\">\n",
              "            単一方向 (Causal)\n",
              "        </text>\n",
              "        <text x=\"440\" y=\"330\" font-size=\"13\" fill=\"#333\">\n",
              "            • 単語の<tspan font-weight=\"bold\">左側</tspan>（過去）のみ参照する\n",
              "        </text>\n",
              "        <text x=\"440\" y=\"355\" font-size=\"13\" fill=\"#333\">\n",
              "            • 予測トークン(?)は過去の単語のみ注目\n",
              "        </text>\n",
              "        <text x=\"440\" y=\"380\" font-size=\"13\" fill=\"#333\">\n",
              "            • 訓練: 次に来る単語を予測\n",
              "        </text>\n",
              "        <text x=\"440\" y=\"405\" font-size=\"13\" fill=\"#333\">\n",
              "            • 最適なタスク: <tspan font-weight=\"bold\">テキスト生成系</tspan>タスク\n",
              "        </text>\n",
              "        <text x=\"440\" y=\"430\" font-size=\"13\" fill=\"#333\">\n",
              "            (テキスト補完, 対話)\n",
              "        </text>\n",
              "    </g>\n",
              "\n",
              "    <rect x=\"150\" y=\"470\" width=\"500\" height=\"25\" rx=\"5\" fill=\"#FFF9C4\" stroke=\"#F57F17\" stroke-width=\"2\"/>\n",
              "    <text x=\"400\" y=\"488\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#F57F17\">\n",
              "        キー: BERTは文脈を理解し、GPTは続きを生成する\n",
              "    </text>\n",
              "</svg>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 埋め込み「層（Layer）」とは？\n",
        "\n",
        "`Transformer`（BERTやGPT）も、Word2Vecと同様に、まず単語をベクトルに変換する仕組みを持っています。これが「埋め込み」であり、それを行うのが「埋め込み層」です。\n",
        "\n",
        "> **層（Layer）とは？**\n",
        "> AIモデル（ニューラルネットワーク）は、特定の処理を行う「**部品**」を何層にも重ねて作られています。この「部品」のことを「**層（Layer）**」と呼びます。\n",
        ">\n",
        "> **埋め込み層（Embedding Layer）とは？**\n",
        "> 「埋め込み層」は、その部品（層）の一つで、モデルの一番最初の入り口にあります。\n",
        "> その役割は、**巨大な「辞書（対応表）」を持つこと**です。\n",
        ">\n",
        "> 1.  入力として「単語のID番号」（例: `apple` = 5025番）を受け取ります。\n",
        "> 2.  「埋め込み層」が持っている辞書テーブルから、「5025番に対応するベクトル（例: 768次元の数値リスト）」を引き出します。\n",
        "> 3.  そのベクトルを、次の層（ステップ2で学ぶAttention層など）に渡します。\n",
        "\n",
        "> **埋め込み（Embedding）とは？**\n",
        "> この「埋め込み層」によって変換された「単語を表すベクトル」のこと、あるいは「ベクトルに変換する処理」そのものを指します。\n",
        ">\n",
        "> * **古い方法 (One-Hot):** `apple` = `[0, 0, 1, 0, ...]` のように、単語の場所だけ1にする方法。これでは単語同士の類似性が分かりません。\n",
        "> * **今の方法 (Word2Vec, BERTなど):** 「単語の意味は、周囲の単語によって決まる」（**分布仮説**）に基づき、意味が近い単語がベクトル空間上でも近くに配置されるよう、埋め込み層の「辞書テーブル（ベクトル）」を学習させます。\n",
        "\n",
        "### BERTの埋め込みは「初期値」\n",
        "\n",
        "Word2VecとBERT/GPTの大きな違いは、この埋め込みベクトルの使い方です。\n",
        "\n",
        "* **Word2Vec:** `bank` のベクトル（辞書の値）は一度計算されたら固定です。\n",
        "* **BERT/GPT:** 埋め込み層が持つベクトルは、あくまで「**初期値**」（文脈を考慮する前の、単語の基本的な意味）です。\n",
        "\n",
        "BERTやGPTは、`Transformer` の中核技術である「**Attention**」（ステップ2で学びます）を使って、文脈（\"river\" や \"money\"）に応じて、この初期値のベクトルを「文脈を反映したベクトル」に動的に作り変えます。\n",
        "\n",
        "このステップでは、まずBERT（軽量版の `DistilBERT`）の「初期値」である埋め込み層の動作を確認します。\n",
        "Word2Vecと同様に、**この段階ではまだ文脈が反映されていない**（＝ `bank` のベクトルが文脈によらず同じである）ことを見てみましょう。"
      ],
      "metadata": {
        "id": "6UA_pKltqL51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 埋め込みベクトルの取得\n",
        "#@markdown ---\n",
        "#@markdown ### フォーム\n",
        "#@markdown 比較したい2つの文と、その文に含まれる「共通の単語」を指定してください。\n",
        "text1 = \"I ate an apple.\" #@param {type:\"string\"}\n",
        "text2 = \"I work at Apple.\" #@param {type:\"string\"}\n",
        "common_word = \"apple\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### ベクトル表示オプション\n",
        "#@markdown ベクトルの値をどこまで表示しますか？（-1で全て表示）\n",
        "display_dimensions = 5 #@param {type:\"slider\", min:-1, max:768, step:1}\n",
        "#@markdown ---\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 軽量なBERT (DistilBERT) のモデルとトークナイザがロード済みか確認\n",
        "if 'model' not in globals():\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "    model = DistilBertModel.from_pretrained(model_name).to(device)\n",
        "    print(\"Model and Tokenizer loaded.\")\n",
        "\n",
        "# --- 1. トークナイズ ---\n",
        "inputs1 = tokenizer(text1, return_tensors=\"pt\").to(device)\n",
        "inputs2 = tokenizer(text2, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(f\"文1: {tokenizer.convert_ids_to_tokens(inputs1['input_ids'][0])}\")\n",
        "print(f\"文2: {tokenizer.convert_ids_to_tokens(inputs2['input_ids'][0])}\")\n",
        "\n",
        "# --- 2. 共通単語のIDを取得 ---\n",
        "try:\n",
        "    token_id = tokenizer.convert_tokens_to_ids(common_word.lower())\n",
        "    if token_id == tokenizer.unk_token_id:\n",
        "        raise ValueError(f\"単語 '{common_word}' が辞書にありません。\")\n",
        "\n",
        "    print(f\"\\n'{common_word}' のID番号: {token_id}\")\n",
        "except ValueError as e:\n",
        "    print(e)\n",
        "    # ここで処理を中断\n",
        "    pass\n",
        "\n",
        "if 'token_id' in locals() and token_id != tokenizer.unk_token_id:\n",
        "    # --- 3. 埋め込み層からベクトルを取得 ---\n",
        "    # (モデルの埋め込み層を直接使います)\n",
        "    embedding_layer = model.embeddings.word_embeddings\n",
        "    token_id_tensor = torch.tensor([token_id]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embedding_vector = embedding_layer(token_id_tensor)\n",
        "\n",
        "    print(f\"\\n'{common_word}' の埋め込みベクトルのサイズ: {embedding_vector.shape}\")\n",
        "    print(\"（これは 768次元のベクトルです）\")\n",
        "\n",
        "    # ▼▼▼ 質問2への回答（ベクトル表示） ▼▼▼\n",
        "    if display_dimensions == -1:\n",
        "        print(f\"ベクトル全体 (768次元):\\n{embedding_vector[0]}\")\n",
        "    else:\n",
        "        print(f\"ベクトルの一部（最初の{display_dimensions}次元）: {embedding_vector[0, :display_dimensions]}\")\n",
        "    # ▲▲▲ 修正ここまで ▲▲▲\n",
        "\n",
        "    # --- 4. 2つの文での差の確認 ---\n",
        "    with torch.no_grad():\n",
        "        embeddings1 = model.embeddings(inputs1['input_ids'])\n",
        "        embeddings2 = model.embeddings(inputs2['input_ids'])\n",
        "\n",
        "    index1_list = (inputs1['input_ids'][0] == token_id).nonzero(as_tuple=True)[0]\n",
        "    index2_list = (inputs2['input_ids'][0] == token_id).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    if len(index1_list) == 0 or len(index2_list) == 0:\n",
        "        print(f\"\\nエラー: '{common_word}' が両方の文に含まれているか確認してください。\")\n",
        "    else:\n",
        "        index1 = index1_list[0]\n",
        "        index2 = index2_list[0]\n",
        "        vec1_embedding = embeddings1[0, index1]\n",
        "        vec2_embedding = embeddings2[0, index2]\n",
        "\n",
        "        difference = torch.abs(vec1_embedding - vec2_embedding).sum()\n",
        "        print(f\"\\n埋め込み層の段階では、2つの'{common_word}'ベクトルの差は: {difference.item()}\")\n",
        "        if difference.item() < 1e-6:\n",
        "            print(\"➡️ 成功: 同じベクトルです。文脈に関わらず固定です。\")\n",
        "        else:\n",
        "            print(\"➡️ 失敗？: ベクトルに差があります。\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cS7yP6hCRxYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 👩‍🎓 **課題**\n",
        ">\n",
        "> 1.  上のセルのフォーム機能を使って、`text1`, `text2`, `common_word` を、あなたが考えた別の多義語（例: `bat`, `right`, `fly`など）を含む文に変えてみましょう。\n",
        "> 2.  `common_word` を変えても、ベクトルの差（difference）が常にほぼゼロになることを確認してください。\n",
        "> 3.  （考察）なぜこの段階では、文脈が違うのにベクトルが同じになるのか、ステップ1の解説を読んで自分の言葉で説明してください。（自由記述）"
      ],
      "metadata": {
        "id": "x-mdgj8yV8L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💡 ステップ2: 文脈で意味が変わる (Contextualization by Attention)\n",
        "\n",
        "ステップ1では、単語（`apple`）は文脈（「食べる」か「働く」か）に関わらず、同じベクトルでした。\n",
        "これでは、「果物」と「会社」の区別がつきません。\n",
        "\n",
        "そこで登場するのが **Attention（アテンション）** 機構です。\n",
        "\n",
        "Attentionは、ご指摘の通り、「文の中にある他の単語の埋め込みベクトルとともに重み付け」をします。\n",
        "\n",
        "* \"I ate an **apple**.\" という文では、Attentionは `apple` のベクトルを作るときに、`ate` や `an` に「注目」します。\n",
        "* \"I work at **Apple**.\" という文では、Attentionは `work` や `at` に「注目」します。\n",
        "\n",
        "この「注目（Attention）」による重み付け集約の結果、元の「埋め込みベクトル」（文脈なし）が、文脈を反映した「**文脈ベクトル**」（Contextual Vector）に変換されます。\n",
        "\n",
        "今度は、`DistilBERT` の「埋め込み層」だけでなく、モデル全体（Attention層を含む）に文を入力して、出力されるベクトルがどう変わるか見てみましょう。多義語 `bank`（銀行／土手）で試します。"
      ],
      "metadata": {
        "id": "d8srH30KSRd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 文脈ベクトルの比較\n",
        "#@markdown ---\n",
        "#@markdown ### フォーム\n",
        "#@markdown 比較したい2つの文と、その文に含まれる「共通の単語」を指定してください。\n",
        "text_context1 = \"I sat on the river bank.\" #@param {type:\"string\"}\n",
        "text_context2 = \"I went to the bank to deposit money.\" #@param {type:\"string\"}\n",
        "common_word_context = \"bank\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown 比較したい文脈語（文1から）\n",
        "context_word1 = \"river\" #@param {type:\"string\"}\n",
        "#@markdown 比較したい文脈語（文2から）\n",
        "context_word2 = \"money\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. トークナイズ ---\n",
        "inputs_context1 = tokenizer(text_context1, return_tensors=\"pt\").to(device)\n",
        "inputs_context2 = tokenizer(text_context2, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# --- 2. モデル全体に入力 ---\n",
        "with torch.no_grad():\n",
        "    outputs_context1 = model(**inputs_context1)\n",
        "    outputs_context2 = model(**inputs_context2)\n",
        "\n",
        "contextual_embeddings1 = outputs_context1.last_hidden_state\n",
        "contextual_embeddings2 = outputs_context2.last_hidden_state\n",
        "\n",
        "# --- 3. IDを取得 ---\n",
        "try:\n",
        "    common_token_id = tokenizer.convert_tokens_to_ids(common_word_context.lower())\n",
        "    context1_token_id = tokenizer.convert_tokens_to_ids(context_word1.lower())\n",
        "    context2_token_id = tokenizer.convert_tokens_to_ids(context_word2.lower())\n",
        "\n",
        "    # --- 4. ベクトルを抽出 ---\n",
        "    common_index1 = (inputs_context1['input_ids'][0] == common_token_id).nonzero(as_tuple=True)[0][0]\n",
        "    common_index2 = (inputs_context2['input_ids'][0] == common_token_id).nonzero(as_tuple=True)[0][0]\n",
        "\n",
        "    context1_index = (inputs_context1['input_ids'][0] == context1_token_id).nonzero(as_tuple=True)[0][0]\n",
        "    context2_index = (inputs_context2['input_ids'][0] == context2_token_id).nonzero(as_tuple=True)[0][0]\n",
        "\n",
        "    common_vec1_context = contextual_embeddings1[0, common_index1].cpu().numpy()\n",
        "    common_vec2_context = contextual_embeddings2[0, common_index2].cpu().numpy()\n",
        "\n",
        "    context1_vec = contextual_embeddings1[0, context1_index].cpu().numpy()\n",
        "    context2_vec = contextual_embeddings2[0, context2_index].cpu().numpy()\n",
        "\n",
        "    # --- 5. 比較 (コサイン類似度) ---\n",
        "    similarity_common = cosine_similarity([common_vec1_context], [common_vec2_context])\n",
        "    print(f\"\\n--- 文脈ベクトルの比較 ({common_word_context} vs {common_word_context}) ---\")\n",
        "    print(f\"文1の'{common_word_context}' と 文2の'{common_word_context}' のコサイン類似度: {similarity_common[0][0]:.4f}\")\n",
        "    if similarity_common[0][0] < 0.7:\n",
        "        print(\"➡️ 成功: 類似度が低く、意味が区別されています。\")\n",
        "    else:\n",
        "        print(\"➡️ 注意: 類似度があまり下がりませんでした。\")\n",
        "\n",
        "\n",
        "    # --- 6. (おまけ) 文脈との類似度 ---\n",
        "    sim1_v_context1 = cosine_similarity([common_vec1_context], [context1_vec])[0][0]\n",
        "    sim1_v_context2 = cosine_similarity([common_vec1_context], [context2_vec])[0][0]\n",
        "    sim2_v_context1 = cosine_similarity([common_vec2_context], [context1_vec])[0][0]\n",
        "    sim2_v_context2 = cosine_similarity([common_vec2_context], [context2_vec])[0][0]\n",
        "\n",
        "    print(\"\\n--- (おまけ) 文脈との類似度 ---\")\n",
        "\n",
        "    # 可視化 (英語)\n",
        "    df = pd.DataFrame({\n",
        "        f\"'{common_word_context}' (Context 1)\": [sim1_v_context1, sim1_v_context2],\n",
        "        f\"'{common_word_context}' (Context 2)\": [sim2_v_context1, sim2_v_context2]\n",
        "    }, index=[f\"'{context_word1}' (from C1)\", f\"'{context_word2}' (from C2)\"])\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(df, annot=True, cmap=\"viridis\", fmt=\".4f\")\n",
        "    plt.title(f\"Similarity between '{common_word_context}' and context words\")\n",
        "    plt.show()\n",
        "\n",
        "except IndexError:\n",
        "    print(f\"\\nエラー: フォームに入力された単語が、指定された文に含まれているか確認してください。\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nエラーが発生しました: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YeshR6kVSSqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 👩‍🎓 **課題 (ステップ2)**\n",
        ">\n",
        "> 1.  上のセルのフォーム機能を使って、ステップ1で試した多義語（例: `bat`）の文を入力してみましょう。\n",
        "> 2.  ステップ1では類似度が高かった（差がほぼ0だった）のに対し、ステップ2（Attention通過後）では類似度が低くなる（例: 0.7未満）ことを確認してください。\n",
        "> 3.  （考察）なぜAttentionを通すと、同じ単語でも文脈によってベクトルが変わるのか、ステップ2の解説を読んで自分の言葉で説明してください。（自由記述）"
      ],
      "metadata": {
        "id": "-lVsz80TV0GE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✍️ ステップ3: 次の単語を予測する (Generation)\n",
        "\n",
        "ステップ2では、BERT（の軽量版）が文脈を読んで単語の意味を理解する仕組み（穴埋め問題が得意）を見ました。BERTは文の**両側**（左と右）を見て文脈を判断します。\n",
        "\n",
        "一方、**GPT** は、ご指摘の通り「**そこまでの文章の意味を集約**」し、「**次の単語**」を予測することに特化しています。左から右へ、一方通行で処理します。\n",
        "\n",
        "GPTが次の単語を予測するプロセスは、以下の通りです。\n",
        "\n",
        "1.  入力された文（例: \"My hobby is\"）をトークンに区切ります。\n",
        "2.  各トークンを「埋め込みベクトル」にします（ステップ1）。\n",
        "3.  Attention（GPTの場合は、自分より左側の単語にしか注目できない仕組み）を使って、各トークンを「文脈ベクトル」に変換します（ステップ2）。\n",
        "4.  **重要:** 文の**一番最後**のトークン（この例では `is`）の「文脈ベクトル」を取り出します。このベクトルが、「\"My hobby is\" まで読んだ時点での文脈の集約」です。\n",
        "5.  ご指摘の通り、この集約された文脈ベクトル（例: 768次元）を、ある特別な「**全結合層（Linear Layer）**」に入力します。\n",
        "6.  この全結合層は、**LM Head (Language Model Head)** と呼ばれ、その出力の次元数は、モデルが知っている**辞書の総単語数**（例: 50257次元）になっています。\n",
        "7.  出力された50257個の数値（**ロジット**と呼ばれます）を、**Softmax** という関数に通すことで、合計が1.0になる「確率」に変換します。\n",
        "8.  これが、「\"My hobby is\" の次にくる単語の確率分布」です。\n",
        "9.  確率が最も高い単語（例: `reading`）を選んだり、確率に基づいてランダムに選んだり（サンプリング）することで、文章が生成されます。\n",
        "\n",
        "実際に `GPT-2`（GPTの初期の公開モデル）を使って、次の単語の確率が計算される様子を見てみましょう。"
      ],
      "metadata": {
        "id": "yAnDU6ObSc4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 次単語予測の確率分布\n",
        "#@markdown ---\n",
        "#@markdown ### フォーム\n",
        "#@markdown 次の単語を予測させたい「入力文（プロンプト）」を指定してください。\n",
        "prompt_text = \"My hobby is\" #@param {type:\"string\"}\n",
        "#@markdown 表示したい予測候補の数\n",
        "top_k_value = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "#@markdown ---\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GPT-2 のモデルとトークナイザがロード済みか確認\n",
        "if 'model_gpt' not in globals():\n",
        "    model_name_gpt = \"gpt2\"\n",
        "    tokenizer_gpt = GPT2Tokenizer.from_pretrained(model_name_gpt)\n",
        "    model_gpt = GPT2LMHeadModel.from_pretrained(model_name_gpt).to(device)\n",
        "    if tokenizer_gpt.pad_token is None:\n",
        "        tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
        "        model_gpt.config.pad_token_id = model_gpt.config.eos_token_id\n",
        "    print(\"GPT-2 Model and Tokenizer loaded.\")\n",
        "\n",
        "# --- 1. トークナイズ ---\n",
        "inputs_gpt = tokenizer_gpt(prompt_text, return_tensors=\"pt\").to(device)\n",
        "input_ids = inputs_gpt['input_ids']\n",
        "\n",
        "print(f\"入力文: '{prompt_text}'\")\n",
        "\n",
        "# 'Ġ' が表示されないように修正\n",
        "token_ids_list = input_ids[0].tolist()\n",
        "tokens_display = [tokenizer_gpt.decode([token_id]) for token_id in token_ids_list]\n",
        "print(f\"  -> トークン: {tokens_display}\")\n",
        "\n",
        "\n",
        "# --- 2. モデルに入力 ---\n",
        "with torch.no_grad():\n",
        "    outputs_gpt = model_gpt(**inputs_gpt)\n",
        "\n",
        "# --- 3. ロジットの取得 ---\n",
        "logits = outputs_gpt.logits\n",
        "if logits.shape[1] == 0: # 入力が空の場合\n",
        "     print(\"エラー: 入力が空です。\")\n",
        "else:\n",
        "    # --- 4. 「最後のトークン」のロジットを取得 ---\n",
        "    last_token_logits = logits[0, -1, :]\n",
        "\n",
        "    # --- 5. Softmax で確率に変換 ---\n",
        "    probabilities = F.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "    # --- 6. 確率が高い単語を表示 ---\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k_value)\n",
        "\n",
        "    print(f\"\\n--- 「{prompt_text}」の次にくる単語の予測 (Top {top_k_value}) ---\")\n",
        "\n",
        "    results = []\n",
        "    for i in range(top_k_value):\n",
        "        token_id = top_k_indices[i].item()\n",
        "        token_str = tokenizer_gpt.decode(token_id)\n",
        "        prob = top_k_probs[i].item()\n",
        "        results.append((token_str, prob * 100))\n",
        "\n",
        "    df_probs = pd.DataFrame(results, columns=[\"Predicted Word\", \"Probability (%)\"])\n",
        "    print(df_probs.to_string(index=False))\n",
        "\n",
        "    # --- 7. 可視化 (英語) ---\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    df_probs.set_index(\"Predicted Word\").plot(kind='bar', legend=False)\n",
        "    plt.title(f\"Next word probabilities for '{prompt_text}'\")\n",
        "    plt.ylabel(\"Probability (%)\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7) # ← 修正後\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- 8. 動的な結果の表示 ---\n",
        "    top_word = df_probs.iloc[0][\"Predicted Word\"]\n",
        "    print(f\"\\n➡️ GPTは '{top_word}' が次に来る確率が最も高いと予測しました！\")"
      ],
      "metadata": {
        "id": "5DYxpVD6SevU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 👩‍🎓 **課題 (ステップ3)**\n",
        ">\n",
        "> 1.  上のセルのフォーム機能を使って、`prompt_text` に様々な（英語の）文章を入力してみましょう。\n",
        "> 2.  （例: \"The capital of Japan is\", \"My hobby is\", \"I like to eat\" など）\n",
        "> 3.  GPTが予測する「次の単語」は、文脈として妥当でしょうか？ `top_k_value` を変えると、予測候補はどのように変わるか観察してください。\n",
        "> 4.  （考察）GPTは、なぜ「そこまでの文脈」だけで次の単語を予測できるのでしょうか？ ステップ3の解説（特にLM Head）を読んで、自分の言葉で説明してください。（自由記述）"
      ],
      "metadata": {
        "id": "wL7Vf_d7VwBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 振り返り課題（穴埋め）\n",
        "\n",
        "ここまでのステップで学んだことを、以下の穴埋め問題で振り返りましょう。\n",
        "\n",
        "（ `...` の部分に当てはまる適切な語句を考えてください）\n",
        "\n",
        "1.  コンピュータは単語をそのまま扱えないため、「`...`」(Embedding) という、単語を数値のリスト（ベクトル）に変換する層を使います。\n",
        "2.  ステップ1の段階では、`bank` のような多義語も、文脈に関わらず `...` (同じ/異なる) ベクトルで表現されます。\n",
        "3.  ステップ2では、`...` (Attention) 機構が、文中の他の単語に「注目」し、元のベクトルを文脈に応じた「文脈ベクトル」に変換しました。\n",
        "4.  ステップ3のGPTは、文の `...` (最初/最後) のトークンの文脈ベクトルを、「そこまでの文脈の集約」として利用します。\n",
        "5.  この集約されたベクトルを、出力次元が辞書の総単語数になっている `...` (LM Head / 埋め込み層) と呼ばれる全結合層に入力します。\n",
        "6.  最後に `...` (Softmax) 関数を使って、辞書の全単語が「次にくる確率」を計算し、テキストを生成します。"
      ],
      "metadata": {
        "id": "4xtIANXyZDgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 🚀 課題: BERT と GPT の違い\n",
        "\n",
        "私たちはステップ2・3で `BERT`（の軽量版）を、ステップ3で `GPT` を使いました。どちらも同じ「Transformer」という技術をベースにしていますが、目的が大きく異なります。\n",
        "\n",
        "### 1. 目的の違い（穴埋め）\n",
        "\n",
        "以下の説明を読み、`...` に（BERT / GPT）のどちらが入るか考えてみましょう。\n",
        "\n",
        "* **`...` (BERT)** は、文の**両側**（左右）の文脈を読んで、文中の単語の意味を深く理解するのが得意です。主なタスクは「穴埋め問題（Masked Language Model）」や「文章分類」です。\n",
        "* **`...` (GPT)** は、文の**左側**（そこまで）の文脈だけを読んで、**次**に来る単語を予測するのが得意です。主なタスクは「文章生成（Generative）」です。\n",
        "\n",
        "### 2. 「文脈ベクトル」の意味の違い\n",
        "\n",
        "この目的の違いは、「文脈ベクトル」が何を表しているかに現れます。\n",
        "\n",
        "* **BERTの文脈ベクトル:**\n",
        "    * `bank` のベクトルは、その単語「自体」が文脈（\"river\" や \"money\"）の中で持つ意味（土手/銀行）を強く表現します。穴埋めに使うため、その単語の意味特定が重要です。\n",
        "* **GPTの文脈ベクトル:**\n",
        "    * `bank` のベクトルも文脈（\"river\"）を読み込みますが、その目的は「`bank` **の次**にくる単語」を予測するための「途中経過」です。\n",
        "    * GPTが最も重要視するのは、**最後のトークン**のベクトルです。これが「そこまでの全情報の集約」となり、次の単語予測に使われます。\n",
        "\n",
        "ご指摘の通り、GPTにとって「`bank` が小川か銀行か」の区別は、**次の単語を予測するのに必要な分だけ**行われます。BERTほどその単語自体の意味を突き詰める必要はありません。\n",
        "\n",
        "### 3. 実践: BERTとGPTの「最後のベクトル」の比較\n",
        "\n",
        "同じ文をBERTとGPTに入力し、**最後のトークン**（`bank`）のベクトルが、それぞれどの単語と一番似ているか（近いか）を比較してみましょう。\n",
        "\n",
        "* BERT（`DistilBERT`）は `bank` の意味（`river` に近い）を強く反映するはずです。\n",
        "* GPT（`GPT-2`）は `bank` の意味も持ちつつ、「次」の予測のための情報を集約しているはずです。"
      ],
      "metadata": {
        "id": "g2oJzb7rZFVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧑‍🎓 BERT vs GPT ベクトル比較\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### フォーム\n",
        "#@markdown 比較したい共通の文（最後が多義語だと分かりやすい）\n",
        "common_text = \"I sat on the river bank\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- 1. BERT (DistilBERT) でベクトル取得 ---\n",
        "print(\"--- 1. BERT (DistilBERT) ---\")\n",
        "inputs_bert = tokenizer(common_text, return_tensors=\"pt\").to(device)\n",
        "tokens_bert = tokenizer.convert_ids_to_tokens(inputs_bert['input_ids'][0])\n",
        "print(f\"BERT トークン: {tokens_bert}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_bert = model(**inputs_bert)\n",
        "\n",
        "# BERTの最後のトークン (\"bank\") のベクトル\n",
        "# [CLS] トークンが先頭に、[SEP] トークンが末尾に追加されるため、\n",
        "# 文の最後の単語 'bank' は、リストの最後から2番目 (-2) になります。\n",
        "last_token_index_bert = -2\n",
        "bert_last_vec = outputs_bert.last_hidden_state[0, last_token_index_bert, :].cpu().numpy()\n",
        "\n",
        "# BERTの \"river\" のベクトル\n",
        "river_token_id = tokenizer.convert_tokens_to_ids(\"river\")\n",
        "river_index_bert_list = (inputs_bert['input_ids'][0] == river_token_id).nonzero(as_tuple=True)[0]\n",
        "if len(river_index_bert_list) == 0:\n",
        "    print(\"エラー: BERTトークナイザで 'river' が見つかりません。\")\n",
        "    bert_river_vec = None\n",
        "else:\n",
        "    river_index_bert = river_index_bert_list[0]\n",
        "    bert_river_vec = outputs_bert.last_hidden_state[0, river_index_bert, :].cpu().numpy()\n",
        "\n",
        "\n",
        "# --- 2. GPT-2 でベクトル取得 ---\n",
        "print(\"\\n--- 2. GPT-2 ---\")\n",
        "inputs_gpt = tokenizer_gpt(common_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# ▼▼▼ 'Ġ' が表示されないように修正 ▼▼▼\n",
        "token_ids_gpt_list = inputs_gpt['input_ids'][0].tolist()\n",
        "tokens_gpt_display = [tokenizer_gpt.decode([token_id]) for token_id in token_ids_gpt_list]\n",
        "print(f\"GPT トークン: {tokens_gpt_display}\")\n",
        "# ▲▲▲ 修正ここまで ▲▲▲\n",
        "\n",
        "with torch.no_grad():\n",
        "    # GPTは .last_hidden_state を直接返さないので、.transformer(...) から取得\n",
        "    gpt_hidden_states = model_gpt.transformer(**inputs_gpt).last_hidden_state\n",
        "\n",
        "# GPTの最後のトークン (\"bank\") のベクトル\n",
        "# (GPT-2は [CLS] や [SEP] を自動で追加しないため、文の最後がリストの最後です)\n",
        "last_token_index_gpt = -1 # 穴埋め: -1\n",
        "gpt_last_vec = gpt_hidden_states[0, last_token_index_gpt, :].cpu().numpy()\n",
        "\n",
        "# GPTの \"river\" のベクトル\n",
        "# (GPTのトークナイザは 'Ġriver' のように空白を記号で扱う)\n",
        "# 'Ġriver' (スペース付き) を優先的に探す\n",
        "river_token_id_gpt = tokenizer_gpt.convert_tokens_to_ids(\"Ġriver\")\n",
        "if river_token_id_gpt == tokenizer_gpt.unk_token_id:\n",
        "    # 見つからなければ 'river' (スペースなし) を探す\n",
        "    river_token_id_gpt = tokenizer_gpt.convert_tokens_to_ids(\"river\")\n",
        "\n",
        "river_index_gpt_list = (inputs_gpt['input_ids'][0] == river_token_id_gpt).nonzero(as_tuple=True)[0]\n",
        "if len(river_index_gpt_list) == 0:\n",
        "    print(\"エラー: GPT-2トークナイザで 'river' (または 'Ġriver') が見つかりません。\")\n",
        "    gpt_river_vec = None\n",
        "else:\n",
        "    river_index_gpt = river_index_gpt_list[0]\n",
        "    gpt_river_vec = gpt_hidden_states[0, river_index_gpt, :].cpu().numpy()\n",
        "\n",
        "\n",
        "# --- 3. 類似度の比較 ---\n",
        "# 両方のベクトルが正常に取得できた場合のみ実行\n",
        "if bert_river_vec is not None and gpt_river_vec is not None:\n",
        "    print(\"\\n--- 3. 類似度比較 ---\")\n",
        "\n",
        "    # BERT: \"bank\" と \"river\" の類似度\n",
        "    sim_bert = cosine_similarity([bert_last_vec], [bert_river_vec]) # 穴埋め: [bert_last_vec], [bert_river_vec]\n",
        "    print(f\"BERT: 最後の 'bank' と 'river' の類似度: {sim_bert[0][0]:.4f}\")\n",
        "\n",
        "    # GPT: \"bank\" と \"river\" の類似度\n",
        "    sim_gpt = cosine_similarity([gpt_last_vec], [gpt_river_vec]) # 穴埋め: [gpt_last_vec], [gpt_river_vec]\n",
        "    print(f\"GPT: 最後の 'bank' と 'river' の類似度: {sim_gpt[0][0]:.4f}\")\n",
        "\n",
        "    print(\"\\n(注: GPT-2は 'river' を ' river' とトークナイズすることがあり、BERTと条件が完全には一致しませんが、傾向を比較します)\")\n",
        "else:\n",
        "    print(f\"\\n--- 3. 類似度比較 (スキップ) ---\")\n",
        "    print(\"文脈語 ('river') が見つからなかったため、類似度比較をスキップしました。\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RvlYhQ3LZG53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 👩‍🎓 **課題 (BERT vs GPT 考察)**\n",
        ">\n",
        "> 1.  BERTとGPTで、最後の単語（`bank`）と文脈語（`river`）の類似度にどのような違いが出たでしょうか？\n",
        "> 2.  （考察）ステップ3で学んだように、GPTはこの「最後のベクトル」を使って**次の単語**を予測します。なぜ、`bank` 自体の意味（`river` に近いかどうか）を突き詰めるよりも、「そこまでの文脈を集約したベクトル」を使う方が、次の単語（例: \"and\", \"is\", \"was\" など）を予測するのに適していると考えられるか、あなたの考えを説明してください。（自由記述）"
      ],
      "metadata": {
        "id": "FpMXn975ZQnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🎉 まとめ\n",
        "\n",
        "お疲れ様でした！この課題では、GPTのような大規模言語モデルがテキストを生成する3つの基本的なステップを学びました。\n",
        "\n",
        "1.  **📝 埋め込み (Embedding):**\n",
        "    単語を「意味を持つ数値ベクトル」に変換します。この時点では、まだ文脈（多義語）は考慮されていません。\n",
        "\n",
        "2.  **💡 文脈化 (Contextualization):**\n",
        "    **Attention** という仕組みが、文中の他の単語に「注目」し、元の埋め込みベクトルを「文脈ベクトル」にアップグレードします。これにより `bank` のような多義語も区別できるようになりました。\n",
        "\n",
        "3.  **✍️ 生成 (Generation):**\n",
        "    GPTは、**最後の単語の文脈ベクトル**（＝そこまでの文脈の集約）を **LM Head（全結合層）** に通します。これにより、辞書にある全単語の「次の単語としての確率」が計算され、最も確率の高い単語が選ばれます。\n",
        "\n",
        "今回は、Attention がどのように重みを計算しているか？（Self-Attention や QKV）という核心部分には触れませんでしたが、入力（埋め込み）と出力（文脈ベクトル、確率）がどう変わるかを見ることで、GPT によるテキスト生成の全体像を掴んでいただけたかと思います。"
      ],
      "metadata": {
        "id": "DPWTNqjISoA0"
      }
    }
  ]
}
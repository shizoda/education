{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPqWYTpQgg2jEnqSeZqm+GK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/transformer/Word2Vec_BERT_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– GPTã¯ã©ã®ã‚ˆã†ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã®ã‹ï¼Ÿ\n",
        "\n",
        "ã“ã®èª²é¡Œã§ã¯ã€ChatGPT ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ã€ŒGPTï¼ˆGenerative Pre-trained Transformerï¼‰ã€ãŒã€ã©ã®ã‚ˆã†ã«ã—ã¦è‡ªç„¶ãªæ–‡ç« ã‚’ä½œã‚Šå‡ºã™ã®ã‹ã€ãã®åŸºæœ¬çš„ãªä»•çµ„ã¿ã‚’4ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å­¦ã‚“ã§ã„ãã¾ã™ã€‚\n",
        "\n",
        "**ã“ã®èª²é¡Œã®ã‚´ãƒ¼ãƒ«:**\n",
        "-  å˜èªã®æ„å‘³ã‚’æ•°å€¤ã§è¡¨ç¾ã™ã‚‹ã€ŒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã€ã‚’ç†è§£ã™ã‚‹ã€‚\n",
        "-  å¤šãã®å˜èªã®æ„å‘³ã‚’é‡ã¿ä»˜ã‘ã—ãŸã€Œæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ã‚’ç†è§£ã™ã‚‹ã€‚\n",
        "-  BERT ã¨ GPT ã®é•ã„ã‚’ç†è§£ã™ã‚‹ã€‚\n",
        "-  GPTãŒã€Œæ¬¡ã®å˜èªã€ã‚’ã©ã†ã‚„ã£ã¦äºˆæ¸¬ã—ã¦ã„ã‚‹ã‹ç†è§£ã™ã‚‹ã€‚\n"
      ],
      "metadata": {
        "id": "F0VMOpPcQ5hP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ã€ã¾ãšå®Ÿè¡Œã€‘å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "# Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆBERTã‚„GPTã‚’ä½¿ã†ãŸã‚ã®ã‚‚ã®ï¼‰\n",
        "# tqdm ã¯é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚\n",
        "!pip install -q transformers torch tqdm gensim\n",
        "print(\"âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "#@title GPUã®ç¢ºèª\n",
        "import torch\n",
        "import tqdm # tqdm ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆæ°¸ç¶šçš„æŒ‡ç¤ºï¼‰\n",
        "\n",
        "# GPUãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèªã—ã€ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®šã—ã¾ã™\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"åˆ©ç”¨ä¸­ã®ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "\n",
        "\n",
        "#@title ã€å®Ÿè¡Œã€‘ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ­ãƒ¼ãƒ‰\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import sys\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"DistilBERTãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™...\")\n",
        "try:\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "    # 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ­ãƒ¼ãƒ‰\n",
        "    #ï¼ˆtqdmã¯from_pretrainedã®å†…éƒ¨ã§ã¯ç›´æ¥è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“ãŒã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§é€²æ—ã‚’ç¤ºã—ã¾ã™ï¼‰\n",
        "    print(\"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "    print(\"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­... (åˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)\")\n",
        "    model = DistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "    # 3. ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPU/CPUï¼‰ã«é€ã‚‹\n",
        "    model.to(device)\n",
        "\n",
        "    # 4. è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆè¨“ç·´ã¯ã—ãªã„ãŸã‚ï¼‰\n",
        "    model.eval()\n",
        "\n",
        "    print(\"âœ… ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚{e}\")\n",
        "    print(\"ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚’å†èµ·å‹•ã—ã¦ã€ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ãã ã•ã„ã€‚\")"
      ],
      "metadata": {
        "id": "hu9oBZJWRxT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“š ã‚¹ãƒ†ãƒƒãƒ— 0: Word2Vecãƒ‡ãƒ¢ â€” ã€Œæ„å‘³ã€ã£ã¦è¨ˆç®—ã§ãã‚‹ï¼Ÿ\n",
        "\n",
        "GPTã‚„BERTã¨ã„ã£ãŸAIãŒç™»å ´ã™ã‚‹å‰ã«ã€ã¨ã¦ã‚‚æ´»èºã—ãŸã€ŒWord2Vecï¼ˆãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒˆã‚¥ãƒ»ãƒ™ãƒƒã‚¯ï¼‰ã€ã¨ã„ã†æŠ€è¡“ãŒã‚ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã‚‚ã€å˜èªã‚’ã€Œæ•°å€¤ã®ãƒªã‚¹ãƒˆã€ã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã“ã®æ•°å€¤ãƒªã‚¹ãƒˆã®ã™ã”ã„ã¨ã“ã‚ã¯ã€å˜ãªã‚‹æ•°å€¤ã®ç¾…åˆ—ã§ã¯ãªãã€**å˜èªã®ã€Œæ„å‘³ã€ãŒãã®æ•°å€¤ã«åæ˜ ã•ã‚Œã‚‹**ç‚¹ã«ã‚ã‚Šã¾ã™ã€‚ã•ã‚‰ã«ã€ãã®æ•°å€¤ã‚’ä½¿ã£ã¦ **ã€Œæ„å‘³ã€ã‚’è¨ˆç®—** ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
        "\n",
        "## ğŸ§® æœ‰åãªè¨ˆç®—å¼: king - man + woman = queen\n",
        "\n",
        "Word2Vecã®ä¸–ç•Œã§æœ€ã‚‚æœ‰åãªã®ãŒã€ã“ã®è¨ˆç®—å¼ã§ã™ã€‚\n",
        "\n",
        "> `(king) - (man) + (woman) â‰ˆ (queen)`\n",
        "\n",
        "ã€Œ`king`ï¼ˆç‹æ§˜ï¼‰ã€ãŒæŒã¤æ•°å€¤ã‹ã‚‰ã€Œ`man`ï¼ˆç”·æ€§ï¼‰ã€ã£ã½ã•ã‚’å¼•ãç®—ã—ã€ãã“ã«ã€Œ`woman`ï¼ˆå¥³æ€§ï¼‰ã€ã£ã½ã•ã‚’è¶³ã—ç®—ã™ã‚‹ã¨ã€ãã®è¨ˆç®—çµæœã¯ã€Œ`queen`ï¼ˆå¥³ç‹ï¼‰ã€ãŒæŒã¤æ•°å€¤ã¨ã€ã»ã¼åŒã˜å€¤ã«ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ§­ ãªãœãã‚“ãªè¨ˆç®—ãŒã§ãã‚‹ã®ï¼Ÿ\n",
        "\n",
        "Word2VecãŒå˜èªã‚’ã€Œ**æ„å‘³ã®ç©ºé–“**ã€ã«é…ç½®ã™ã‚‹ã‹ã‚‰ã§ã™ã€‚\n",
        "\n",
        "**1. ã€Œä¼¼ãŸã‚‚ã®ã€ã¯è¿‘ãã«ç½®ã**\n",
        "ã¾ãšã€ã“ã®åœ°å›³ã§ã¯ã€ä¼¼ãŸã‚ˆã†ãªæ–‡è„ˆã§ä½¿ã‚ã‚Œã‚‹å˜èªï¼ˆä¾‹ï¼šã€ŒéŠ€è¡Œã€ã¨ã€Œä¿¡ç”¨é‡‘åº«ã€ï¼‰ã¯ã€è¿‘ã„å ´æ‰€ã«é…ç½®ã•ã‚Œã¾ã™ã€‚\n",
        "\n",
        "**2. ã€Œé–¢ä¿‚æ€§ã€ã‚’ã€Œæ–¹å‘ã€ã¨ã—ã¦æƒãˆã‚‹ï¼ˆã“ã“ãŒé‡è¦ï¼ï¼‰**\n",
        "Word2Vecã®æœ¬å½“ã«ã™ã”ã„ç‚¹ã¯ã€å˜èªã¨å˜èªã®ã€Œé–¢ä¿‚æ€§ã€ã‚‚åœ°å›³ä¸Šã®ã€Œæ–¹å‘ã€ã¨ã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
        "\n",
        "ä¾‹ãˆã°ã€\n",
        "* ã€Œ`man`ï¼ˆç”·æ€§ï¼‰ã€ã¨ã„ã†ç‚¹ã‹ã‚‰ã€Œ`king`ï¼ˆç‹æ§˜ï¼‰ã€ã¨ã„ã†ç‚¹ã«å‘ã‹ã†ã€Œæ–¹å‘ã€\n",
        "* ã€Œ`woman`ï¼ˆå¥³æ€§ï¼‰ã€ã¨ã„ã†ç‚¹ã‹ã‚‰ã€Œ`queen`ï¼ˆå¥³ç‹ï¼‰ã€ã¨ã„ã†ç‚¹ã«å‘ã‹ã†ã€Œæ–¹å‘ã€\n",
        "\n",
        "ã“ã®2ã¤ã®ã€Œæ–¹å‘ã€ã¯ã€ã©ã¡ã‚‰ã‚‚**ã€Œï¼ˆãã®æ€§åˆ¥ã®ï¼‰ç‹æ—ã«ãªã‚‹ã€**ã¨ã„ã†æ„å‘³ã®ã€Œé–¢ä¿‚æ€§ã€ã‚’è¡¨ã—ã¦ã„ã¾ã™ã‚ˆã­ã€‚\n",
        "\n",
        "Word2Vecã¯ã€å¤§é‡ã®æ–‡ç« ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ã€Œã‚ã€ã“ã®2ã¤ã®é–¢ä¿‚æ€§ï¼ˆæ–¹å‘ï¼‰ã¯ã€æ„å‘³çš„ã«åŒã˜ã ãªã€ã¨æ°—ã¥ãã€åœ°å›³ã®ä¸Šã§ã“ã® **2ã¤ã®çŸ¢å°ãŒã»ã¼åŒã˜å‘ããƒ»åŒã˜é•·ã•ï¼ˆï¼å¹³è¡Œï¼‰** ã«ãªã‚‹ã‚ˆã†ã«ã€å„å˜èªã®å ´æ‰€ï¼ˆæ•°å€¤ï¼‰ã‚’èª¿æ•´ã—ã¦ã„ãã¾ã™ã€‚\n"
      ],
      "metadata": {
        "id": "1KMOFBFmRDcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§® è¨ˆç®—ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå›³ã®è§£èª¬ï¼‰\n",
        "\n",
        "ã“ã®è¨ˆç®—ãŒæˆã‚Šç«‹ã¤ç†ç”±ã‚’ã€å›³ã‚’è¦‹ãªãŒã‚‰ã‚¹ãƒ†ãƒƒãƒ—ã§è¿½ã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "1.  **ã€Œç‹æ—ã®æ–¹å‘ã€ã‚’æ±‚ã‚ã‚‹:**\n",
        "    ã¾ãšã€ã€Œ`man`ã€ã®å ´æ‰€ã‹ã‚‰ã€Œ`king`ã€ã®å ´æ‰€ã¸ã®çŸ¢å°ï¼ˆå›³ã®èµ¤ã„çŸ¢å°ï¼‰ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ã“ã‚ŒãŒã€Œç‹æ—ã«ãªã‚‹ã€ã¨ã„ã†é–¢ä¿‚æ€§ã‚’è¡¨ã™ã€Œæ–¹å‘ã€ã§ã™ã€‚\n",
        "    ï¼ˆè¨ˆç®—å¼: `king - man`ï¼‰\n",
        "\n",
        "2.  **ã€Œ`woman`ã€ã®å ´æ‰€ã«è¶³ã™:**\n",
        "    æ¬¡ã«ã€ã“ã®ã€Œç‹æ—ã®æ–¹å‘ã€ã®çŸ¢å°ã‚’ã€ã€Œ`woman`ã€ã®å ´æ‰€ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã•ã›ã¾ã™ã€‚\n",
        "    ï¼ˆè¨ˆç®—å¼: `woman + (king - man)`ï¼‰\n",
        "\n",
        "3.  **ç€åœ°ç‚¹ã¯ã€Œ`queen`ã€:**\n",
        "    å›³ãŒç¤ºã™ã‚ˆã†ã«ã€ã“ã®åœ°å›³ã§ã¯é–¢ä¿‚æ€§ï¼ˆçŸ¢å°ï¼‰ãŒå¹³è¡Œã«ä¿ãŸã‚Œã¦ã„ã‚‹ãŸã‚ã€ã€Œ`woman`ã€ã®å ´æ‰€ã‹ã‚‰ã€Œç‹æ—ã®æ–¹å‘ã€ã¸é€²ã‚“ã å…ˆã«ã¯ã€ã¡ã‚‡ã†ã©ã€Œ`queen`ã€ã®å ´æ‰€ãŒä½ç½®ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ã“ã‚ŒãŒã€Œæ„å‘³ã®è¨ˆç®—ã€ã®æ­£ä½“ã§ã™ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "## âœ¨ é‡è¦æ€§\n",
        "\n",
        "Word2Vecã¯ã€ãŸã å˜èªã‚’æš—è¨˜ã™ã‚‹ã®ã§ã¯ãªãã€å¤§é‡ã®æ–‡ç« ã‹ã‚‰ **ã€Œå˜èªã¨å˜èªã®é–“ã®é–¢ä¿‚æ€§ã€ã‚’è‡ªå‹•çš„ã«è¦‹ã¤ã‘å‡ºã—ã€ãã‚Œã‚’ã€Œåœ°å›³ä¸Šã®æ–¹å‘ï¼ˆæ•°å€¤ã®å·®åˆ†ï¼‰ã€ã¨ã—ã¦å­¦ç¿’** ã—ã¾ã™ã€‚\n",
        "\n",
        "ã€Œæ€§åˆ¥ã€ã ã‘ã§ãªãã€ã€Œå›½ã®é¦–éƒ½ï¼ˆä¾‹: Japan â†’ Tokyoã€France â†’ Parisï¼‰ã€ã‚„ã€Œå‹•è©ã®æ™‚åˆ¶ï¼ˆä¾‹: walk â†’ walkedï¼‰ã€ã¨ã„ã£ãŸæ§˜ã€…ãªé–¢ä¿‚æ€§ã‚‚ã€åŒã˜ã‚ˆã†ã«ã€Œæ–¹å‘ã€ã¨ã—ã¦æ•°å€¤ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚\n",
        "\n",
        "ã“ã®ã€Œ**æ„å‘³ã‚’ä½ç½®ã‚„æ–¹å‘ã¨ã—ã¦æ‰ãˆã€è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹**ã€ã¨ã„ã†è€ƒãˆæ–¹ã¯ã€ã®ã¡ã®GPTã‚„BERTã¨ã„ã£ãŸã€ã‚ˆã‚Šè¤‡é›‘ãªAIãŒæ–‡ç« ã®ã€Œæ–‡è„ˆã€ã‚’æ·±ãç†è§£ã™ã‚‹ãŸã‚ã®ã€ã¨ã¦ã‚‚å¤§åˆ‡ãªåœŸå°ã¨ãªã£ã¦ã„ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "CUDr5Nxaw_EJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®— `A - B + C` (è¨ˆç®—çµŒè·¯ã®å¯è¦–åŒ–)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ãƒ•ã‚©ãƒ¼ãƒ \n",
        "#@markdown ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®— `A - B + C` ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
        "#@markdown (ä¾‹: A=king, B=man, C=woman)\n",
        "wordA = \"king\" #@param {\"type\":\"string\",\"placeholder\":\"king\"}\n",
        "wordB = \"man\" #@param {\"type\":\"string\",\"placeholder\":\"man\"}\n",
        "wordC = \"woman\" #@param {\"type\":\"string\",\"placeholder\":\"woman\"}\n",
        "#@markdown ---\n",
        "#@markdown ### å¯è¦–åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
        "#@markdown æ¬¡å…ƒå‰Šæ¸› (PCA) ã‚’ç”¨ã„ã¦ã€è¨ˆç®—çµæœã‚’2Dãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚\n",
        "enable_visualization = True #@param {type:\"boolean\"}\n",
        "#@markdown ä¸Šä½ä½•ä»¶ã®é¡ä¼¼å˜èªã‚’ãƒ—ãƒ­ãƒƒãƒˆã«å«ã‚ã¾ã™ã‹ï¼Ÿ\n",
        "top_k_to_plot = 7 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "#@markdown ---\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. åŸ‹ã‚è¾¼ã¿å±¤ã®å–å¾— (DistilBERTãƒ¢ãƒ‡ãƒ«ã‹ã‚‰)\n",
        "embedding_layer = model.embeddings.word_embeddings\n",
        "# åŸ‹ã‚è¾¼ã¿è¡Œåˆ— (è¾æ›¸ã®å…¨å˜èªã®ãƒ™ã‚¯ãƒˆãƒ«)\n",
        "embedding_matrix = embedding_layer.weight.data\n",
        "\n",
        "print(f\"è¾æ›¸ã®ç·å˜èªæ•°: {embedding_matrix.shape[0]}\")\n",
        "print(f\"ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°: {embedding_matrix.shape[1]}\")\n",
        "\n",
        "# 2. å˜èªã®IDã¨ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—\n",
        "words = [wordA.lower(), wordB.lower(), wordC.lower()]\n",
        "vectors = {}\n",
        "all_words_found = True\n",
        "\n",
        "with torch.no_grad():\n",
        "    for word in words:\n",
        "        if not word: # ç©ºæ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯\n",
        "            print(\"ã‚¨ãƒ©ãƒ¼: å˜èªãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
        "            all_words_found = False\n",
        "            break\n",
        "        try:\n",
        "            token_id = tokenizer.convert_tokens_to_ids(word)\n",
        "            if token_id == tokenizer.unk_token_id:\n",
        "                raise ValueError(f\"å˜èª '{word}' ãŒè¾æ›¸ã«ã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "            token_id_tensor = torch.tensor([token_id]).to(device)\n",
        "            vectors[word] = embedding_layer(token_id_tensor).squeeze(0) # (1, 768) -> (768)\n",
        "            print(f\"'{word}' ã®ãƒ™ã‚¯ãƒˆãƒ«å–å¾—å®Œäº†ã€‚\")\n",
        "\n",
        "        except (ValueError, IndexError) as e:\n",
        "            print(f\"ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "            all_words_found = False\n",
        "            break\n",
        "\n",
        "# 3. ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—\n",
        "if all_words_found:\n",
        "    with torch.no_grad():\n",
        "        vecA = vectors[wordA.lower()]\n",
        "        vecB = vectors[wordB.lower()]\n",
        "        vecC = vectors[wordC.lower()]\n",
        "\n",
        "        # â–¼â–¼â–¼ è¦æ±‚ã•ã‚ŒãŸå…¨6ç‚¹ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¨ˆç®— â–¼â–¼â–¼\n",
        "        vec_A_minus_B = vecA - vecB   # A - B (é–¢ä¿‚æ€§ãƒ™ã‚¯ãƒˆãƒ«)\n",
        "        vec_A_plus_C = vecA + vecC    # A + C (ä¸­é–“è¨ˆç®—)\n",
        "        vec_Result = vecA - vecB + vecC # A - B + C (æœ€çµ‚çµæœ)\n",
        "\n",
        "        # 4. æœ€çµ‚çµæœ (A-B+C) ã¨å…¨å˜èªã®é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
        "        result_vec_norm = F.normalize(vec_Result.unsqueeze(0), p=2)\n",
        "        embedding_matrix_norm = F.normalize(embedding_matrix, p=2)\n",
        "        cosine_similarities = torch.matmul(result_vec_norm, embedding_matrix_norm.T)\n",
        "\n",
        "        # 5. é¡ä¼¼åº¦ãŒé«˜ã„ä¸Šä½å˜èªã‚’å–å¾—\n",
        "        top_k = top_k_to_plot\n",
        "        top_k_similarities, top_k_indices = torch.topk(cosine_similarities.squeeze(0), top_k)\n",
        "\n",
        "        print(f\"\\n--- è¨ˆç®—çµæœ (vec({wordA}) - vec({wordB}) + vec({wordC})) ã«è¿‘ã„å˜èª Top {top_k} ---\")\n",
        "\n",
        "        results = []\n",
        "        top_result_labels = [] # å¯è¦–åŒ–ç”¨\n",
        "        top_result_vectors = [] # å¯è¦–åŒ–ç”¨\n",
        "\n",
        "        for i in range(top_k):\n",
        "            token_id = top_k_indices[i].item()\n",
        "            token_str = tokenizer.decode(token_id)\n",
        "            similarity = top_k_similarities[i].item()\n",
        "            results.append((token_str, similarity))\n",
        "\n",
        "            if token_str not in words:\n",
        "                top_result_labels.append(token_str)\n",
        "                token_id_tensor = torch.tensor([token_id]).to(device)\n",
        "                top_result_vectors.append(embedding_layer(token_id_tensor).squeeze(0))\n",
        "\n",
        "        df_results = pd.DataFrame(results, columns=[\"Word\", \"Cosine Similarity\"])\n",
        "        print(df_results.to_string(index=False))\n",
        "\n",
        "    # --- 6. â–¼â–¼â–¼ å¯è¦–åŒ–æ©Ÿèƒ½ï¼ˆè¦æ±‚ã•ã‚ŒãŸ6ç‚¹ã¨4çŸ¢å°ï¼‰ â–¼â–¼â–¼ ---\n",
        "    if enable_visualization:\n",
        "        print(\"\\n--- 2Då¯è¦–åŒ– (PCA) ---\")\n",
        "\n",
        "        # 1. åé›†ã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã¨ãƒ©ãƒ™ãƒ«\n",
        "        labels_to_plot = [wordA, wordB, wordC]\n",
        "        vectors_to_plot = [vecA, vecB, vecC]\n",
        "\n",
        "        labels_to_plot.extend(top_result_labels)\n",
        "        vectors_to_plot.extend(top_result_vectors)\n",
        "\n",
        "        # â–¼â–¼â–¼ A-B, A+C, Result ã‚‚PCAã®å¯¾è±¡ã«å«ã‚ã‚‹ â–¼â–¼â–¼\n",
        "        ab_label = f\"{wordA}-{wordB}\"\n",
        "        ac_label = f\"{wordA}+{wordC}\"\n",
        "        result_label = f\"Result ({wordA}-{wordB}+{wordC})\"\n",
        "\n",
        "        labels_to_plot.extend([ab_label, ac_label, result_label])\n",
        "        vectors_to_plot.extend([vec_A_minus_B, vec_A_plus_C, vec_Result])\n",
        "\n",
        "        # 2. PyTorchãƒ†ãƒ³ã‚½ãƒ«ã‚’NumPyé…åˆ—ã«å¤‰æ›\n",
        "        vectors_np = torch.stack(vectors_to_plot).cpu().numpy()\n",
        "\n",
        "        # 3. PCAã§2æ¬¡å…ƒã«å‰Šæ¸›\n",
        "        pca = PCA(n_components=2)\n",
        "        vectors_2d = pca.fit_transform(vectors_np)\n",
        "\n",
        "        # 4. ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "        plt.figure(figsize=(12, 9))\n",
        "\n",
        "        # åº§æ¨™ã‚’è¾æ›¸ã«\n",
        "        coords = {label: vectors_2d[i] for i, label in enumerate(labels_to_plot)}\n",
        "\n",
        "        # A, B, C ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "        plt.scatter(coords[wordA][0], coords[wordA][1], c='blue', s=150, label=f\"A: {wordA}\", zorder=5)\n",
        "        plt.annotate(wordA, (coords[wordA][0], coords[wordA][1]), fontsize=12, zorder=6)\n",
        "\n",
        "        plt.scatter(coords[wordB][0], coords[wordB][1], c='red', s=150, label=f\"B: {wordB}\", zorder=5)\n",
        "        plt.annotate(wordB, (coords[wordB][0], coords[wordB][1]), fontsize=12, zorder=6)\n",
        "\n",
        "        plt.scatter(coords[wordC][0], coords[wordC][1], c='green', s=150, label=f\"C: {wordC}\", zorder=5)\n",
        "        plt.annotate(wordC, (coords[wordC][0], coords[wordC][1]), fontsize=12, zorder=6)\n",
        "\n",
        "        # è¨ˆç®—çµæœ (A-B) ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "        plt.scatter(coords[ab_label][0], coords[ab_label][1], c='purple', s=150, marker='X', label=ab_label, zorder=10)\n",
        "        plt.annotate(ab_label, (coords[ab_label][0], coords[ab_label][1]), fontsize=14, zorder=11)\n",
        "\n",
        "        # è¨ˆç®—çµæœ (A+C) ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "        plt.scatter(coords[ac_label][0], coords[ac_label][1], c='brown', s=150, marker='X', label=ac_label, zorder=10)\n",
        "        plt.annotate(ac_label, (coords[ac_label][0], coords[ac_label][1]), fontsize=14, zorder=11)\n",
        "\n",
        "        # æœ€çµ‚è¨ˆç®—çµæœ (Result) ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "        plt.scatter(coords[result_label][0], coords[result_label][1], c='orange', s=300, marker='*', label=result_label, zorder=10)\n",
        "        plt.annotate(result_label, (coords[result_label][0], coords[result_label][1]), fontsize=14, zorder=11)\n",
        "\n",
        "        # Top-k ã®çµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "        for label in top_result_labels:\n",
        "            plt.scatter(coords[label][0], coords[label][1], c='gray', s=50, alpha=0.7)\n",
        "            plt.annotate(label, (coords[label][0], coords[label][1]), alpha=0.7)\n",
        "\n",
        "        # 6. â–¼â–¼â–¼ è¦æ±‚ã•ã‚ŒãŸ4ã¤ã®çŸ¢å° â–¼â–¼â–¼\n",
        "\n",
        "        # çŸ¢å°1: A -> A-B (èµ¤è‰², -B)\n",
        "        plt.arrow(coords[wordA][0], coords[wordA][1],\n",
        "                  coords[ab_label][0] - coords[wordA][0],\n",
        "                  coords[ab_label][1] - coords[wordA][1],\n",
        "                  color='red', linestyle='--', head_width=0.01, length_includes_head=True, alpha=0.5, label=f\"- {wordB}\")\n",
        "\n",
        "        # çŸ¢å°2: A -> A+C (ç·‘è‰², +C)\n",
        "        plt.arrow(coords[wordA][0], coords[wordA][1],\n",
        "                  coords[ac_label][0] - coords[wordA][0],\n",
        "                  coords[ac_label][1] - coords[wordA][1],\n",
        "                  color='green', linestyle='--', head_width=0.01, length_includes_head=True, alpha=0.5, label=f\"+ {wordC}\")\n",
        "\n",
        "        # çŸ¢å°3: A+C -> Result (èµ¤è‰², -B)\n",
        "        plt.arrow(coords[ac_label][0], coords[ac_label][1],\n",
        "                  coords[result_label][0] - coords[ac_label][0],\n",
        "                  coords[result_label][1] - coords[ac_label][1],\n",
        "                  color='red', linestyle='-.', head_width=0.01, length_includes_head=True, alpha=0.5)\n",
        "\n",
        "        # çŸ¢å°4: A-B -> Result (ç·‘è‰², +C)\n",
        "        plt.arrow(coords[ab_label][0], coords[ab_label][1],\n",
        "                  coords[result_label][0] - coords[ab_label][0],\n",
        "                  coords[result_label][1] - coords[ab_label][1],\n",
        "                  color='green', linestyle='-.', head_width=0.01, length_includes_head=True, alpha=0.5)\n",
        "\n",
        "        plt.title(f\"PCA Visualization of Vector Calculation: (A={wordA}, B={wordB}, C={wordC})\")\n",
        "        plt.xlabel(\"Principal Component 1\")\n",
        "        plt.ylabel(\"Principal Component 2\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "r9-C0RHikwY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ğŸ‘©â€ğŸ“ **èª²é¡Œ (ã‚¹ãƒ†ãƒƒãƒ—0 è€ƒå¯Ÿãƒ»ç©´åŸ‹ã‚)**\n",
        ">\n",
        "> 1.  ä¸Šã®ã‚»ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã€`king - man + woman` ä»¥å¤–ã®ã‚¢ãƒŠãƒ­ã‚¸ãƒ¼ï¼ˆé¡æ¨ï¼‰è¨ˆç®—ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        ">     * ä¾‹: `Paris - France + Japan` ï¼ˆé¦–éƒ½ã®é–¢ä¿‚ï¼‰\n",
        ">     * ä¾‹: `walking - walk + swam` ï¼ˆå‹•è©ã®æ™‚åˆ¶ã®é–¢ä¿‚ï¼‰\n",
        "> 2.  ï¼ˆç©´åŸ‹ã‚ï¼‰ Word2Vec ã¯ã€å¤§é‡ã®æ–‡ç« ã‚’å­¦ç¿’ã—ã€å˜èªã‚’ã€ŒåŸ‹ã‚è¾¼ã¿ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ã€ã¨ã—ã¦è¡¨ç¾ã—ã¾ã™ã€‚ã“ã®ãƒ™ã‚¯ãƒˆãƒ«ã§ã¯ã€å˜èªã®ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ãŒãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ä¸Šã®ã€Œä½ç½®ã€ã‚„ã€Œå‘ãï¼ˆé–¢ä¿‚æ€§ï¼‰ã€ã¨ã—ã¦è¡¨ç¾ã•ã‚Œã¾ã™ã€‚\n",
        ">\n",
        ">     ä¾‹ãˆã°ã€`king - man` ãŒè¡¨ã™ã€Œç”·æ€§ã‹ã‚‰ç‹æ§˜ã¸ã®é–¢ä¿‚æ€§ã€ã®ãƒ™ã‚¯ãƒˆãƒ«ã¨ã€`queen - woman` ãŒè¡¨ã™ã€Œï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ã‹ã‚‰å¥³ç‹ã¸ã®é–¢ä¿‚æ€§ã€ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒã€ç©ºé–“ä¸Šã§ã»ã¼åŒã˜å‘ãã«ãªã‚Šã¾ã™ã€‚ã“ã®ã‚ˆã†ã«ã€å˜èªã‚’è¨ˆç®—å¯èƒ½ãªãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯å˜èªã®ï¼¿ï¼¿ï¼¿ï¼¿ï¼¿ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n",
        "> 3.  ã“ã®ã‚°ãƒ©ãƒ•ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®— `A - B + C = Result` ãŒ `A - B = Result - C` ã¨ã„ã†é–¢ä¿‚ï¼ˆå¹³è¡Œå››è¾ºå½¢ï¼‰ã§è¦–è¦šåŒ–ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€`A - B`ï¼ˆä¾‹: `king - man`ï¼‰ã®ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€ã©ã®ã‚ˆã†ãªã€Œæ„å‘³ã€ã‚„ã€Œé–¢ä¿‚æ€§ã€ã‚’æŒã£ã¦ã„ã‚‹ã¨è§£é‡ˆã§ãã‚‹ã‹ã€ã‚ãªãŸã®è€ƒãˆã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ï¼ˆè‡ªç”±è¨˜è¿°ï¼‰\n",
        "\n",
        "----"
      ],
      "metadata": {
        "id": "NvSTrkJrxKKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“ ã‚¹ãƒ†ãƒƒãƒ—1: (BERT) åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ã€Œæ–‡è„ˆã€ã®å•é¡Œ\n",
        "\n",
        "ã‚¹ãƒ†ãƒƒãƒ—0ã§ã¯ã€Word2Vec (GloVe) ãŒå˜èªã®æ„å‘³ã‚’ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆç©ºé–“ä¸Šã®ä½ç½®ï¼‰ã§è¡¨ç¾ã§ãã‚‹ã“ã¨ã‚’è¦‹ã¾ã—ãŸã€‚\n",
        "\n",
        "ã—ã‹ã—ã€Word2Vecã«ã¯å¤§ããªå•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œã¯ã€Œ**å¤šç¾©èª**ã€ã‚’åŒºåˆ¥ã§ããªã„ã“ã¨ã§ã™ã€‚\n",
        "Word2Vecã§ã¯ã€`bank`ï¼ˆéŠ€è¡Œï¼‰ã¨ `bank`ï¼ˆåœŸæ‰‹ï¼‰ã¯ã€è¾æ›¸ã«1ã¤ã®å˜èªã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€**å¸¸ã«åŒã˜ãƒ™ã‚¯ãƒˆãƒ«**ã«ãªã£ã¦ã—ã¾ã„ã¾ã™ã€‚ã“ã‚Œã§ã¯ã€æ–‡è„ˆã‚’æ­£ã—ãç†è§£ã§ãã¾ã›ã‚“ã€‚\n",
        "\n",
        "ã“ã®ã€Œæ–‡è„ˆã€ã®å•é¡Œã‚’è§£æ±ºã—ãŸã®ãŒã€**Transformer** ã¨ã„ã†æŠ€è¡“ã§ã™ã€‚\n",
        "\n",
        "---\n",
        "### ğŸ§  Transformer, BERT, GPT ã¨ã¯ï¼Ÿ\n",
        "\n",
        "ã“ã®èª²é¡Œã§ã¯ `BERT` ã‚„ `GPT` ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã„ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯å…¨ã¦ `Transformer` ã¨ã„ã†å¼·åŠ›ãªAIãƒ¢ãƒ‡ãƒ«ï¼ˆè¨­è¨ˆå›³ï¼‰ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ï¼ˆRNNã€LSTMã¨ã„ã£ãŸæŠ€è¡“ã‚’çŸ¥ã‚‰ãªãã¦ã‚‚ã€ä»¥ä¸‹ã®èª¬æ˜ã§é€²ã‚ã‚‰ã‚Œã¾ã™ã€‚ï¼‰\n",
        "\n",
        "* **Transformer (ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼):**\n",
        "    * 2017å¹´ã«ç™»å ´ã—ãŸã€AIãƒ¢ãƒ‡ãƒ«ã®ã€Œè¨­è¨ˆå›³ã€ã‚ã‚‹ã„ã¯ã€Œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ã®åå‰ã§ã™ã€‚\n",
        "    * æ–‡ç« ã®ä¸­ã®ã€Œã©ã®å˜èªã¨ã©ã®å˜èªãŒå¼·ãé–¢ä¿‚ã—ã¦ã„ã‚‹ã‹ã€ï¼ˆï¼æ–‡è„ˆï¼‰ã‚’éå¸¸ã«ã†ã¾ãæ‰±ã†ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ä¸­æ ¸æŠ€è¡“ãŒã€Œ**Attention**ï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰ã€ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2ã§å­¦ã³ã¾ã™ï¼‰ã§ã™ã€‚\n",
        "\n",
        "* **BERT (ãƒãƒ¼ãƒˆ):**\n",
        "    * `Transformer` ã®è¨­è¨ˆå›³ã‚’ä½¿ã£ã¦ã€Œ**æ–‡è„ˆã‚’æ·±ãç†è§£ã™ã‚‹**ã€ãŸã‚ã«GoogleãŒè¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
        "        * ã€Œæ„Ÿæƒ…åˆ†æã€ã‚„ã€Œè³ªå•å¿œç­”ã€ãªã©æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã«ä½¿ã‚ã‚Œã¾ã™ï¼ˆã“ã‚Œã¯ **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°** ã¨å‘¼ã°ã‚Œã¾ã™ï¼‰ã€‚\n",
        "        * ã—ã‹ã—ã€ãã®ãŸã‚ã®ã€ŒåŸºç¤ä½“åŠ›ã€ã‚’ä½œã‚‹è¨“ç·´ï¼ˆ**äº‹å‰å­¦ç¿’**ï¼‰ã¨ã—ã¦ã€BERTãŒï¼ˆä¸‹è¨˜ã®GPTã¨ã¯ç•°ãªã‚‹æ–¹æ³•ã§ï¼‰è§£ãã®ãŒã€Œ**ç©´åŸ‹ã‚å•é¡Œ**ï¼ˆMasked Language Modelï¼‰ã€ã§ã™ã€‚\n",
        "    * **ãªãœç©´åŸ‹ã‚å•é¡Œã‹ï¼Ÿ** ãã‚Œã¯ã€æ–‡ã®**ä¸¡å´**ï¼ˆå·¦å³ï¼‰ã®æ–‡è„ˆã‚’åŒæ™‚ã«è¦‹ã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’å¼·åˆ¶ã§ãã‚‹ã‹ã‚‰ã§ã™ã€‚\n",
        "        * ä¾‹ãˆã°ã€`I went to the [?] to deposit money.` (ç§ã¯ãŠé‡‘ã‚’ãŠã‚ã—ã« [?] ã«è¡Œã£ãŸ) ã¨ã„ã†ç©´åŸ‹ã‚ã‚’è§£ãã«ã¯ã€å·¦å´ã® `went to the` ã ã‘ã§ãªãã€å³å´ã® `deposit money` ã‚’è¦‹ã‚‹ã“ã¨ãŒä¸å¯æ¬ ã§ã™ã€‚\n",
        "        * ã“ã®ã‚ˆã†ã«ä¸¡å´ã®æ–‡è„ˆã‚’è¦‹ã‚‹è¨“ç·´ï¼ˆäº‹å‰å­¦ç¿’ï¼‰ã‚’å¤§é‡ã«è¡Œã†ã“ã¨ã§ã€BERTã¯ã€Œå˜èªã®æ–‡è„ˆä¸Šã®æ·±ã„æ„å‘³ã€ã‚’ç†è§£ã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ã“ã®ã€Œè³¢ããªã£ãŸBERTã€ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ã€æ„Ÿæƒ…åˆ†æãªã©ã®å¿œç”¨ã‚¿ã‚¹ã‚¯ã‚’ï¼ˆãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ï¼‰è§£ã‹ã›ã‚‹ã‚ã‘ã§ã™ã€‚\n",
        "\n",
        "* **GPT (ã‚¸ãƒ¼ãƒ”ãƒ¼ãƒ†ã‚£ãƒ¼):**\n",
        "    * `Transformer` ã®è¨­è¨ˆå›³ã‚’ä½¿ã£ã¦ã€Œ**æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹**ã€ãŸã‚ã«OpenAIãŒè¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
        "    * ä¸»ãªç›®çš„ã¯ã€æ–‡ç« ã®ç¶šãã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ï¼ˆä¾‹: \"I sat on the river\" â†’ `bank`ï¼‰ã§ã™ã€‚\n",
        "    * æ–‡ã®**å·¦å´**ï¼ˆã™ã§ã«å…¥åŠ›ã•ã‚ŒãŸéƒ¨åˆ†ï¼‰ã ã‘ã‚’å‚ç…§ã—ã¦ã€æ¬¡ã«æ¥ã‚‹å˜èªã‚’äºˆæ¸¬ã—ã¾ã™ã€‚\n",
        "\n",
        "ãªãŠã€ä»Šå›ã®èª²é¡Œã®ã‚´ãƒ¼ãƒ«ã¯ã€ã“ã®GPTã®ä»•çµ„ã¿ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
        "\n"
      ],
      "metadata": {
        "id": "-zvvL_bzlsHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title æ¨¡å¼å›³ã‚’è¡¨ç¤º\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# åŸ‹ã‚è¾¼ã¿ãŸã„SVGã‚³ãƒ¼ãƒ‰ï¼ˆBERT vs GPTï¼‰ã‚’Pythonæ–‡å­—åˆ—ã¨ã—ã¦å®šç¾©ã—ã¾ã™\n",
        "# (ã‚³ãƒ”ãƒ¼æ™‚ã«æ¬ ã‘ã¦ã„ãŸå¯èƒ½æ€§ã®ã‚ã‚‹ç®‡æ‰€ã‚’ä¿®æ­£ã—ã¦ã„ã¾ã™)\n",
        "svg_code_string = \"\"\"\n",
        "<svg width=\"900px\" viewBox=\"0 0 800 500\" xmlns=\"http://www.w3.org/2000/svg\">\n",
        "Â  Â  <text x=\"400\" y=\"30\" font-size=\"24\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#333\">\n",
        "Â  Â  BERT vs GPT: Attention Patterns\n",
        "Â  </text>\n",
        "\n",
        "Â  Â  <g id=\"bert-section\">\n",
        "Â  Â  Â  Â  <text x=\"200\" y=\"80\" font-size=\"20\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1976D2\">\n",
        "Â  Â  Â  BERT (Bidirectional)\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"200\" y=\"105\" font-size=\"14\" text-anchor=\"middle\" fill=\"#666\">\n",
        "Â  Â  Â  Masked Language Model\n",
        "Â  Â  </text>\n",
        "\n",
        "Â  Â  Â  Â  <g id=\"bert-words\">\n",
        "Â  Â  Â  <rect x=\"40\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "Â  Â  Â  <text x=\"70\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">I went</text>\n",
        "\n",
        "Â  Â  Â  <rect x=\"110\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "Â  Â  Â  <text x=\"140\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to the</text>\n",
        "\n",
        "Â  Â  Â  <rect x=\"180\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#FF6B6B\" stroke=\"#D32F2F\" stroke-width=\"3\"/>\n",
        "Â  Â  Â  <text x=\"210\" y=\"155\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#FFF\">[MASK]</text>\n",
        "\n",
        "Â  Â  Â  <rect x=\"250\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "Â  Â  Â  <text x=\"280\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">to</text>\n",
        "\n",
        "Â  Â  Â  <rect x=\"320\" y=\"130\" width=\"60\" height=\"40\" rx=\"5\" fill=\"#E3F2FD\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "Â  Â  Â  <text x=\"350\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">deposit</text>\n",
        "Â  Â  </g>\n",
        "\n",
        "Â  Â  Â  Â  <defs>\n",
        "Â  Â  Â  <marker id=\"arrowhead-blue\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\">\n",
        "Â  Â  Â  Â  <polygon points=\"0 0, 10 3, 0 6\" fill=\"#1976D2\" />\n",
        "Â  Â  Â  </marker>\n",
        "Â  Â  </defs>\n",
        "\n",
        "Â  Â  Â  Â  <path d=\"M 100 145 Q 150 110 185 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
        "Â  Â  <path d=\"M 170 145 Q 180 120 195 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
        "\n",
        "Â  Â  Â  Â  <path d=\"M 280 145 Q 250 110 225 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
        "Â  Â  <path d=\"M 350 145 Q 280 100 220 145\" fill=\"none\" stroke=\"#1976D2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-blue)\" opacity=\"0.7\"/>\n",
        "\n",
        "Â  Â  Â  Â  <text x=\"210\" y=\"205\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#4CAF50\">â†“</text>\n",
        "Â  Â  <rect x=\"170\" y=\"210\" width=\"80\" height=\"35\" rx=\"5\" fill=\"#C8E6C9\" stroke=\"#4CAF50\" stroke-width=\"2\"/>\n",
        "Â  Â  <text x=\"210\" y=\"232\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2E7D32\">bank</text>\n",
        "Â  </g>\n",
        "\n",
        "Â  Â  <g id=\"gpt-section\">\n",
        "Â  Â  Â  Â  <text x=\"600\" y=\"80\" font-size=\"20\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#7B1FA2\">\n",
        "Â  Â  Â  GPT (Unidirectional)\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"600\" y=\"105\" font-size=\"14\" text-anchor=\"middle\" fill=\"#666\">\n",
        "Â  Â  Â  Next Token Prediction\n",
        "Â  Â  </text>\n",
        "\n",
        "Â  Â  Â  Â  <g id=\"gpt-words\">\n",
        "Â  Â  Â  <rect x=\"460\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "s.\n",
        "Â  Â  Â  <text x=\"485\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">I sat</text>\n",
        "\n",
        "Â  Â  Â  <rect x=\"520\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "Â  Â  Â  <text x=\"545\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">on</text>\n",
        "\n",
        "Â  Â  Â  <rect x=\"580\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "Â  Â  Â  <text x=\"605\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">the</text>\n",
        "\n",
        "Â  Â  Â  <rect x=\"640\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#F3E5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "Â  Â  Â  <text x=\"665\" y=\"155\" font-size=\"14\" text-anchor=\"middle\" fill=\"#333\">river</text>\n",
        "\n",
        "Â  Â  Â  <rect x=\"700\" y=\"130\" width=\"50\" height=\"40\" rx=\"5\" fill=\"#FFE0B2\" stroke=\"#F57C00\" stroke-width=\"3\"/>\n",
        "Â  Â  Â  <text x=\"725\" y=\"155\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#E65100\">?</text>\n",
        "Â  Â  </g>\n",
        "\n",
        "Â  Â  Â  Â  <defs>\n",
        "Â  Â  Â  <marker id=\"arrowhead-purple\" markerWidth=\"10\" markerHeight=\"10\" refX=\"9\" refY=\"3\" orient=\"auto\">\n",
        "Â  Â  Â  Â  <polygon points=\"0 0, 10 3, 0 6\" fill=\"#7B1FA2\" />\n",
        "Â  Â  Â  </marker>\n",
        "Â  Â  </defs>\n",
        "\n",
        "Â  Â  Â  Â  <path d=\"M 510 145 Q 600 110 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
        "Â  Â  <path d=\"M 570 145 Q 640 120 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
        "Â  Â  <path d=\"M 630 145 Q 670 130 705 145\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
        "Â  Â  <path d=\"M 690 150 L 705 150\" fill=\"none\" stroke=\"#7B1FA2\" stroke-width=\"2\" marker-end=\"url(#arrowhead-purple)\" opacity=\"0.7\"/>\n",
        "\n",
        "Â  Â  Â  Â  <text x=\"725\" y=\"205\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#4CAF50\">â†“</text>\n",
        "Â  Â  <rect x=\"685\" y=\"210\" width=\"80\" height=\"35\" rx=\"5\" fill=\"#C8E6C9\" stroke=\"#4CAF50\" stroke-width=\"2\"/>\n",
        "Â  Â  <text x=\"725\" y=\"232\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#2E7D32\">bank</text>\n",
        "Â  </g>\n",
        "Â s\n",
        "Â  Â  <g id=\"explanations\">\n",
        "Â  Â  Â  Â  <rect x=\"30\" y=\"280\" width=\"340\" height=\"180\" rx=\"8\" fill=\"#F5F5F5\" stroke=\"#1976D2\" stroke-width=\"2\"/>\n",
        "Â  Â  <text x=\"200\" y=\"305\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#1976D2\">\n",
        "Â  Â  Â  Bidirectional Context\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"40\" y=\"330\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  â€¢ Sees words from BOTH directions\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"40\" y=\"355\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  â€¢ [MASK] token attends to all words\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"40\" y=\"380\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  â€¢ Training: Predict masked words\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"40\" y=\"405\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  â€¢ Best for: Understanding tasks\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"40\" y=\"430\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  (classification, Q&amp;A, NER)\n",
        "Â  Â  </text>\n",
        "\n",
        "Â  Â  Â  Â  <rect x=\"430\" y=\"280\" width=\"340\" height=\"180\" rx=\"8\" fill=\"#F5F5F5\" stroke=\"#7B1FA2\" stroke-width=\"2\"/>\n",
        "Â  Â  <text x=\"600\" y=\"305\" font-size=\"16\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#7B1FA2\">\n",
        "Â  Â  Â  Unidirectional (Causal)\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"440\" y=\"330\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  â€¢ Sees words from LEFT only\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"440\" y=\"355\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  â€¢ Next token attends to past only\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"440\" y=\"380\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  â€¢ Training: Predict next word\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"440\" y=\"405\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  â€¢ Best for: Generation tasks\n",
        "Â  Â  </text>\n",
        "Â  Â  <text x=\"440\" y=\"430\" font-size=\"13\" fill=\"#333\">\n",
        "Â  Â  Â  (text completion, dialogue)\n",
        "Â  Â  </text>\n",
        "Â  </g>\n",
        "\n",
        "Â  Â  <rect x=\"150\" y=\"470\" width=\"500\" height=\"25\" rx=\"5\" fill=\"#FFF9C4\" stroke=\"#F57F17\" stroke-width=\"2\"/>\n",
        "Â  <text x=\"400\" y=\"488\" font-size=\"14\" font-weight=\"bold\" text-anchor=\"middle\" fill=\"#F57F17\">\n",
        "Â  Â  Key: BERT learns context, GPT learns to continue\n",
        "Â  </text>\n",
        "</svg>\n",
        "\"\"\"\n",
        "\n",
        "# HTML() ã‚’ä½¿ã£ã¦è¡¨ç¤ºã—ã¾ã™\n",
        "display(HTML(svg_code_string))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1JiQIKzcsXWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### åŸ‹ã‚è¾¼ã¿ã€Œå±¤ï¼ˆLayerï¼‰ã€ã¨ã¯ï¼Ÿ\n",
        "\n",
        "`Transformer`ï¼ˆBERTã‚„GPTï¼‰ã‚‚ã€Word2Vecã¨åŒæ§˜ã«ã€ã¾ãšå˜èªã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ä»•çµ„ã¿ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚ŒãŒã€ŒåŸ‹ã‚è¾¼ã¿ã€ã§ã‚ã‚Šã€ãã‚Œã‚’è¡Œã†ã®ãŒã€ŒåŸ‹ã‚è¾¼ã¿å±¤ã€ã§ã™ã€‚\n",
        "\n",
        "> **å±¤ï¼ˆLayerï¼‰ã¨ã¯ï¼Ÿ**\n",
        "> AIãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ã¯ã€ç‰¹å®šã®å‡¦ç†ã‚’è¡Œã†ã€Œ**éƒ¨å“**ã€ã‚’ä½•å±¤ã«ã‚‚é‡ã­ã¦ä½œã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ã€Œéƒ¨å“ã€ã®ã“ã¨ã‚’ã€Œ**å±¤ï¼ˆLayerï¼‰**ã€ã¨å‘¼ã³ã¾ã™ã€‚\n",
        ">\n",
        "> **åŸ‹ã‚è¾¼ã¿å±¤ï¼ˆEmbedding Layerï¼‰ã¨ã¯ï¼Ÿ**\n",
        "> ã€ŒåŸ‹ã‚è¾¼ã¿å±¤ã€ã¯ã€ãã®éƒ¨å“ï¼ˆå±¤ï¼‰ã®ä¸€ã¤ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ä¸€ç•ªæœ€åˆã®å…¥ã‚Šå£ã«ã‚ã‚Šã¾ã™ã€‚\n",
        "> ãã®å½¹å‰²ã¯ã€**å·¨å¤§ãªã€Œè¾æ›¸ï¼ˆå¯¾å¿œè¡¨ï¼‰ã€ã‚’æŒã¤ã“ã¨**ã§ã™ã€‚\n",
        ">\n",
        "> 1.  å…¥åŠ›ã¨ã—ã¦ã€Œå˜èªã®IDç•ªå·ã€ï¼ˆä¾‹: `apple` = 5025ç•ªï¼‰ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚\n",
        "> 2.  ã€ŒåŸ‹ã‚è¾¼ã¿å±¤ã€ãŒæŒã£ã¦ã„ã‚‹è¾æ›¸ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã€ã€Œ5025ç•ªã«å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆä¾‹: 768æ¬¡å…ƒã®æ•°å€¤ãƒªã‚¹ãƒˆï¼‰ã€ã‚’å¼•ãå‡ºã—ã¾ã™ã€‚\n",
        "> 3.  ãã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€æ¬¡ã®å±¤ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2ã§å­¦ã¶Attentionå±¤ãªã©ï¼‰ã«æ¸¡ã—ã¾ã™ã€‚\n",
        "\n",
        "> **åŸ‹ã‚è¾¼ã¿ï¼ˆEmbeddingï¼‰ã¨ã¯ï¼Ÿ**\n",
        "> ã“ã®ã€ŒåŸ‹ã‚è¾¼ã¿å±¤ã€ã«ã‚ˆã£ã¦å¤‰æ›ã•ã‚ŒãŸã€Œå˜èªã‚’è¡¨ã™ãƒ™ã‚¯ãƒˆãƒ«ã€ã®ã“ã¨ã€ã‚ã‚‹ã„ã¯ã€Œãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹å‡¦ç†ã€ãã®ã‚‚ã®ã‚’æŒ‡ã—ã¾ã™ã€‚\n",
        ">\n",
        "> * **å¤ã„æ–¹æ³• (One-Hot):** `apple` = `[0, 0, 1, 0, ...]` ã®ã‚ˆã†ã«ã€å˜èªã®å ´æ‰€ã ã‘1ã«ã™ã‚‹æ–¹æ³•ã€‚ã“ã‚Œã§ã¯å˜èªåŒå£«ã®é¡ä¼¼æ€§ãŒåˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚\n",
        "> * **ä»Šã®æ–¹æ³• (Word2Vec, BERTãªã©):** ã€Œå˜èªã®æ„å‘³ã¯ã€å‘¨å›²ã®å˜èªã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ã€ï¼ˆ**åˆ†å¸ƒä»®èª¬**ï¼‰ã«åŸºã¥ãã€æ„å‘³ãŒè¿‘ã„å˜èªãŒãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ä¸Šã§ã‚‚è¿‘ãã«é…ç½®ã•ã‚Œã‚‹ã‚ˆã†ã€åŸ‹ã‚è¾¼ã¿å±¤ã®ã€Œè¾æ›¸ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã€ã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚\n",
        "\n",
        "### BERTã®åŸ‹ã‚è¾¼ã¿ã¯ã€ŒåˆæœŸå€¤ã€\n",
        "\n",
        "Word2Vecã¨BERT/GPTã®å¤§ããªé•ã„ã¯ã€ã“ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ä½¿ã„æ–¹ã§ã™ã€‚\n",
        "\n",
        "* **Word2Vec:** `bank` ã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆè¾æ›¸ã®å€¤ï¼‰ã¯ä¸€åº¦è¨ˆç®—ã•ã‚ŒãŸã‚‰å›ºå®šã§ã™ã€‚\n",
        "* **BERT/GPT:** åŸ‹ã‚è¾¼ã¿å±¤ãŒæŒã¤ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€ã‚ãã¾ã§ã€Œ**åˆæœŸå€¤**ã€ï¼ˆæ–‡è„ˆã‚’è€ƒæ…®ã™ã‚‹å‰ã®ã€å˜èªã®åŸºæœ¬çš„ãªæ„å‘³ï¼‰ã§ã™ã€‚\n",
        "\n",
        "BERTã‚„GPTã¯ã€`Transformer` ã®ä¸­æ ¸æŠ€è¡“ã§ã‚ã‚‹ã€Œ**Attention**ã€ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2ã§å­¦ã³ã¾ã™ï¼‰ã‚’ä½¿ã£ã¦ã€æ–‡è„ˆï¼ˆ\"river\" ã‚„ \"money\"ï¼‰ã«å¿œã˜ã¦ã€ã“ã®åˆæœŸå€¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€Œæ–‡è„ˆã‚’åæ˜ ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã€ã«å‹•çš„ã«ä½œã‚Šå¤‰ãˆã¾ã™ã€‚\n",
        "\n",
        "ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã¾ãšBERTï¼ˆè»½é‡ç‰ˆã® `DistilBERT`ï¼‰ã®ã€ŒåˆæœŸå€¤ã€ã§ã‚ã‚‹åŸ‹ã‚è¾¼ã¿å±¤ã®å‹•ä½œã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
        "Word2Vecã¨åŒæ§˜ã«ã€**ã“ã®æ®µéšã§ã¯ã¾ã æ–‡è„ˆãŒåæ˜ ã•ã‚Œã¦ã„ãªã„**ï¼ˆï¼ `bank` ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒæ–‡è„ˆã«ã‚ˆã‚‰ãšåŒã˜ã§ã‚ã‚‹ï¼‰ã“ã¨ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
      ],
      "metadata": {
        "id": "6UA_pKltqL51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å–å¾—\n",
        "#@markdown ---\n",
        "#@markdown ### ãƒ•ã‚©ãƒ¼ãƒ \n",
        "#@markdown æ¯”è¼ƒã—ãŸã„2ã¤ã®æ–‡ã¨ã€ãã®æ–‡ã«å«ã¾ã‚Œã‚‹ã€Œå…±é€šã®å˜èªã€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n",
        "text1 = \"I ate an apple.\" #@param {type:\"string\"}\n",
        "text2 = \"I work at Apple.\" #@param {type:\"string\"}\n",
        "common_word = \"apple\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown ### ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¤ºã‚ªãƒ—ã‚·ãƒ§ãƒ³\n",
        "#@markdown ãƒ™ã‚¯ãƒˆãƒ«ã®å€¤ã‚’ã©ã“ã¾ã§è¡¨ç¤ºã—ã¾ã™ã‹ï¼Ÿï¼ˆ-1ã§å…¨ã¦è¡¨ç¤ºï¼‰\n",
        "display_dimensions = 5 #@param {type:\"slider\", min:-1, max:768, step:1}\n",
        "#@markdown ---\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# è»½é‡ãªBERT (DistilBERT) ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ç¢ºèª\n",
        "if 'model' not in globals():\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "    model = DistilBertModel.from_pretrained(model_name).to(device)\n",
        "    print(\"Model and Tokenizer loaded.\")\n",
        "\n",
        "# --- 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º ---\n",
        "inputs1 = tokenizer(text1, return_tensors=\"pt\").to(device)\n",
        "inputs2 = tokenizer(text2, return_tensors=\"pt\").to(device)\n",
        "\n",
        "print(f\"æ–‡1: {tokenizer.convert_ids_to_tokens(inputs1['input_ids'][0])}\")\n",
        "print(f\"æ–‡2: {tokenizer.convert_ids_to_tokens(inputs2['input_ids'][0])}\")\n",
        "\n",
        "# --- 2. å…±é€šå˜èªã®IDã‚’å–å¾— ---\n",
        "try:\n",
        "    token_id = tokenizer.convert_tokens_to_ids(common_word.lower())\n",
        "    if token_id == tokenizer.unk_token_id:\n",
        "        raise ValueError(f\"å˜èª '{common_word}' ãŒè¾æ›¸ã«ã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "    print(f\"\\n'{common_word}' ã®IDç•ªå·: {token_id}\")\n",
        "except ValueError as e:\n",
        "    print(e)\n",
        "    # ã“ã“ã§å‡¦ç†ã‚’ä¸­æ–­\n",
        "    pass\n",
        "\n",
        "if 'token_id' in locals() and token_id != tokenizer.unk_token_id:\n",
        "    # --- 3. åŸ‹ã‚è¾¼ã¿å±¤ã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾— ---\n",
        "    # (ãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿å±¤ã‚’ç›´æ¥ä½¿ã„ã¾ã™)\n",
        "    embedding_layer = model.embeddings.word_embeddings\n",
        "    token_id_tensor = torch.tensor([token_id]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embedding_vector = embedding_layer(token_id_tensor)\n",
        "\n",
        "    print(f\"\\n'{common_word}' ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚µã‚¤ã‚º: {embedding_vector.shape}\")\n",
        "    print(\"ï¼ˆã“ã‚Œã¯ 768æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã§ã™ï¼‰\")\n",
        "\n",
        "    # â–¼â–¼â–¼ è³ªå•2ã¸ã®å›ç­”ï¼ˆãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¤ºï¼‰ â–¼â–¼â–¼\n",
        "    if display_dimensions == -1:\n",
        "        print(f\"ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ (768æ¬¡å…ƒ):\\n{embedding_vector[0]}\")\n",
        "    else:\n",
        "        print(f\"ãƒ™ã‚¯ãƒˆãƒ«ã®ä¸€éƒ¨ï¼ˆæœ€åˆã®{display_dimensions}æ¬¡å…ƒï¼‰: {embedding_vector[0, :display_dimensions]}\")\n",
        "    # â–²â–²â–² ä¿®æ­£ã“ã“ã¾ã§ â–²â–²â–²\n",
        "\n",
        "    # --- 4. 2ã¤ã®æ–‡ã§ã®å·®ã®ç¢ºèª ---\n",
        "    with torch.no_grad():\n",
        "        embeddings1 = model.embeddings(inputs1['input_ids'])\n",
        "        embeddings2 = model.embeddings(inputs2['input_ids'])\n",
        "\n",
        "    index1_list = (inputs1['input_ids'][0] == token_id).nonzero(as_tuple=True)[0]\n",
        "    index2_list = (inputs2['input_ids'][0] == token_id).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    if len(index1_list) == 0 or len(index2_list) == 0:\n",
        "        print(f\"\\nã‚¨ãƒ©ãƒ¼: '{common_word}' ãŒä¸¡æ–¹ã®æ–‡ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "    else:\n",
        "        index1 = index1_list[0]\n",
        "        index2 = index2_list[0]\n",
        "        vec1_embedding = embeddings1[0, index1]\n",
        "        vec2_embedding = embeddings2[0, index2]\n",
        "\n",
        "        difference = torch.abs(vec1_embedding - vec2_embedding).sum()\n",
        "        print(f\"\\nåŸ‹ã‚è¾¼ã¿å±¤ã®æ®µéšã§ã¯ã€2ã¤ã®'{common_word}'ãƒ™ã‚¯ãƒˆãƒ«ã®å·®ã¯: {difference.item()}\")\n",
        "        if difference.item() < 1e-6:\n",
        "            print(\"â¡ï¸ æˆåŠŸ: åŒã˜ãƒ™ã‚¯ãƒˆãƒ«ã§ã™ã€‚æ–‡è„ˆã«é–¢ã‚ã‚‰ãšå›ºå®šã§ã™ã€‚\")\n",
        "        else:\n",
        "            print(\"â¡ï¸ å¤±æ•—ï¼Ÿ: ãƒ™ã‚¯ãƒˆãƒ«ã«å·®ãŒã‚ã‚Šã¾ã™ã€‚\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cS7yP6hCRxYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ğŸ‘©â€ğŸ“ **èª²é¡Œ**\n",
        ">\n",
        "> 1.  ä¸Šã®ã‚»ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã€`text1`, `text2`, `common_word` ã‚’ã€ã‚ãªãŸãŒè€ƒãˆãŸåˆ¥ã®å¤šç¾©èªï¼ˆä¾‹: `bat`, `right`, `fly`ãªã©ï¼‰ã‚’å«ã‚€æ–‡ã«å¤‰ãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "> 2.  `common_word` ã‚’å¤‰ãˆã¦ã‚‚ã€ãƒ™ã‚¯ãƒˆãƒ«ã®å·®ï¼ˆdifferenceï¼‰ãŒå¸¸ã«ã»ã¼ã‚¼ãƒ­ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "> 3.  ï¼ˆè€ƒå¯Ÿï¼‰ãªãœã“ã®æ®µéšã§ã¯ã€æ–‡è„ˆãŒé•ã†ã®ã«ãƒ™ã‚¯ãƒˆãƒ«ãŒåŒã˜ã«ãªã‚‹ã®ã‹ã€ã‚¹ãƒ†ãƒƒãƒ—1ã®è§£èª¬ã‚’èª­ã‚“ã§è‡ªåˆ†ã®è¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ï¼ˆè‡ªç”±è¨˜è¿°ï¼‰"
      ],
      "metadata": {
        "id": "x-mdgj8yV8L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ’¡ ã‚¹ãƒ†ãƒƒãƒ—2: æ–‡è„ˆã§æ„å‘³ãŒå¤‰ã‚ã‚‹ (Contextualization by Attention)\n",
        "\n",
        "ã‚¹ãƒ†ãƒƒãƒ—1ã§ã¯ã€å˜èªï¼ˆ`apple`ï¼‰ã¯æ–‡è„ˆï¼ˆã€Œé£Ÿã¹ã‚‹ã€ã‹ã€Œåƒãã€ã‹ï¼‰ã«é–¢ã‚ã‚‰ãšã€åŒã˜ãƒ™ã‚¯ãƒˆãƒ«ã§ã—ãŸã€‚\n",
        "ã“ã‚Œã§ã¯ã€ã€Œæœç‰©ã€ã¨ã€Œä¼šç¤¾ã€ã®åŒºåˆ¥ãŒã¤ãã¾ã›ã‚“ã€‚\n",
        "\n",
        "ãã“ã§ç™»å ´ã™ã‚‹ã®ãŒ **Attentionï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼‰** æ©Ÿæ§‹ã§ã™ã€‚\n",
        "\n",
        "Attentionã¯ã€ã”æŒ‡æ‘˜ã®é€šã‚Šã€ã€Œæ–‡ã®ä¸­ã«ã‚ã‚‹ä»–ã®å˜èªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã¨ã¨ã‚‚ã«é‡ã¿ä»˜ã‘ã€ã‚’ã—ã¾ã™ã€‚\n",
        "\n",
        "* \"I ate an **apple**.\" ã¨ã„ã†æ–‡ã§ã¯ã€Attentionã¯ `apple` ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œã‚‹ã¨ãã«ã€`ate` ã‚„ `an` ã«ã€Œæ³¨ç›®ã€ã—ã¾ã™ã€‚\n",
        "* \"I work at **Apple**.\" ã¨ã„ã†æ–‡ã§ã¯ã€Attentionã¯ `work` ã‚„ `at` ã«ã€Œæ³¨ç›®ã€ã—ã¾ã™ã€‚\n",
        "\n",
        "ã“ã®ã€Œæ³¨ç›®ï¼ˆAttentionï¼‰ã€ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘é›†ç´„ã®çµæœã€å…ƒã®ã€ŒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã€ï¼ˆæ–‡è„ˆãªã—ï¼‰ãŒã€æ–‡è„ˆã‚’åæ˜ ã—ãŸã€Œ**æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«**ã€ï¼ˆContextual Vectorï¼‰ã«å¤‰æ›ã•ã‚Œã¾ã™ã€‚\n",
        "\n",
        "ä»Šåº¦ã¯ã€`DistilBERT` ã®ã€ŒåŸ‹ã‚è¾¼ã¿å±¤ã€ã ã‘ã§ãªãã€ãƒ¢ãƒ‡ãƒ«å…¨ä½“ï¼ˆAttentionå±¤ã‚’å«ã‚€ï¼‰ã«æ–‡ã‚’å…¥åŠ›ã—ã¦ã€å‡ºåŠ›ã•ã‚Œã‚‹ãƒ™ã‚¯ãƒˆãƒ«ãŒã©ã†å¤‰ã‚ã‚‹ã‹è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚å¤šç¾©èª `bank`ï¼ˆéŠ€è¡Œï¼åœŸæ‰‹ï¼‰ã§è©¦ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "d8srH30KSRd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã®æ¯”è¼ƒ\n",
        "#@markdown ---\n",
        "#@markdown ### ãƒ•ã‚©ãƒ¼ãƒ \n",
        "#@markdown æ¯”è¼ƒã—ãŸã„2ã¤ã®æ–‡ã¨ã€ãã®æ–‡ã«å«ã¾ã‚Œã‚‹ã€Œå…±é€šã®å˜èªã€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n",
        "text_context1 = \"I sat on the river bank.\" #@param {type:\"string\"}\n",
        "text_context2 = \"I went to the bank to deposit money.\" #@param {type:\"string\"}\n",
        "common_word_context = \"bank\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown æ¯”è¼ƒã—ãŸã„æ–‡è„ˆèªï¼ˆæ–‡1ã‹ã‚‰ï¼‰\n",
        "context_word1 = \"river\" #@param {type:\"string\"}\n",
        "#@markdown æ¯”è¼ƒã—ãŸã„æ–‡è„ˆèªï¼ˆæ–‡2ã‹ã‚‰ï¼‰\n",
        "context_word2 = \"money\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º ---\n",
        "inputs_context1 = tokenizer(text_context1, return_tensors=\"pt\").to(device)\n",
        "inputs_context2 = tokenizer(text_context2, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# --- 2. ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã«å…¥åŠ› ---\n",
        "with torch.no_grad():\n",
        "    outputs_context1 = model(**inputs_context1)\n",
        "    outputs_context2 = model(**inputs_context2)\n",
        "\n",
        "contextual_embeddings1 = outputs_context1.last_hidden_state\n",
        "contextual_embeddings2 = outputs_context2.last_hidden_state\n",
        "\n",
        "# --- 3. IDã‚’å–å¾— ---\n",
        "try:\n",
        "    common_token_id = tokenizer.convert_tokens_to_ids(common_word_context.lower())\n",
        "    context1_token_id = tokenizer.convert_tokens_to_ids(context_word1.lower())\n",
        "    context2_token_id = tokenizer.convert_tokens_to_ids(context_word2.lower())\n",
        "\n",
        "    # --- 4. ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º ---\n",
        "    common_index1 = (inputs_context1['input_ids'][0] == common_token_id).nonzero(as_tuple=True)[0][0]\n",
        "    common_index2 = (inputs_context2['input_ids'][0] == common_token_id).nonzero(as_tuple=True)[0][0]\n",
        "\n",
        "    context1_index = (inputs_context1['input_ids'][0] == context1_token_id).nonzero(as_tuple=True)[0][0]\n",
        "    context2_index = (inputs_context2['input_ids'][0] == context2_token_id).nonzero(as_tuple=True)[0][0]\n",
        "\n",
        "    common_vec1_context = contextual_embeddings1[0, common_index1].cpu().numpy()\n",
        "    common_vec2_context = contextual_embeddings2[0, common_index2].cpu().numpy()\n",
        "\n",
        "    context1_vec = contextual_embeddings1[0, context1_index].cpu().numpy()\n",
        "    context2_vec = contextual_embeddings2[0, context2_index].cpu().numpy()\n",
        "\n",
        "    # --- 5. æ¯”è¼ƒ (ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦) ---\n",
        "    similarity_common = cosine_similarity([common_vec1_context], [common_vec2_context])\n",
        "    print(f\"\\n--- æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã®æ¯”è¼ƒ ({common_word_context} vs {common_word_context}) ---\")\n",
        "    print(f\"æ–‡1ã®'{common_word_context}' ã¨ æ–‡2ã®'{common_word_context}' ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {similarity_common[0][0]:.4f}\")\n",
        "    if similarity_common[0][0] < 0.7:\n",
        "        print(\"â¡ï¸ æˆåŠŸ: é¡ä¼¼åº¦ãŒä½ãã€æ„å‘³ãŒåŒºåˆ¥ã•ã‚Œã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\"â¡ï¸ æ³¨æ„: é¡ä¼¼åº¦ãŒã‚ã¾ã‚Šä¸‹ãŒã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "\n",
        "    # --- 6. (ãŠã¾ã‘) æ–‡è„ˆã¨ã®é¡ä¼¼åº¦ ---\n",
        "    sim1_v_context1 = cosine_similarity([common_vec1_context], [context1_vec])[0][0]\n",
        "    sim1_v_context2 = cosine_similarity([common_vec1_context], [context2_vec])[0][0]\n",
        "    sim2_v_context1 = cosine_similarity([common_vec2_context], [context1_vec])[0][0]\n",
        "    sim2_v_context2 = cosine_similarity([common_vec2_context], [context2_vec])[0][0]\n",
        "\n",
        "    print(\"\\n--- (ãŠã¾ã‘) æ–‡è„ˆã¨ã®é¡ä¼¼åº¦ ---\")\n",
        "\n",
        "    # å¯è¦–åŒ– (è‹±èª)\n",
        "    df = pd.DataFrame({\n",
        "        f\"'{common_word_context}' (Context 1)\": [sim1_v_context1, sim1_v_context2],\n",
        "        f\"'{common_word_context}' (Context 2)\": [sim2_v_context1, sim2_v_context2]\n",
        "    }, index=[f\"'{context_word1}' (from C1)\", f\"'{context_word2}' (from C2)\"])\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(df, annot=True, cmap=\"viridis\", fmt=\".4f\")\n",
        "    plt.title(f\"Similarity between '{common_word_context}' and context words\")\n",
        "    plt.show()\n",
        "\n",
        "except IndexError:\n",
        "    print(f\"\\nã‚¨ãƒ©ãƒ¼: ãƒ•ã‚©ãƒ¼ãƒ ã«å…¥åŠ›ã•ã‚ŒãŸå˜èªãŒã€æŒ‡å®šã•ã‚ŒãŸæ–‡ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YeshR6kVSSqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ğŸ‘©â€ğŸ“ **èª²é¡Œ (ã‚¹ãƒ†ãƒƒãƒ—2)**\n",
        ">\n",
        "> 1.  ä¸Šã®ã‚»ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã€ã‚¹ãƒ†ãƒƒãƒ—1ã§è©¦ã—ãŸå¤šç¾©èªï¼ˆä¾‹: `bat`ï¼‰ã®æ–‡ã‚’å…¥åŠ›ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "> 2.  ã‚¹ãƒ†ãƒƒãƒ—1ã§ã¯é¡ä¼¼åº¦ãŒé«˜ã‹ã£ãŸï¼ˆå·®ãŒã»ã¼0ã ã£ãŸï¼‰ã®ã«å¯¾ã—ã€ã‚¹ãƒ†ãƒƒãƒ—2ï¼ˆAttentioné€šéå¾Œï¼‰ã§ã¯é¡ä¼¼åº¦ãŒä½ããªã‚‹ï¼ˆä¾‹: 0.7æœªæº€ï¼‰ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n",
        "> 3.  ï¼ˆè€ƒå¯Ÿï¼‰ãªãœAttentionã‚’é€šã™ã¨ã€åŒã˜å˜èªã§ã‚‚æ–‡è„ˆã«ã‚ˆã£ã¦ãƒ™ã‚¯ãƒˆãƒ«ãŒå¤‰ã‚ã‚‹ã®ã‹ã€ã‚¹ãƒ†ãƒƒãƒ—2ã®è§£èª¬ã‚’èª­ã‚“ã§è‡ªåˆ†ã®è¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ï¼ˆè‡ªç”±è¨˜è¿°ï¼‰"
      ],
      "metadata": {
        "id": "-lVsz80TV0GE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœï¸ ã‚¹ãƒ†ãƒƒãƒ—3: æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ (Generation)\n",
        "\n",
        "ã‚¹ãƒ†ãƒƒãƒ—2ã§ã¯ã€BERTï¼ˆã®è»½é‡ç‰ˆï¼‰ãŒæ–‡è„ˆã‚’èª­ã‚“ã§å˜èªã®æ„å‘³ã‚’ç†è§£ã™ã‚‹ä»•çµ„ã¿ï¼ˆç©´åŸ‹ã‚å•é¡ŒãŒå¾—æ„ï¼‰ã‚’è¦‹ã¾ã—ãŸã€‚BERTã¯æ–‡ã®**ä¸¡å´**ï¼ˆå·¦ã¨å³ï¼‰ã‚’è¦‹ã¦æ–‡è„ˆã‚’åˆ¤æ–­ã—ã¾ã™ã€‚\n",
        "\n",
        "ä¸€æ–¹ã€**GPT** ã¯ã€ã”æŒ‡æ‘˜ã®é€šã‚Šã€Œ**ãã“ã¾ã§ã®æ–‡ç« ã®æ„å‘³ã‚’é›†ç´„**ã€ã—ã€ã€Œ**æ¬¡ã®å˜èª**ã€ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã«ç‰¹åŒ–ã—ã¦ã„ã¾ã™ã€‚å·¦ã‹ã‚‰å³ã¸ã€ä¸€æ–¹é€šè¡Œã§å‡¦ç†ã—ã¾ã™ã€‚\n",
        "\n",
        "GPTãŒæ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚\n",
        "\n",
        "1.  å…¥åŠ›ã•ã‚ŒãŸæ–‡ï¼ˆä¾‹: \"The quick brown fox\"ï¼‰ã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åŒºåˆ‡ã‚Šã¾ã™ã€‚\n",
        "2.  å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã€ŒåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã€ã«ã—ã¾ã™ï¼ˆã‚¹ãƒ†ãƒƒãƒ—1ï¼‰ã€‚\n",
        "3.  Attentionï¼ˆGPTã®å ´åˆã¯ã€è‡ªåˆ†ã‚ˆã‚Šå·¦å´ã®å˜èªã«ã—ã‹æ³¨ç›®ã§ããªã„ä»•çµ„ã¿ï¼‰ã‚’ä½¿ã£ã¦ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã€Œæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ã«å¤‰æ›ã—ã¾ã™ï¼ˆã‚¹ãƒ†ãƒƒãƒ—2ï¼‰ã€‚\n",
        "4.  **é‡è¦:** æ–‡ã®**ä¸€ç•ªæœ€å¾Œ**ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆã“ã®ä¾‹ã§ã¯ `fox`ï¼‰ã®ã€Œæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ã‚’å–ã‚Šå‡ºã—ã¾ã™ã€‚ã“ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒã€ã€Œ\"The quick brown fox\" ã¾ã§èª­ã‚“ã æ™‚ç‚¹ã§ã®æ–‡è„ˆã®é›†ç´„ã€ã§ã™ã€‚\n",
        "5.  ã”æŒ‡æ‘˜ã®é€šã‚Šã€ã“ã®é›†ç´„ã•ã‚ŒãŸæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ï¼ˆä¾‹: 768æ¬¡å…ƒï¼‰ã‚’ã€ã‚ã‚‹ç‰¹åˆ¥ãªã€Œ**å…¨çµåˆå±¤ï¼ˆLinear Layerï¼‰**ã€ã«å…¥åŠ›ã—ã¾ã™ã€‚\n",
        "6.  ã“ã®å…¨çµåˆå±¤ã¯ã€**LM Head (Language Model Head)** ã¨å‘¼ã°ã‚Œã€ãã®å‡ºåŠ›ã®æ¬¡å…ƒæ•°ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒçŸ¥ã£ã¦ã„ã‚‹**è¾æ›¸ã®ç·å˜èªæ•°**ï¼ˆä¾‹: 50257æ¬¡å…ƒï¼‰ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
        "7.  å‡ºåŠ›ã•ã‚ŒãŸ50257å€‹ã®æ•°å€¤ï¼ˆ**ãƒ­ã‚¸ãƒƒãƒˆ**ã¨å‘¼ã°ã‚Œã¾ã™ï¼‰ã‚’ã€**Softmax** ã¨ã„ã†é–¢æ•°ã«é€šã™ã“ã¨ã§ã€åˆè¨ˆãŒ1.0ã«ãªã‚‹ã€Œç¢ºç‡ã€ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
        "8.  ã“ã‚ŒãŒã€ã€Œ\"The quick brown fox\" ã®æ¬¡ã«ãã‚‹å˜èªã®ç¢ºç‡åˆ†å¸ƒã€ã§ã™ã€‚\n",
        "9.  ç¢ºç‡ãŒæœ€ã‚‚é«˜ã„å˜èªï¼ˆä¾‹: `jumps`ï¼‰ã‚’é¸ã‚“ã ã‚Šã€ç¢ºç‡ã«åŸºã¥ã„ã¦ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã‚“ã ã‚Šï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰ã™ã‚‹ã“ã¨ã§ã€æ–‡ç« ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚\n",
        "\n",
        "å®Ÿéš›ã« `GPT-2`ï¼ˆGPTã®åˆæœŸã®å…¬é–‹ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ä½¿ã£ã¦ã€æ¬¡ã®å˜èªã®ç¢ºç‡ãŒè¨ˆç®—ã•ã‚Œã‚‹æ§˜å­ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
      ],
      "metadata": {
        "id": "yAnDU6ObSc4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title æ¬¡å˜èªäºˆæ¸¬ã®ç¢ºç‡åˆ†å¸ƒ\n",
        "#@markdown ---\n",
        "#@markdown ### ãƒ•ã‚©ãƒ¼ãƒ \n",
        "#@markdown æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã•ã›ãŸã„ã€Œå…¥åŠ›æ–‡ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n",
        "prompt_text = \"The quick brown fox\" #@param {type:\"string\"}\n",
        "#@markdown è¡¨ç¤ºã—ãŸã„äºˆæ¸¬å€™è£œã®æ•°\n",
        "top_k_value = 10 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "#@markdown ---\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# GPT-2 ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãŒãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ç¢ºèª\n",
        "if 'model_gpt' not in globals():\n",
        "    model_name_gpt = \"gpt2\"\n",
        "    tokenizer_gpt = GPT2Tokenizer.from_pretrained(model_name_gpt)\n",
        "    model_gpt = GPT2LMHeadModel.from_pretrained(model_name_gpt).to(device)\n",
        "    if tokenizer_gpt.pad_token is None:\n",
        "        tokenizer_gpt.pad_token = tokenizer_gpt.eos_token\n",
        "        model_gpt.config.pad_token_id = model_gpt.config.eos_token_id\n",
        "    print(\"GPT-2 Model and Tokenizer loaded.\")\n",
        "\n",
        "# --- 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º ---\n",
        "inputs_gpt = tokenizer_gpt(prompt_text, return_tensors=\"pt\").to(device)\n",
        "input_ids = inputs_gpt['input_ids']\n",
        "\n",
        "print(f\"å…¥åŠ›æ–‡: '{prompt_text}'\")\n",
        "\n",
        "# 'Ä ' ãŒè¡¨ç¤ºã•ã‚Œãªã„ã‚ˆã†ã«ä¿®æ­£\n",
        "token_ids_list = input_ids[0].tolist()\n",
        "tokens_display = [tokenizer_gpt.decode([token_id]) for token_id in token_ids_list]\n",
        "print(f\"  -> ãƒˆãƒ¼ã‚¯ãƒ³: {tokens_display}\")\n",
        "\n",
        "\n",
        "# --- 2. ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ› ---\n",
        "with torch.no_grad():\n",
        "    outputs_gpt = model_gpt(**inputs_gpt)\n",
        "\n",
        "# --- 3. ãƒ­ã‚¸ãƒƒãƒˆã®å–å¾— ---\n",
        "logits = outputs_gpt.logits\n",
        "if logits.shape[1] == 0: # å…¥åŠ›ãŒç©ºã®å ´åˆ\n",
        "     print(\"ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãŒç©ºã§ã™ã€‚\")\n",
        "else:\n",
        "    # --- 4. ã€Œæœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã€ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’å–å¾— ---\n",
        "    last_token_logits = logits[0, -1, :]\n",
        "\n",
        "    # --- 5. Softmax ã§ç¢ºç‡ã«å¤‰æ› ---\n",
        "    probabilities = F.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "    # --- 6. ç¢ºç‡ãŒé«˜ã„å˜èªã‚’è¡¨ç¤º ---\n",
        "    top_k_probs, top_k_indices = torch.topk(probabilities, top_k_value)\n",
        "\n",
        "    print(f\"\\n--- ã€Œ{prompt_text}ã€ã®æ¬¡ã«ãã‚‹å˜èªã®äºˆæ¸¬ (Top {top_k_value}) ---\")\n",
        "\n",
        "    results = []\n",
        "    for i in range(top_k_value):\n",
        "        token_id = top_k_indices[i].item()\n",
        "        token_str = tokenizer_gpt.decode(token_id)\n",
        "        prob = top_k_probs[i].item()\n",
        "        results.append((token_str, prob * 100))\n",
        "\n",
        "    df_probs = pd.DataFrame(results, columns=[\"Predicted Word\", \"Probability (%)\"])\n",
        "    print(df_probs.to_string(index=False))\n",
        "\n",
        "    # --- 7. å¯è¦–åŒ– (è‹±èª) ---\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    df_probs.set_index(\"Predicted Word\").plot(kind='bar', legend=False)\n",
        "    plt.title(f\"Next word probabilities for '{prompt_text}'\")\n",
        "    plt.ylabel(\"Probability (%)\")\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7) # â† ä¿®æ­£å¾Œ\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # --- 8. å‹•çš„ãªçµæœã®è¡¨ç¤º ---\n",
        "    top_word = df_probs.iloc[0][\"Predicted Word\"]\n",
        "    print(f\"\\nâ¡ï¸ GPTã¯ '{top_word}' ãŒæ¬¡ã«æ¥ã‚‹ç¢ºç‡ãŒæœ€ã‚‚é«˜ã„ã¨äºˆæ¸¬ã—ã¾ã—ãŸï¼\")"
      ],
      "metadata": {
        "id": "5DYxpVD6SevU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ğŸ‘©â€ğŸ“ **èª²é¡Œ (ã‚¹ãƒ†ãƒƒãƒ—3)**\n",
        ">\n",
        "> 1.  ä¸Šã®ã‚»ãƒ«ã®ãƒ•ã‚©ãƒ¼ãƒ æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ã€`prompt_text` ã«æ§˜ã€…ãªï¼ˆè‹±èªã®ï¼‰æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "> 2.  ï¼ˆä¾‹: \"The capital of Japan is\", \"My hobby is\", \"I like to eat\" ãªã©ï¼‰\n",
        "> 3.  GPTãŒäºˆæ¸¬ã™ã‚‹ã€Œæ¬¡ã®å˜èªã€ã¯ã€æ–‡è„ˆã¨ã—ã¦å¦¥å½“ã§ã—ã‚‡ã†ã‹ï¼Ÿ `top_k_value` ã‚’å¤‰ãˆã‚‹ã¨ã€äºˆæ¸¬å€™è£œã¯ã©ã®ã‚ˆã†ã«å¤‰ã‚ã‚‹ã‹è¦³å¯Ÿã—ã¦ãã ã•ã„ã€‚\n",
        "> 4.  ï¼ˆè€ƒå¯Ÿï¼‰GPTã¯ã€ãªãœã€Œãã“ã¾ã§ã®æ–‡è„ˆã€ã ã‘ã§æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã§ãã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ ã‚¹ãƒ†ãƒƒãƒ—3ã®è§£èª¬ï¼ˆç‰¹ã«LM Headï¼‰ã‚’èª­ã‚“ã§ã€è‡ªåˆ†ã®è¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ï¼ˆè‡ªç”±è¨˜è¿°ï¼‰"
      ],
      "metadata": {
        "id": "wL7Vf_d7VwBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# æŒ¯ã‚Šè¿”ã‚Šèª²é¡Œï¼ˆç©´åŸ‹ã‚ï¼‰\n",
        "\n",
        "ã“ã“ã¾ã§ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å­¦ã‚“ã ã“ã¨ã‚’ã€ä»¥ä¸‹ã®ç©´åŸ‹ã‚å•é¡Œã§æŒ¯ã‚Šè¿”ã‚Šã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "ï¼ˆ `...` ã®éƒ¨åˆ†ã«å½“ã¦ã¯ã¾ã‚‹é©åˆ‡ãªèªå¥ã‚’è€ƒãˆã¦ãã ã•ã„ï¼‰\n",
        "\n",
        "1.  ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯å˜èªã‚’ãã®ã¾ã¾æ‰±ãˆãªã„ãŸã‚ã€ã€Œ`...`ã€(Embedding) ã¨ã„ã†ã€å˜èªã‚’æ•°å€¤ã®ãƒªã‚¹ãƒˆï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«å¤‰æ›ã™ã‚‹å±¤ã‚’ä½¿ã„ã¾ã™ã€‚\n",
        "2.  ã‚¹ãƒ†ãƒƒãƒ—1ã®æ®µéšã§ã¯ã€`bank` ã®ã‚ˆã†ãªå¤šç¾©èªã‚‚ã€æ–‡è„ˆã«é–¢ã‚ã‚‰ãš `...` (åŒã˜/ç•°ãªã‚‹) ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ç¾ã•ã‚Œã¾ã™ã€‚\n",
        "3.  ã‚¹ãƒ†ãƒƒãƒ—2ã§ã¯ã€`...` (Attention) æ©Ÿæ§‹ãŒã€æ–‡ä¸­ã®ä»–ã®å˜èªã«ã€Œæ³¨ç›®ã€ã—ã€å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ–‡è„ˆã«å¿œã˜ãŸã€Œæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ã«å¤‰æ›ã—ã¾ã—ãŸã€‚\n",
        "4.  ã‚¹ãƒ†ãƒƒãƒ—3ã®GPTã¯ã€æ–‡ã® `...` (æœ€åˆ/æœ€å¾Œ) ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€ã€Œãã“ã¾ã§ã®æ–‡è„ˆã®é›†ç´„ã€ã¨ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚\n",
        "5.  ã“ã®é›†ç´„ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€å‡ºåŠ›æ¬¡å…ƒãŒè¾æ›¸ã®ç·å˜èªæ•°ã«ãªã£ã¦ã„ã‚‹ `...` (LM Head / åŸ‹ã‚è¾¼ã¿å±¤) ã¨å‘¼ã°ã‚Œã‚‹å…¨çµåˆå±¤ã«å…¥åŠ›ã—ã¾ã™ã€‚\n",
        "6.  æœ€å¾Œã« `...` (Softmax) é–¢æ•°ã‚’ä½¿ã£ã¦ã€è¾æ›¸ã®å…¨å˜èªãŒã€Œæ¬¡ã«ãã‚‹ç¢ºç‡ã€ã‚’è¨ˆç®—ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "4xtIANXyZDgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ğŸš€ èª²é¡Œ: BERT ã¨ GPT ã®é•ã„\n",
        "\n",
        "ç§ãŸã¡ã¯ã‚¹ãƒ†ãƒƒãƒ—2ãƒ»3ã§ `BERT`ï¼ˆã®è»½é‡ç‰ˆï¼‰ã‚’ã€ã‚¹ãƒ†ãƒƒãƒ—3ã§ `GPT` ã‚’ä½¿ã„ã¾ã—ãŸã€‚ã©ã¡ã‚‰ã‚‚åŒã˜ã€ŒTransformerã€ã¨ã„ã†æŠ€è¡“ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ãŒã€ç›®çš„ãŒå¤§ããç•°ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "### 1. ç›®çš„ã®é•ã„ï¼ˆç©´åŸ‹ã‚ï¼‰\n",
        "\n",
        "ä»¥ä¸‹ã®èª¬æ˜ã‚’èª­ã¿ã€`...` ã«ï¼ˆBERT / GPTï¼‰ã®ã©ã¡ã‚‰ãŒå…¥ã‚‹ã‹è€ƒãˆã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "* **`...` (BERT)** ã¯ã€æ–‡ã®**ä¸¡å´**ï¼ˆå·¦å³ï¼‰ã®æ–‡è„ˆã‚’èª­ã‚“ã§ã€æ–‡ä¸­ã®å˜èªã®æ„å‘³ã‚’æ·±ãç†è§£ã™ã‚‹ã®ãŒå¾—æ„ã§ã™ã€‚ä¸»ãªã‚¿ã‚¹ã‚¯ã¯ã€Œç©´åŸ‹ã‚å•é¡Œï¼ˆMasked Language Modelï¼‰ã€ã‚„ã€Œæ–‡ç« åˆ†é¡ã€ã§ã™ã€‚\n",
        "* **`...` (GPT)** ã¯ã€æ–‡ã®**å·¦å´**ï¼ˆãã“ã¾ã§ï¼‰ã®æ–‡è„ˆã ã‘ã‚’èª­ã‚“ã§ã€**æ¬¡**ã«æ¥ã‚‹å˜èªã‚’äºˆæ¸¬ã™ã‚‹ã®ãŒå¾—æ„ã§ã™ã€‚ä¸»ãªã‚¿ã‚¹ã‚¯ã¯ã€Œæ–‡ç« ç”Ÿæˆï¼ˆGenerativeï¼‰ã€ã§ã™ã€‚\n",
        "\n",
        "### 2. ã€Œæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ã®æ„å‘³ã®é•ã„\n",
        "\n",
        "ã“ã®ç›®çš„ã®é•ã„ã¯ã€ã€Œæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ãŒä½•ã‚’è¡¨ã—ã¦ã„ã‚‹ã‹ã«ç¾ã‚Œã¾ã™ã€‚\n",
        "\n",
        "* **BERTã®æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«:**\n",
        "    * `bank` ã®ãƒ™ã‚¯ãƒˆãƒ«ã¯ã€ãã®å˜èªã€Œè‡ªä½“ã€ãŒæ–‡è„ˆï¼ˆ\"river\" ã‚„ \"money\"ï¼‰ã®ä¸­ã§æŒã¤æ„å‘³ï¼ˆåœŸæ‰‹/éŠ€è¡Œï¼‰ã‚’å¼·ãè¡¨ç¾ã—ã¾ã™ã€‚ç©´åŸ‹ã‚ã«ä½¿ã†ãŸã‚ã€ãã®å˜èªã®æ„å‘³ç‰¹å®šãŒé‡è¦ã§ã™ã€‚\n",
        "* **GPTã®æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«:**\n",
        "    * `bank` ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚‚æ–‡è„ˆï¼ˆ\"river\"ï¼‰ã‚’èª­ã¿è¾¼ã¿ã¾ã™ãŒã€ãã®ç›®çš„ã¯ã€Œ`bank` **ã®æ¬¡**ã«ãã‚‹å˜èªã€ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®ã€Œé€”ä¸­çµŒéã€ã§ã™ã€‚\n",
        "    * GPTãŒæœ€ã‚‚é‡è¦è¦–ã™ã‚‹ã®ã¯ã€**æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³**ã®ãƒ™ã‚¯ãƒˆãƒ«ã§ã™ã€‚ã“ã‚ŒãŒã€Œãã“ã¾ã§ã®å…¨æƒ…å ±ã®é›†ç´„ã€ã¨ãªã‚Šã€æ¬¡ã®å˜èªäºˆæ¸¬ã«ä½¿ã‚ã‚Œã¾ã™ã€‚\n",
        "\n",
        "ã”æŒ‡æ‘˜ã®é€šã‚Šã€GPTã«ã¨ã£ã¦ã€Œ`bank` ãŒå°å·ã‹éŠ€è¡Œã‹ã€ã®åŒºåˆ¥ã¯ã€**æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ã®ã«å¿…è¦ãªåˆ†ã ã‘**è¡Œã‚ã‚Œã¾ã™ã€‚BERTã»ã©ãã®å˜èªè‡ªä½“ã®æ„å‘³ã‚’çªãè©°ã‚ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
        "\n",
        "### 3. å®Ÿè·µ: BERTã¨GPTã®ã€Œæœ€å¾Œã®ãƒ™ã‚¯ãƒˆãƒ«ã€ã®æ¯”è¼ƒ\n",
        "\n",
        "åŒã˜æ–‡ã‚’BERTã¨GPTã«å…¥åŠ›ã—ã€**æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³**ï¼ˆ`bank`ï¼‰ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒã€ãã‚Œãã‚Œã©ã®å˜èªã¨ä¸€ç•ªä¼¼ã¦ã„ã‚‹ã‹ï¼ˆè¿‘ã„ã‹ï¼‰ã‚’æ¯”è¼ƒã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "* BERTï¼ˆ`DistilBERT`ï¼‰ã¯ `bank` ã®æ„å‘³ï¼ˆ`river` ã«è¿‘ã„ï¼‰ã‚’å¼·ãåæ˜ ã™ã‚‹ã¯ãšã§ã™ã€‚\n",
        "* GPTï¼ˆ`GPT-2`ï¼‰ã¯ `bank` ã®æ„å‘³ã‚‚æŒã¡ã¤ã¤ã€ã€Œæ¬¡ã€ã®äºˆæ¸¬ã®ãŸã‚ã®æƒ…å ±ã‚’é›†ç´„ã—ã¦ã„ã‚‹ã¯ãšã§ã™ã€‚"
      ],
      "metadata": {
        "id": "g2oJzb7rZFVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ğŸ§‘â€ğŸ“ BERT vs GPT ãƒ™ã‚¯ãƒˆãƒ«æ¯”è¼ƒ\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ãƒ•ã‚©ãƒ¼ãƒ \n",
        "#@markdown æ¯”è¼ƒã—ãŸã„å…±é€šã®æ–‡ï¼ˆæœ€å¾ŒãŒå¤šç¾©èªã ã¨åˆ†ã‹ã‚Šã‚„ã™ã„ï¼‰\n",
        "common_text = \"I sat on the river bank\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- 1. BERT (DistilBERT) ã§ãƒ™ã‚¯ãƒˆãƒ«å–å¾— ---\n",
        "print(\"--- 1. BERT (DistilBERT) ---\")\n",
        "inputs_bert = tokenizer(common_text, return_tensors=\"pt\").to(device)\n",
        "tokens_bert = tokenizer.convert_ids_to_tokens(inputs_bert['input_ids'][0])\n",
        "print(f\"BERT ãƒˆãƒ¼ã‚¯ãƒ³: {tokens_bert}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs_bert = model(**inputs_bert)\n",
        "\n",
        "# BERTã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ (\"bank\") ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
        "# [CLS] ãƒˆãƒ¼ã‚¯ãƒ³ãŒå…ˆé ­ã«ã€[SEP] ãƒˆãƒ¼ã‚¯ãƒ³ãŒæœ«å°¾ã«è¿½åŠ ã•ã‚Œã‚‹ãŸã‚ã€\n",
        "# æ–‡ã®æœ€å¾Œã®å˜èª 'bank' ã¯ã€ãƒªã‚¹ãƒˆã®æœ€å¾Œã‹ã‚‰2ç•ªç›® (-2) ã«ãªã‚Šã¾ã™ã€‚\n",
        "last_token_index_bert = -2\n",
        "bert_last_vec = outputs_bert.last_hidden_state[0, last_token_index_bert, :].cpu().numpy()\n",
        "\n",
        "# BERTã® \"river\" ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
        "river_token_id = tokenizer.convert_tokens_to_ids(\"river\")\n",
        "river_index_bert_list = (inputs_bert['input_ids'][0] == river_token_id).nonzero(as_tuple=True)[0]\n",
        "if len(river_index_bert_list) == 0:\n",
        "    print(\"ã‚¨ãƒ©ãƒ¼: BERTãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã§ 'river' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    bert_river_vec = None\n",
        "else:\n",
        "    river_index_bert = river_index_bert_list[0]\n",
        "    bert_river_vec = outputs_bert.last_hidden_state[0, river_index_bert, :].cpu().numpy()\n",
        "\n",
        "\n",
        "# --- 2. GPT-2 ã§ãƒ™ã‚¯ãƒˆãƒ«å–å¾— ---\n",
        "print(\"\\n--- 2. GPT-2 ---\")\n",
        "inputs_gpt = tokenizer_gpt(common_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# â–¼â–¼â–¼ 'Ä ' ãŒè¡¨ç¤ºã•ã‚Œãªã„ã‚ˆã†ã«ä¿®æ­£ â–¼â–¼â–¼\n",
        "token_ids_gpt_list = inputs_gpt['input_ids'][0].tolist()\n",
        "tokens_gpt_display = [tokenizer_gpt.decode([token_id]) for token_id in token_ids_gpt_list]\n",
        "print(f\"GPT ãƒˆãƒ¼ã‚¯ãƒ³: {tokens_gpt_display}\")\n",
        "# â–²â–²â–² ä¿®æ­£ã“ã“ã¾ã§ â–²â–²â–²\n",
        "\n",
        "with torch.no_grad():\n",
        "    # GPTã¯ .last_hidden_state ã‚’ç›´æ¥è¿”ã•ãªã„ã®ã§ã€.transformer(...) ã‹ã‚‰å–å¾—\n",
        "    gpt_hidden_states = model_gpt.transformer(**inputs_gpt).last_hidden_state\n",
        "\n",
        "# GPTã®æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ (\"bank\") ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
        "# (GPT-2ã¯ [CLS] ã‚„ [SEP] ã‚’è‡ªå‹•ã§è¿½åŠ ã—ãªã„ãŸã‚ã€æ–‡ã®æœ€å¾ŒãŒãƒªã‚¹ãƒˆã®æœ€å¾Œã§ã™)\n",
        "last_token_index_gpt = -1 # ç©´åŸ‹ã‚: -1\n",
        "gpt_last_vec = gpt_hidden_states[0, last_token_index_gpt, :].cpu().numpy()\n",
        "\n",
        "# GPTã® \"river\" ã®ãƒ™ã‚¯ãƒˆãƒ«\n",
        "# (GPTã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯ 'Ä river' ã®ã‚ˆã†ã«ç©ºç™½ã‚’è¨˜å·ã§æ‰±ã†)\n",
        "# 'Ä river' (ã‚¹ãƒšãƒ¼ã‚¹ä»˜ã) ã‚’å„ªå…ˆçš„ã«æ¢ã™\n",
        "river_token_id_gpt = tokenizer_gpt.convert_tokens_to_ids(\"Ä river\")\n",
        "if river_token_id_gpt == tokenizer_gpt.unk_token_id:\n",
        "    # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã° 'river' (ã‚¹ãƒšãƒ¼ã‚¹ãªã—) ã‚’æ¢ã™\n",
        "    river_token_id_gpt = tokenizer_gpt.convert_tokens_to_ids(\"river\")\n",
        "\n",
        "river_index_gpt_list = (inputs_gpt['input_ids'][0] == river_token_id_gpt).nonzero(as_tuple=True)[0]\n",
        "if len(river_index_gpt_list) == 0:\n",
        "    print(\"ã‚¨ãƒ©ãƒ¼: GPT-2ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã§ 'river' (ã¾ãŸã¯ 'Ä river') ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "    gpt_river_vec = None\n",
        "else:\n",
        "    river_index_gpt = river_index_gpt_list[0]\n",
        "    gpt_river_vec = gpt_hidden_states[0, river_index_gpt, :].cpu().numpy()\n",
        "\n",
        "\n",
        "# --- 3. é¡ä¼¼åº¦ã®æ¯”è¼ƒ ---\n",
        "# ä¸¡æ–¹ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒæ­£å¸¸ã«å–å¾—ã§ããŸå ´åˆã®ã¿å®Ÿè¡Œ\n",
        "if bert_river_vec is not None and gpt_river_vec is not None:\n",
        "    print(\"\\n--- 3. é¡ä¼¼åº¦æ¯”è¼ƒ ---\")\n",
        "\n",
        "    # BERT: \"bank\" ã¨ \"river\" ã®é¡ä¼¼åº¦\n",
        "    sim_bert = cosine_similarity([bert_last_vec], [bert_river_vec]) # ç©´åŸ‹ã‚: [bert_last_vec], [bert_river_vec]\n",
        "    print(f\"BERT: æœ€å¾Œã® 'bank' ã¨ 'river' ã®é¡ä¼¼åº¦: {sim_bert[0][0]:.4f}\")\n",
        "\n",
        "    # GPT: \"bank\" ã¨ \"river\" ã®é¡ä¼¼åº¦\n",
        "    sim_gpt = cosine_similarity([gpt_last_vec], [gpt_river_vec]) # ç©´åŸ‹ã‚: [gpt_last_vec], [gpt_river_vec]\n",
        "    print(f\"GPT: æœ€å¾Œã® 'bank' ã¨ 'river' ã®é¡ä¼¼åº¦: {sim_gpt[0][0]:.4f}\")\n",
        "\n",
        "    print(\"\\n(æ³¨: GPT-2ã¯ 'river' ã‚’ ' river' ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã™ã‚‹ã“ã¨ãŒã‚ã‚Šã€BERTã¨æ¡ä»¶ãŒå®Œå…¨ã«ã¯ä¸€è‡´ã—ã¾ã›ã‚“ãŒã€å‚¾å‘ã‚’æ¯”è¼ƒã—ã¾ã™)\")\n",
        "else:\n",
        "    print(f\"\\n--- 3. é¡ä¼¼åº¦æ¯”è¼ƒ (ã‚¹ã‚­ãƒƒãƒ—) ---\")\n",
        "    print(\"æ–‡è„ˆèª ('river') ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€é¡ä¼¼åº¦æ¯”è¼ƒã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RvlYhQ3LZG53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ğŸ‘©â€ğŸ“ **èª²é¡Œ (BERT vs GPT è€ƒå¯Ÿ)**\n",
        ">\n",
        "> 1.  BERTã¨GPTã§ã€æœ€å¾Œã®å˜èªï¼ˆ`bank`ï¼‰ã¨æ–‡è„ˆèªï¼ˆ`river`ï¼‰ã®é¡ä¼¼åº¦ã«ã©ã®ã‚ˆã†ãªé•ã„ãŒå‡ºãŸã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
        "> 2.  ï¼ˆè€ƒå¯Ÿï¼‰ã‚¹ãƒ†ãƒƒãƒ—3ã§å­¦ã‚“ã ã‚ˆã†ã«ã€GPTã¯ã“ã®ã€Œæœ€å¾Œã®ãƒ™ã‚¯ãƒˆãƒ«ã€ã‚’ä½¿ã£ã¦**æ¬¡ã®å˜èª**ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚ãªãœã€`bank` è‡ªä½“ã®æ„å‘³ï¼ˆ`river` ã«è¿‘ã„ã‹ã©ã†ã‹ï¼‰ã‚’çªãè©°ã‚ã‚‹ã‚ˆã‚Šã‚‚ã€ã€Œãã“ã¾ã§ã®æ–‡è„ˆã‚’é›†ç´„ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã€ã‚’ä½¿ã†æ–¹ãŒã€æ¬¡ã®å˜èªï¼ˆä¾‹: \"and\", \"is\", \"was\" ãªã©ï¼‰ã‚’äºˆæ¸¬ã™ã‚‹ã®ã«é©ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã‹ã€ã‚ãªãŸã®è€ƒãˆã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ï¼ˆè‡ªç”±è¨˜è¿°ï¼‰"
      ],
      "metadata": {
        "id": "FpMXn975ZQnE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ‰ ã¾ã¨ã‚\n",
        "\n",
        "ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ã“ã®èª²é¡Œã§ã¯ã€GPTã®ã‚ˆã†ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹3ã¤ã®åŸºæœ¬çš„ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’å­¦ã³ã¾ã—ãŸã€‚\n",
        "\n",
        "1.  **ğŸ“ åŸ‹ã‚è¾¼ã¿ (Embedding):**\n",
        "    å˜èªã‚’ã€Œæ„å‘³ã‚’æŒã¤æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã€ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã®æ™‚ç‚¹ã§ã¯ã€ã¾ã æ–‡è„ˆï¼ˆå¤šç¾©èªï¼‰ã¯è€ƒæ…®ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\n",
        "\n",
        "2.  **ğŸ’¡ æ–‡è„ˆåŒ– (Contextualization):**\n",
        "    **Attention** ã¨ã„ã†ä»•çµ„ã¿ãŒã€æ–‡ä¸­ã®ä»–ã®å˜èªã«ã€Œæ³¨ç›®ã€ã—ã€å…ƒã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ã€Œæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Š `bank` ã®ã‚ˆã†ãªå¤šç¾©èªã‚‚åŒºåˆ¥ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n",
        "\n",
        "3.  **âœï¸ ç”Ÿæˆ (Generation):**\n",
        "    GPTã¯ã€**æœ€å¾Œã®å˜èªã®æ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«**ï¼ˆï¼ãã“ã¾ã§ã®æ–‡è„ˆã®é›†ç´„ï¼‰ã‚’ **LM Headï¼ˆå…¨çµåˆå±¤ï¼‰** ã«é€šã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¾æ›¸ã«ã‚ã‚‹å…¨å˜èªã®ã€Œæ¬¡ã®å˜èªã¨ã—ã¦ã®ç¢ºç‡ã€ãŒè¨ˆç®—ã•ã‚Œã€æœ€ã‚‚ç¢ºç‡ã®é«˜ã„å˜èªãŒé¸ã°ã‚Œã¾ã™ã€‚\n",
        "\n",
        "ä»Šå›ã¯ã€Attention ãŒã©ã®ã‚ˆã†ã«é‡ã¿ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã‹ï¼Ÿï¼ˆSelf-Attention ã‚„ QKVï¼‰ã¨ã„ã†æ ¸å¿ƒéƒ¨åˆ†ã«ã¯è§¦ã‚Œã¾ã›ã‚“ã§ã—ãŸãŒã€å…¥åŠ›ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‰ã¨å‡ºåŠ›ï¼ˆæ–‡è„ˆãƒ™ã‚¯ãƒˆãƒ«ã€ç¢ºç‡ï¼‰ãŒã©ã†å¤‰ã‚ã‚‹ã‹ã‚’è¦‹ã‚‹ã“ã¨ã§ã€GPT ã«ã‚ˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å…¨ä½“åƒã‚’æ´ã‚“ã§ã„ãŸã ã‘ãŸã‹ã¨æ€ã„ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "DPWTNqjISoA0"
      }
    }
  ]
}
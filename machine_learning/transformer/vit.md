# Vision Transformerについて

## 概要
Vision Transformer (ViT) は、画像処理タスクにTransformerアーキテクチャを応用した革新的なモデルです。従来の畳み込みニューラルネットワーク（CNN）が画像の局所的な特徴を段階的に抽出するのに対し、ViTは画像を複数のパッチに分割し、これら全てのパッチを一度に考慮することで、画像全体のコンテキストを把握します。このアプローチにより、ViTは画像内の広範囲にわたる関係性を捉え、特に大規模なデータセットにおいて、従来のCNNモデルを上回る性能を発揮することが示されています。

自然言語処理 (NLP) 分野で成功を収めたTransformerモデルを画像に適用することで、ViTは、単語の並びや文脈を理解するのに類似した方法で、画像の「部分」とそれらがどのように全体の意味を形成するかを理解します。このように、ViTはNLPの技術を画像分析に応用し、画像の全体的な理解という新たな次元を開拓しました。

## 基本設定
- **入力画像サイズ**: $X \times Y$ ピクセル、チャネル数 $C$ （例: 224x224ピクセル, RGB画像として $C=3$ ）
- **パッチサイズ**: $P \times P$ ピクセル（例: 16x16ピクセル）
- **パッチの数 $(N)$**: $\frac{X}{P} \times \frac{Y}{P}$ （例: $14 \times 14 = 196$）
- **パッチの次元 $(D)$**: モデル固有のパラメータ（例: 512）
- **モデルのヘッド数 $(H)$**: モデル固有のパラメータ（例: 8）

## パッチの準備
ViTでは、入力画像（ $X \times Y$ ピクセル・ $C$ チャネル、例: 224x224ピクセル・3チャネル）を、 $P \times P$ ピクセルのパッチ（例: 16x16ピクセル）に分割して、各パッチを独立した情報の単位として扱います。パッチはフラット化され、全結合層を通じて特定の長さのベクトルに変換されます。

## 位置情報の追加
Transformerは元来、テキストデータを扱うために設計されました。テキストデータで順序があるように、画像のパッチが全体のどの位置にあるのかをモデルが理解するためには、位置情報を明示的に付与する必要があります。ViTでは、各パッチベクトルに位置エンコーディングのためのベクトルを加算します。

位置エンコーディングには
- 固定式（sin/cos関数に基づく）
- 学習可能なエンコーディング
の2種類があります。いずれも、パッチが元の画像のどこにあったかを表す情報をモデルに提供します。

## Transformer エンコード

### 前提

#### Self-Attention
自然言語処理で提案された Self-Attention は、ここでは各パッチが他の全パッチとどのように関連しているかを計算し、その情報を基に新たな特徴表現を生成します。

#### Multi-Head Attention
Multi-Head Attention では、各パッチに対して複数の「Attentionヘッド」を用いて処理を行います。これにより、モデルは画像の異なる部分から異なる種類の情報を同時に抽出することができます。Multi-Head Attention の主な目的は、画像の異なる特徴やパターンを複数の「視点」から捉えることです。

### Attention ヘッドごとの処理

#### Query, Key, Valueの生成
入力された特徴量ベクトル（ $D$ 次元、例: 512次元）は、各Headごとに独立した全結合層を通じて、Query(Q)、Key(K)、Value(V)に変換されます。これらのベクトルは、Attentionの計算に不可欠です。各全結合層は入力（ $D$ 次元）から、出力サイズが（ $D/H$ 次元、例: $512/8=64$ 次元）のベクトルを生成します。

### Attentionスコアの計算
QueryとKeyの間の内積を計算し、Attentionスコアを求めます。このスコアは、各要素が他の要素にどれだけ注目すべきかを示します。内積はベクトル間の「近さ」を数値化する一つの方法であり、この場合はあるパッチのQueryが、他の全パッチのKeyとどれだけ「近い」か、すなわちどれだけ関連があるかを示します。

内積は2つのベクトルがどれだけ同じ方向を向いているかを示す指標です。内積が大きいことは、2つのベクトルが似た方向を持つことを意味します。この文脈では、あるパッチが他のパッチとどれだけ関連しているかを示します。

各Queryベクトルに対して、全パッチから得たKeyベクトルとの内積を計算し、結果としてパッチの数 $(N)$ と同じ数のAttentionスコアが得られます（例: 196パッチの場合、196個のスコアがそれぞれのQueryに対して計算されます）。

### Softmaxによる正規化
スケーリングされたAttentionスコアにSoftmax関数を適用し、正規化されたAttentionの重みを得ます。この処理により、各Queryに対して計算されたスコアが0から1の間の値に変換され、全スコアの合計が1になります。これにより、各Keyに対するQueryの関心度が確率として表現されます。

- **Softmax関数とは**: 一連の数値を正規化して確率分布に変換する関数です。式で表すと $\text{Softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}$ となります。これにより各Attentionスコアを確率に変換し、特定のQueryに対する全Keyの関連度を示します。

### Attentionの重みとVの積
得られたAttentionの重みを各Valueに適用し、重み付け和を計算します。この処理により、あるQueryに対して、全パッチからのValueに対する加重平均が計算され、新しい特徴表現が形成されます。この新しい特徴表現は、各AttentionヘッドにおけるValueの次元 $(D/H)$ と同じサイズを持ちます（例: $D=512$, $H=8$ の場合、各出力ベクトルのサイズは 64 次元になります）。


## タスクへの利用
Transformerエンコードにより得られた出力ベクトル（ $D$ 次元、例: 512次元）は、画像分類や物体検出といった各種のタスクに使用できます。このようにして、ViT は画像に含まれる複雑な情報を効果的に活用し、様々な視覚タスクにおいて高い性能を発揮することができます。

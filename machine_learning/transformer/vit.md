# Vision Transformerについて

## 概要
Vision Transformer (ViT) は、画像処理タスクにTransformerアーキテクチャを応用した革新的なモデルです。従来の畳み込みニューラルネットワーク（CNN）が画像の局所的な特徴を段階的に抽出するのに対し、ViTは画像を複数のパッチに分割し、これら全てのパッチを一度に考慮することで、画像全体のコンテキストを把握します。このアプローチにより、ViTは画像内の広範囲にわたる関係性を捉え、特に大規模なデータセットにおいて、従来のCNNモデルを上回る性能を発揮することが示されています。

[自然言語処理の分野で成功を収めたTransformerモデル](./mlp.md)を画像に適用することで、ViTは、単語の並びや文脈を理解するのに類似した方法で、画像の「部分」とそれらがどのように全体の意味を形成するかを理解します。このように、ViTは自然言語処理の技術を画像分析に応用し、画像の全体的な理解を可能としました。


<img src="https://github.com/shizoda/education/assets/34496702/819c94b6-355e-43cc-a79c-a6b0a50f02dd" width=40%>

*[原論文 [Dosovitskiy21]](https://openreview.net/forum?id=YicbFdNTTy) Fig. 6 の一部。注目される領域*


*** 

## 手法

ここから具体的な処理について見ていきます。

<img src="https://github.com/shizoda/education/assets/34496702/f4d8612f-66a6-4302-8163-dd5d8d63f27d" width=60%>

*[原論文 [Dosovitskiy21]](https://openreview.net/forum?id=YicbFdNTTy) Fig. 1 の一部。全体構成を表す*

#### 前提

- **入力画像サイズ**: $X \times Y$ ピクセル、チャネル数 $C$ （例: 224x224ピクセル, RGB画像として $C=3$ ）
- **パッチサイズ**: $P \times P$ ピクセル（例: 16x16ピクセル）
- **パッチの数 $(N)$**: $\frac{X}{P} \times \frac{Y}{P}$ （例: $14 \times 14 = 196$）
- **パッチの次元 $(D)$**: モデル固有のパラメータ（例: 512）
- **モデルのヘッド数 $(H)$**: モデル固有のパラメータ（例: 8）

## パッチの準備
ViTでは、入力画像（ $X \times Y$ ピクセル・ $C$ チャネル、例: 224x224ピクセル・3チャネル）を、 $P \times P$ ピクセルのパッチ（例: 16x16ピクセル）に分割して、各パッチを独立した情報の単位として扱います。パッチはフラット化され、全結合層を通じて特定の長さのベクトルに変換されます。

## 位置情報の追加
Transformerは元来、テキストデータを扱うために設計されました。テキストデータで順序があるように、画像のパッチが全体のどの位置にあるのかをモデルが理解するためには、位置情報を明示的に付与する必要があります。ViTでは、各パッチベクトルに位置を表すベクトルを加算します。

位置エンコーディングには
- 固定式（sin/cos関数に基づく）
- 学習可能なエンコーディング
の2種類があります。いずれも、パッチが元の画像のどこにあったかを表す情報を提供します。

## Transformer エンコード

<img src="https://github.com/shizoda/education/assets/34496702/f077090b-f69b-484a-b448-c2d9bd1a5891" width=20%>

*[原論文 [Dosovitskiy21]](https://openreview.net/forum?id=YicbFdNTTy) Fig. 1 の一部。Transformer エンコーダの構成*

### 前提

#### Self-Attention
自然言語処理で提案された Self-Attention は、ここでは各パッチが他の全パッチとどのように関連しているかを計算し、その情報を基に新たな特徴表現を生成します。

#### Multi-Head Attention
Multi-Head Attention では、各パッチに対して複数の「Attentionヘッド」を用いて処理を行います。これにより、モデルは画像の異なる部分から異なる種類の情報を同時に抽出することができます。Multi-Head Attention の主な目的は、画像の異なる特徴やパターンを複数の「視点」から捉えることです。

### Attention ヘッドごとの処理

#### Query, Key, Valueの生成
入力された特徴量ベクトル（ $D$ 次元、例: 512次元）は、各Headごとに独立した全結合層を通じて、Query(Q)、Key(K)、Value(V)に変換されます。これらのベクトルは、Attentionの計算に不可欠です。各全結合層は入力（ $D$ 次元）から、出力サイズが（ $D/H$ 次元、例では $512/8=64$ 次元）のベクトルを生成します。

### Attentionスコアの計算
各Queryベクトルに対して、全パッチから得たKeyベクトルとの内積を計算し、結果としてパッチの数 （ $N$ 個） と同数の「Attentionスコア」を求めます（例: 196パッチの場合、196個のスコアがそれぞれのQueryに対して計算されます）。このAttentionスコアは、各要素が他の要素にどれだけ注目すべきかを示します。内積はベクトル間の「近さ」を数値化する一つの方法であり、この場合はあるパッチのQueryが、他の全パッチのKeyとどれだけ「近い」か、すなわちどれだけ関連があるかを示します。

内積は2つのベクトルがどれだけ同じ方向を向いているかを示す指標です。 $ \mathbf{Q} \cdot \mathbf{K} $ などと書かれ、内積の値が大きいほど 2 つのベクトルが似た方向を持つことを意味します。ViT では、あるパッチのQueryが、他のパッチのKeyとどれだけ関連しているかを示します。

### Softmaxによる正規化
スケーリングされたAttentionスコアにSoftmax関数を適用し、正規化されたAttentionの重みを得ます。この処理により、各Queryに対して計算されたスコアが0から1の間の値に変換され、全スコアの合計が1になります。これにより、各Keyに対するQueryの関心度が確率として表現されます。

- **Softmax関数とは**: 一連の数値を正規化して確率分布に変換する関数です。式で表すと $ \text{Softmax}(\mathbf{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}} $ となります。これにより各Attentionスコアを確率の形に変換し、特定のQueryに対する全Keyの関連度として用います。

### Attention を重みとした Value の和
得られたAttentionの重みを、全てのパッチから得たValueそれぞれに適用し、重み付け和を計算します。あるパッチのQueryについて、全パッチからのValueの加重平均によって特徴表現が作られます。この特徴表現は、各AttentionヘッドにおけるValueと同じサイズ（$D/H$ 次元）をもちます。例での $D=512$, $H=8$ の場合、各出力ベクトルのサイズは 64 次元となります。。

### 全 Head からの特徴表現の結合

Multi-Head Attentionの各ヘッドから出てくる $D/H$ 次元（例では64次元）のベクトルを結合（concatenate）すると、最終的 $D$ 次元（例では 512 次元）のベクトルとなります。

## タスクへの利用
Transformerエンコードにより得られた出力ベクトル（ $D$ 次元、例: 512次元）は、画像分類や物体検出といった各種のタスクに使用できます。このようにして、ViT は画像に含まれる複雑な情報を効果的に活用し、様々な視覚タスクにおいて高い性能を発揮することができます。

# Vision Transformerについて

## 概要
Vision Transformer (ViT) は、画像処理タスクにTransformerアーキテクチャを応用した革新的なモデルです。従来の畳み込みニューラルネットワーク（CNN）が画像の局所的な特徴を段階的に抽出するのに対し、ViTは画像を複数のパッチに分割し、これら全てのパッチを一度に考慮することで、画像全体のコンテキストを把握します。このアプローチにより、ViTは画像内の広範囲にわたる関係性を捉え、特に大規模なデータセットにおいて、従来のCNNモデルを上回る性能を発揮することが示されています。

自然言語処理 (NLP) 分野で成功を収めたTransformerモデルを画像に適用することで、ViTは、単語の並びや文脈を理解するのに類似した方法で、画像の「部分」とそれらがどのように全体の意味を形成するかを理解します。このように、ViTはNLPの技術を画像分析に応用し、画像の全体的な理解という新たな次元を開拓しました。

## 基本設定
- **入力画像サイズ**: $X \times Y$ ピクセル、チャネル数 $C$ （例: 224x224ピクセル、$C=3$ for RGB画像）
- **パッチサイズ**: $P \times P$ ピクセル（例: 16x16ピクセル）
- **パッチの数 $(N)$**: $\frac{X}{P} \times \frac{Y}{P}$ （例: $14 \times 14 = 196$）
- **パッチの次元 $(D)$**: モデル固有のパラメータ（例: 512）
- **モデルのヘッド数 $(H)$**: モデル固有のパラメータ（例: 8）

## パッチの準備
ViTでは、入力画像（ $X \times Y$ ピクセル・$C$ チャネル、例: 224x224ピクセル・3チャネル）を、$P \times P$ピクセルのパッチ（例: 16x16ピクセル）に分割して、各パッチを独立した情報の単位として扱います。パッチはフラット化され、全結合層を通じて特定の長さのベクトルに変換されます。

## 位置情報の追加
Transformerは元来、テキストデータを扱うために設計されました。テキストデータで順序があるように、画像のパッチが全体のどの位置にあるのかをモデルが理解するためには、位置情報を明示的に付与する必要があります。ViTでは、各パッチベクトルに位置エンコーディングのためのベクトルを加算します。

位置エンコーディングには
- 固定式（sin/cos関数に基づく）
- 学習可能なエンコーディング
の2種類があります。いずれも、パッチが元の画像のどこにあったかを表す情報をモデルに提供します。

## Transformer エンコード

### 前提

#### Self-Attention
自然言語処理で提案された Self-Attention は、ここでは各パッチが他の全パッチとどのように関連しているかを計算し、その情報を基に新たな特徴表現を生成します。

#### Multi-Head Attention
Multi-Head Attention では、各パッチに対して複数の「Attentionヘッド」を用いて処理を行います。これにより、モデルは画像の異なる部分から異なる種類の情報を同時に抽出することができます。Multi-Head Attention の主な目的は、画像の異なる特徴やパターンを複数の「視点」から捉えることです。

### Attention ヘッドごとの処理

#### Query, Key, Valueの生成
入力された特徴量ベクトル（$D$次元、例: 512次元）は、各Headごとに独立した全結合層を通じて、Query(Q)、Key(K)、Value(V)に変換されます。これらのベクトルは、Attentionの計算に不可欠です。各全結合層は入力（$D$次元）から、出力サイズが（$D/H$次元、例: $512/8=64$次元）のベクトルを生成します。

#### Attentionスコアの計算
QueryとKeyの間の内積を計算し、Attentionスコアを得ます。このスコアは、各要素が他の要素にどれだけ注目すべきかを示します。スコアは$D/H$の次元の平方根（例: $\sqrt{64}$）でスケーリングされ、内積が大きすぎることによる勾配の問題を防ぎます。

#### Softmaxによる正規化
スケーリングされたAttentionスコアにSoftmax関数を適用し、正規化されたAttentionの重みを得ます。Softmax関数は、スコアを確率に変換し、各Keyに対するQueryの関心度を確率として表現します。

#### Attentionの重みとVの積
得られたAttentionの重みを各Valueに適用し、重み付け和を計算します。この結果として、各Queryに対する加重平均されたValueが出力され、新しい特徴表現を形成します。

### 出力の結合と次元数
各Headからの出力ベクトル（$D/H$次元、例: 64次元）を結合して、最終的な出力ベクトル（$D$次元、例: 512次元）を再構成します。このベクトルは、画像の全体的なコンテキストを考慮した情報を含んでいます。

## タスクへの利用
Transformerエンコードにより得られた出力ベクトル（$D$次元、例: 512次元）は、画像分類や物体検出といった各種のタスクに使用できます。このようにして、ViTは画像に含まれる複雑な情報を効果的に活用し、様々な視覚タスクにおいて高い性能を発揮することができます。

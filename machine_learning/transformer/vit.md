# Vision Transformer (ViT)

## 概略

Vision Transformer (ViT) は、Transformer モデルの Self-Attention を画像処理に適用したものです。

<img src="https://github.com/shizoda/education/assets/34496702/f4d8612f-66a6-4302-8163-dd5d8d63f27d" width=70%>

*[原論文](https://openreview.net/forum?id=YicbFdNTTy/) Fig. 1 より*                        

#### シーケンスとして画像を扱う

ViT では、画像を一連のパッチ（小さな画像の断片）に分割し、それらを単語のシーケンスと同様に扱います。そして Self-Attention により、各画像パッチ間の関係性を捉えます。これにより、画像内の各位置のパッチが他の位置とどのように関連しているかをモデルが学習します。

#### CNN との違い

CNNでは、畳み込み層が画像の局所的な特徴を抽出し、それらを段階的に組み合わせて画像全体を理解します。それに対して ViT は、Self-Attention により画像全体を理解でき、画像内の遠く離れた要素間の関連性も捉えることができます。

## 手順

#### パッチ化

ViTは入力として与えられた画像を小さな正方形のパッチに分割します。例えば、画像を16x16ピクセルのパッチに分割する場合、224x224ピクセルの画像は、14x14のグリッドに分割され、合計196個のパッチが生成されます。これらのパッチはそれぞれ、自然言語処理における **単語** のように扱われます。

各パッチは、まず固定長のベクトルに埋め込まれます。パッチの生のピクセル値から抽象化された特徴を捉えるためのものです。

#### 位置情報の追加

Transformerは入力シーケンスの要素間の順序情報を自然には捉えられないため、パッチの相対的または絶対的な位置情報をモデルに組み込む必要があります。これを実現するために、各パッチの埋め込みにポジションエンコーディングが加えられます。このエンコーディングは、パッチが画像内でどの位置にあるかを示し、Transformerにシーケンスの順序情報を提供します。

#### Self-Attention の適用

パッチが埋め込まれ、位置情報が加えられた後、Transformer の Self-Attention 層は、各パッチが他のパッチとどのように関連しているかを学習します。これにより、画像全体のコンテキスト内で各パッチの重要性が評価されます。Self-Attentionメカニズムにより、モデルは画像の局所的な特徴だけでなく、画像全体のコンテキストを考慮した特徴を学習できます。すなわち、遠く離れたパッチ間の関連性も捉えられます。

#### Transformerエンコーダ

ViTは複数のTransformerエンコーダ層を通じて、パッチの特徴をさらに処理します。各層は、パッチ間の関係を深く学習し、より抽象的な特徴を抽出します。最終層のあとで、モデルは画像全体を表す特徴ベクトルを生成します。この特徴ベクトルは画像分類などに使用できます。

#### 行いたい処理

例えば画像分類に用いる場合、Transformerモデルの出力に分類ヘッドを追加し、特定のクラスに画像を分類します。

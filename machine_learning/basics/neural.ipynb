{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoxsyA//RcGjfvabXQATPP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/basics/neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 機械学習体験プログラム\n",
        "\n",
        "機械学習の主要な要素を体験しながら、ニューラルネットワークの仕組みや使い方を理解しましょう。\n",
        "\n",
        "## 必要なライブラリ\n",
        "\n",
        "以下のライブラリをインポートします：\n",
        "- `numpy`: 数値計算を効率的に行うためのライブラリ\n",
        "- `scikit-learn`: 機械学習のためのライブラリ。分類、回帰、クラスタリングなど様々な機械学習アルゴリズムを提供します。\n",
        "- `matplotlib`: データの可視化のためのライブラリ。グラフや図を簡単に描画できます。\n",
        "- `pandas`: データ操作と分析のためのライブラリ。"
      ],
      "metadata": {
        "id": "GBGv21QTKGZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXEFK3EIKFwY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.utils import shuffle\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "random_state = 42  # 乱数のシード"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットの選択\n",
        "\n",
        "いくつかの有名なデータセットから選択できます。各データセットの概要は以下の通りです：\n",
        "\n",
        "| データセット名 | 目的 | 特徴量数 | 各特徴量 |\n",
        "| --- | --- | --- | --- |\n",
        "| Iris | 3種類のアヤメの品種を分類 | 4 | ガクの長さ・幅、花弁の長さ・幅 |\n",
        "| Wine | 3種類のワインのタイプを分類 | 13 | アルコール度数、マグネシウム含有量など |\n",
        "| Breast Cancer | 良性・悪性の腫瘍を分類 | 30 | 半径、テクスチャ、周囲長、面積など |\n",
        "以下のコードを実行して、使用するデータセットを選択します。"
      ],
      "metadata": {
        "id": "GqvTZYiVKWl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"データセットを選択してください：\")\n",
        "print(\"1: Iris\")\n",
        "print(\"2: Wine\")\n",
        "print(\"3: Breast Cancer\")\n",
        "\n",
        "dataset_number = int(input(\"番号を入力してください: \"))\n",
        "\n",
        "# データセットをロードする関数\n",
        "def load_dataset(dataset_number):\n",
        "    if dataset_number == 1:\n",
        "        dataset = load_iris()\n",
        "        dataset_name = \"Iris\"\n",
        "        feature_names = [\"ガクの長さ\", \"ガクの幅\", \"花弁の長さ\", \"花弁の幅\"]\n",
        "    elif dataset_number == 2:\n",
        "        dataset = load_wine()\n",
        "        dataset_name = \"Wine\"\n",
        "        feature_names = dataset.feature_names\n",
        "    elif dataset_number == 3:\n",
        "        dataset = load_breast_cancer()\n",
        "        dataset_name = \"Breast Cancer\"\n",
        "        feature_names = dataset.feature_names\n",
        "    else:\n",
        "        dataset = load_iris()\n",
        "        dataset_name = \"Iris\"\n",
        "        feature_names = [\"ガクの長さ\", \"ガクの幅\", \"花弁の長さ\", \"花弁の幅\"]\n",
        "\n",
        "    X = dataset.data\n",
        "    y = dataset.target\n",
        "    return dataset.data, dataset.target, pd.DataFrame(X, columns=feature_names), pd.Series(y, name=\"CLASS\"), dataset_name\n",
        "\n",
        "# データセットのロード\n",
        "X, y, feature_df, y_df, dataset_name = load_dataset(dataset_number)\n",
        "\n",
        "# 特徴量とクラスを結合したDataFrameを作成\n",
        "df_combined = pd.concat([y_df, feature_df], axis=1)\n",
        "\n",
        "# データをシャッフル\n",
        "df_shuffled = shuffle(df_combined, random_state=42).reset_index(drop=True)\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import data_table\n",
        "    shown_table = data_table.DataTable(df_shuffled.head())\n",
        "else:\n",
        "    shown_table = df_shuffled.head().to_string(index=False)\n",
        "shown_table"
      ],
      "metadata": {
        "id": "iZ1TW4sRKTEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題 1**\n",
        "- ひとつのサンプル（花，ワイン，患者）は行ですか，列ですか？\n",
        "- この表では，各サンプルの\n",
        " - クラスはどこですか？\n",
        " - １つのサンプルごとに何次元の特徴量がありますか？\n",
        "- ここからの機械学習モデルでは，＿＿＿＿＿＿＿ をもとに，そのデータの＿＿＿＿＿＿＿を推定します．"
      ],
      "metadata": {
        "id": "PeZkpNFxapUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## データの準備と可視化\n",
        "\n",
        "### データの構造を理解するための可視化\n",
        "\n",
        "機械学習の第一歩として、データを理解することが重要です。まず、データセットの特徴量のうち2つを選んで散布図を描き、データの分布とクラスごとの関係を視覚化します。\n"
      ],
      "metadata": {
        "id": "tKnOnqahKpGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 任意のクラス数に対応するカラーパレットを生成する関数\n",
        "def generate_color_palette(n_colors):\n",
        "    return plt.cm.get_cmap('tab10', n_colors)\n",
        "\n",
        "# 特徴量のうち、2つを選んで散布図を描く関数\n",
        "def plot_2d_projection(X, y, feature_indices, title=\"\"):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    palette = generate_color_palette(len(np.unique(y)))\n",
        "    for target in np.unique(y):\n",
        "        subset = X[y == target]\n",
        "        plt.scatter(subset[:, feature_indices[0]], subset[:, feature_indices[1]], label=target, color=palette(target))\n",
        "    plt.xlabel(f\"Feature {feature_indices[0]}\")\n",
        "    plt.ylabel(f\"Feature {feature_indices[1]}\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 特徴量の組み合わせを選んでプロット\n",
        "plot_2d_projection(X.values if dataset_name == \"Titanic\" else X, y, feature_indices=[0, 1], title=f\"{dataset_name} Data - Features 0 and 1\")\n"
      ],
      "metadata": {
        "id": "SgJ_PnnmKrzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### データの分割\n",
        "\n",
        "データセット全体を1つの塊として使用するのではなく、以下の3つの部分に分割します：\n",
        "\n",
        "1. **教師データ (training data)**:\n",
        "   モデルの学習に使用します。このデータを使用して、モデルは特徴量と目標変数（ラベル）の関係を学びます。\n",
        "\n",
        "2. **検証データ (validation data)**:\n",
        "   モデルの性能を評価し、ハイパーパラメータの調整に使用します。教師データに含まれないデータを使うことで、モデルの汎化性能を確認します。\n",
        "\n",
        "3. **テストデータ (test data)**:\n",
        "   最終的なモデルの性能を評価するために使用します。このデータも教師データには含まれていません。テストデータの性能が高ければ、モデルは未知のデータに対しても良好に動作することが期待されます。\n"
      ],
      "metadata": {
        "id": "_oBLnuEcK2CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hotエンコーディング\n",
        "encoder = OneHotEncoder()\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "\n",
        "# データを分割 (70% training, 15% validation, 15% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y_onehot, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 分割結果の確認\n",
        "print(\"教師データのサイズ:\", X_train.shape)\n",
        "print(\"検証データのサイズ:\", X_val.shape)\n",
        "print(\"テストデータのサイズ:\", X_test.shape)"
      ],
      "metadata": {
        "id": "rhaAbZgLK1Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題２**\n",
        "すべてのデータを３個に分割しました．それぞれ何に使われますか？\n",
        "\n",
        "- 教師データ　：\n",
        "- 検証データ　：\n",
        "- テストデータ：\n"
      ],
      "metadata": {
        "id": "pRcMbvK1tifq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ニューラルネットワークによるモデルの構築とトレーニング\n",
        "\n",
        "次に、ニューラルネットワークを使用してモデルを構築し、トレーニングを行います。ここでは、scikit-learnの`MLPClassifier`を使用します。\n",
        "\n",
        "### ニューラルネットワークの概要\n",
        "\n",
        "ニューラルネットワークは、以下のような主要な要素で構成されます：\n",
        "\n",
        "1. **入力層 (Input Layer)**:\n",
        "   特徴量を入力する層です。特徴量の数だけノードがあります。\n",
        "\n",
        "2. **隠れ層 (Hidden Layer)**:\n",
        "   入力層と出力層の間にある層です。ニューラルネットワークの学習能力を高めるために使用されます。隠れ層の数や各層のノード数はハイパーパラメータです。\n",
        "\n",
        "3. **出力層 (Output Layer)**:\n",
        "   クラスの数だけノードがあります。ここでは、各ノードが特定のクラスに対応します。\n",
        "\n",
        "### 特徴量の次元数の選択\n",
        "\n",
        "データをそのまま使用する方法と、plot_2d_projectionで選んだ2次元のみを使用する方法のいずれかで学習を行います。\n"
      ],
      "metadata": {
        "id": "Oh3lkD5jK951"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2次元の特徴量を選択する関数\n",
        "def select_two_features(X, feature_indices):\n",
        "    return X[:, feature_indices]\n",
        "\n",
        "# 特徴量の次元数を選択するかどうか\n",
        "use_two_features = True\n",
        "feature_indices = [0, 1]\n",
        "\n",
        "if use_two_features:\n",
        "    X_train_transformed = select_two_features(X_train, feature_indices)\n",
        "    X_val_transformed = select_two_features(X_val, feature_indices)\n",
        "    X_test_transformed = select_two_features(X_test, feature_indices)\n",
        "else:\n",
        "    X_train_transformed = X_train\n",
        "    X_val_transformed = X_val\n",
        "    X_test_transformed = X_test"
      ],
      "metadata": {
        "id": "TbJcHsV-LPDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルのトレーニング\n",
        "\n",
        "次に、モデルをトレーニングします。ここでは、隠れ層が1つでノード数が100のシンプルなニューラルネットワークを使用します。\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MKuaKXf1LOdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 初期設定\n",
        "hidden_layer_sizes = (100,) # 隠れ層の数\n",
        "max_epochs = 500          # 最大エポック数\n",
        "\n",
        "# モデルの構築\n",
        "mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=1, warm_start=True, random_state=random_state)\n",
        "\n",
        "# トレーニングループ\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# tqdmを使って進行状況バーを表示\n",
        "with tqdm(total=max_epochs, leave=False) as pbar:\n",
        "    for epoch in range(max_epochs):\n",
        "        mlp.fit(X_train_transformed, y_train)\n",
        "\n",
        "        # 教師データでの予測と評価\n",
        "        y_train_pred = mlp.predict(X_train_transformed)\n",
        "        y_train_pred_proba = mlp.predict_proba(X_train_transformed)\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        train_loss = log_loss(y_train, y_train_pred_proba)\n",
        "\n",
        "        # 検証データでの予測と評価\n",
        "        y_val_pred = mlp.predict(X_val_transformed)\n",
        "        y_val_pred_proba = mlp.predict_proba(X_val_transformed)\n",
        "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "        val_loss = log_loss(y_val, y_val_pred_proba)\n",
        "\n",
        "        # 記録\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # tqdmの説明文を更新\n",
        "        pbar.set_description(f\"Epoch {epoch + 1}/{max_epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Train acc: {train_accuracy:.4f}, Val acc: {val_accuracy:.4f}\")\n",
        "        pbar.update(1)\n",
        "\n",
        "# 最後のエポックの結果を表示\n",
        "print(f\"Epoch {max_epochs}/{max_epochs} - Train accuracy: {train_accuracy:.4f}, Train loss: {train_loss:.4f}, Val accuracy: {val_accuracy:.4f}, Val loss: {val_loss:.4f}\")\n",
        "\n",
        "# プロット用の間引き\n",
        "def reduce_points(data, num_points):\n",
        "    if len(data) > num_points:\n",
        "        indices = np.linspace(0, len(data) - 1, num_points, dtype=int)\n",
        "        return [data[i] for i in indices]\n",
        "    return data\n",
        "\n",
        "# グラフで学習の様子を表示\n",
        "\n",
        "num_points = 100\n",
        "epochs = range(1, max_epochs + 1)\n",
        "epochs_reduced = reduce_points(list(epochs), num_points)\n",
        "train_accuracies_reduced = reduce_points(train_accuracies, num_points)\n",
        "val_accuracies_reduced = reduce_points(val_accuracies, num_points)\n",
        "train_losses_reduced = reduce_points(train_losses, num_points)\n",
        "val_losses_reduced = reduce_points(val_losses, num_points)\n",
        "\n",
        "# プロット\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_reduced, train_accuracies_reduced, label='Train Accuracy')\n",
        "plt.plot(epochs_reduced, val_accuracies_reduced, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.title('Accuracy per Epoch')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_reduced, train_losses_reduced, label='Train Loss')\n",
        "plt.plot(epochs_reduced, val_losses_reduced, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0, max(max(train_losses_reduced), max(val_losses_reduced)))\n",
        "plt.legend()\n",
        "plt.title('Loss per Epoch')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "siueLWKXLXsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題３**\n",
        "\n",
        "- 学習では ＿＿＿＿＿＿ の値を小さくすることを目指します．\n",
        " - ある入力サンプルから得られた仮の出力（各クラスに属する＿＿＿＿）と，正解の ＿＿＿＿＿＿ 表現を比較して，その値が少しだけ小さくなるようにネットワークを更新します．この比較に用いる関数は ＿＿＿＿＿関数とよばれます．\n",
        "\n",
        "- 左のグラフは＿＿＿＿＿＿，右のグラフは＿＿＿＿＿＿です．\n",
        " - 学習で小さくしようとしているのは右ですか，左ですか？\n",
        "\n",
        "- エポックとは？ 今回は何エポックの学習が行われましたか？\n"
      ],
      "metadata": {
        "id": "SktsNjzTxogs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 教師データや検証データで試す"
      ],
      "metadata": {
        "id": "lf1lMnjgWKaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 教師データでの予測\n",
        "y_train_pred = mlp.predict(X_train_transformed)\n",
        "y_train_pred_proba = mlp.predict_proba(X_train_transformed)\n",
        "\n",
        "# 教師データでの評価\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_loss = log_loss(y_train, y_train_pred_proba)\n",
        "\n",
        "print(f\"教師データでの精度: {train_accuracy:.4f}\")\n",
        "print(f\"教師データでの損失: {train_loss:.4f}\")\n",
        "\n",
        "# 検証データでの予測\n",
        "y_val_pred = mlp.predict(X_val_transformed)\n",
        "y_val_pred_proba = mlp.predict_proba(X_val_transformed)\n",
        "\n",
        "# 検証データでの評価\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_loss = log_loss(y_val, y_val_pred_proba)\n",
        "\n",
        "print(f\"検証データでの精度: {val_accuracy:.4f}\")\n",
        "print(f\"検証データでの損失: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "j0wzWMm8OgwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 教師データを１個入力して試す\n",
        "\n",
        "作成されたネットワークがどのように動作するのか、教師データを１個用いて確認してみます。\n",
        "\n",
        "- **ネットワークへの入力（特徴量）**\n",
        "\n",
        "    教師データから特定のサンプルを選び、その特徴量ベクトルを表示します。これは、ネットワーク（モデル）に入力されるデータで、各特徴量が数値として表現されています。\n",
        "\n",
        "- **ネットワークの出力（確率）**\n",
        "\n",
        "    選択した入力に対して、モデルが各クラスに属する確率を計算します。この確率は、入力がそれぞれのクラスに分類される可能性を示しており、各クラスに対して1つの確率値が出力されます。\n",
        "\n",
        "- **推定されたクラス番号の one-hot 表現**\n",
        "\n",
        "    モデルが予測したクラスのone-hot表現を表示します。one-hot表現とは、正解クラスを1、それ以外のクラスを0とするベクトル形式のことです。例えば4クラスの場合、[0, 0, 1, 0]はクラス3を示します。\n",
        "\n",
        "- **クラス推定の表示（整数表現）**\n",
        "\n",
        "    one-hot表現のベクトルを整数に変換し、予測されたクラスを数値として示します。"
      ],
      "metadata": {
        "id": "28kqsh6QWpxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 特徴量ベクトルを入力し、各クラスの確率を出力\n",
        "sample_index = 1  # 任意のサンプルインデックス\n",
        "sample_input = X_train_transformed[sample_index].reshape(1, -1)  # 1サンプルの入力データ\n",
        "print(\"ネットワークへの入力（特徴量）　　 ：\", sample_input)\n",
        "\n",
        "# 確率の出力\n",
        "class_probabilities = mlp.predict_proba(sample_input)\n",
        "print(\"ネットワークの出力（確率）　　 　　：\", class_probabilities)\n",
        "\n",
        "# クラス推定\n",
        "predicted_class = mlp.predict(sample_input)\n",
        "print(\"推定されたクラス番号の one-hot 表現：\", predicted_class)\n",
        "print(\"推定されたクラス番号　　　　　　　 ：\", np.argmax(predicted_class))\n",
        "\n",
        "\n",
        "# クラス推定\n",
        "predicted_class = mlp.predict(sample_input)\n",
        "print(\"正解の one-hot 表現　　　　　　　　：\", y_train[sample_index])\n",
        "print(\"正解　　　　　　　　　　　　　　　 ：\", np.argmax(y_train[sample_index]))"
      ],
      "metadata": {
        "id": "wo3wEk2AWeSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題４**\n",
        "\n",
        "- 出力を読み取りながら埋めてください：\n",
        "\n",
        "  あるサンプルの ＿＿＿＿＿＿＿ をネットワークに入力したところ，＿＿＿＿＿＿ が出力された．\n",
        "\n",
        "  出力は，そのサンプルが各クラスに属する確率を表すものである．その中では ＿＿＿＿ 番目のクラスが最大となったことから，推定されたクラスは ＿＿＿＿ となった．\n",
        "\n",
        "- この結果をもとに，更に学習をするとしたら…\n",
        "\n",
        "＿＿＿＿＿関数を用いて ＿＿＿＿＿＿＿ と ＿＿＿＿＿＿ を比較し，その関数が小さくなるようにネットワークを修正する．その修正の処理を ＿＿＿＿＿ という[（参考）](https://hogetech.info/ml/dl/backpropagation)．"
      ],
      "metadata": {
        "id": "Vj9yir4s0OzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2次元プロットによるモデルの可視化\n",
        "\n",
        "2次元のみで学習を行った場合、得られたモデルの分類領域を視覚化します。大量のサンプル点を生成し、その点に対するモデルの予測結果をプロットします。"
      ],
      "metadata": {
        "id": "BGvykzFKLlO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# グリッドサーチによる領域プロット\n",
        "def plot_decision_boundary(clf, X, y, feature_indices, title=\"\"):\n",
        "    # グリッドを生成\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "    # グリッド全体で予測を実行\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = np.argmax(Z, axis=1)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # プロットの描画\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    palette = plt.cm.get_cmap('tab10', len(np.unique(np.argmax(y, axis=1))))\n",
        "    for target in np.unique(np.argmax(y, axis=1)):\n",
        "        subset = X[np.argmax(y, axis=1) == target]\n",
        "        plt.scatter(subset[:, feature_indices[0]], subset[:, feature_indices[1]], label=target, color=palette(target))\n",
        "    plt.xlabel(f\"Feature {feature_indices[0]}\")\n",
        "    plt.ylabel(f\"Feature {feature_indices[1]}\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 特徴量の組み合わせを選んでプロット\n",
        "if use_two_features == True:\n",
        "  plot_decision_boundary(mlp, X_train_transformed, y_train, feature_indices, title=f\"{dataset_name} Decision Boundary\")"
      ],
      "metadata": {
        "id": "y-aXjWVILkSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題５**\n",
        "\n",
        "- クラスを分ける境界のことを ＿＿＿＿＿＿＿＿ という．\n",
        " - これを適切な位置に決めることが学習であるともいえる．\n",
        " - 各クラスが概ね良好に分離できていれば学習は成功\n",
        " - あまりにも細かくクネクネと分離している状況は ＿＿＿＿＿ とされる．教師データでは高い精度を得られるが，テストデータにはむしろ精度が下がってしまう．学習のしすぎを防ぐために，＿＿＿データを用いてハイパーパラメータを調節したり，ちょうどよいタイミングでで学習を停止したりする工夫が行われる．"
      ],
      "metadata": {
        "id": "u0JmbxqH2LzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テストデータでの評価\n",
        "\n",
        "モデルの最終評価として、テストデータを使用します。"
      ],
      "metadata": {
        "id": "TTAZIzrKLeAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# テストデータでの予測\n",
        "y_test_pred = mlp.predict(X_test_transformed)\n",
        "y_test_pred_proba = mlp.predict_proba(X_test_transformed)\n",
        "\n",
        "# テストデータでの評価\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_loss = log_loss(y_test, y_test_pred_proba)\n",
        "\n",
        "print(f\"テストデータでの精度: {test_accuracy:.4f}\")\n",
        "print(f\"テストデータでの損失: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "DNlCHrSHLdjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題６**\n",
        "\n",
        "学んだことをもとに，ご自身の言葉でまとめてください．\n",
        "\n",
        "- データセット，クラス，特徴量\n",
        "- ニューラルネットワークの入力と出力\n",
        "- 学習とは，何を用いて何をするもの？\n",
        "- 学習されたモデルをどうやって使うと，新たなサンプルのクラスを推定できる？\n"
      ],
      "metadata": {
        "id": "pc6diR8k3rz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 発展\n",
        "\n",
        "今回，１サンプルの特徴量は，多くても 30 次元でした．\n",
        "\n",
        "しかし，たとえば画像を判別するなら，とても小さな画像でも 縦 32 × 横 32 × RGB 3チャンネル ＝ 3072 次元くらいはあります．この場合，画像を特徴量に変換（**特徴抽出**）した上で，判別に使うことになります．\n",
        "\n",
        "この特徴抽出をネットワーク内部で行ってくれるものとして有名なのが **畳み込み層**（Convolutional Layer）です．また，それを含むニューラルネットワークが畳み込みニューラルネットワーク（Convolutional Neural Network; **CNN**）です．\n",
        "\n",
        "[CNN を用いた画像分類の演習に進みましょう．](https://github.com/shizoda/education/blob/main/machine_learning/cnn/cifar10_pytorch.ipynb)"
      ],
      "metadata": {
        "id": "u9SgYRGm-yZ5"
      }
    }
  ]
}
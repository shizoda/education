{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6vZkXST2Bu9XzQps65kbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/basics/neural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 機械学習体験プログラム\n",
        "\n",
        "基本的な分類問題を用いて機械学習を体験しつつ、ニューラルネットワークの仕組みや使い方を理解しましょう。\n",
        "\n",
        "<a title=\"Dake, Mysid, CC BY 1.0 &lt;https://creativecommons.org/licenses/by/1.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Neural_network.svg\"><img width=\"512\" alt=\"Neural network\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/512px-Neural_network.svg.png?20210102213114\"></a>\n",
        "\n",
        "## 必要なライブラリ\n",
        "\n",
        "以下のライブラリをインポートします：\n",
        "- `numpy`: 数値計算を効率的に行うためのライブラリ\n",
        "- `scikit-learn`: 機械学習のためのライブラリ。分類、回帰、クラスタリングなど様々な機械学習アルゴリズムを提供します。\n",
        "- `matplotlib`: データの可視化のためのライブラリ。グラフや図を簡単に描画できます。\n",
        "- `pandas`: データ操作と分析のためのライブラリ。"
      ],
      "metadata": {
        "id": "GBGv21QTKGZe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXEFK3EIKFwY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.utils import shuffle\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "random_state = 42  # 乱数のシード"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセットの選択\n",
        "\n",
        "いくつかの有名なデータセットから選択できます。各データセットの概要は以下の通りです：\n",
        "\n",
        "| データセット名 | 目的 | 特徴量数 | 各特徴量 |\n",
        "| --- | --- | --- | --- |\n",
        "| Iris | 3種類のアヤメの品種を分類 | 4 | ガクの長さ・幅、花弁の長さ・幅 |\n",
        "| Wine | 3種類のワインのタイプを分類 | 13 | アルコール度数、マグネシウム含有量など |\n",
        "| Breast Cancer | 良性・悪性の腫瘍を分類 | 30 | 半径、テクスチャ、周囲長、面積など |\n",
        "以下のコードを実行して、使用するデータセットを選択します。"
      ],
      "metadata": {
        "id": "GqvTZYiVKWl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"データセットを選択してください：\")\n",
        "print(\"1: Iris\")\n",
        "print(\"2: Wine\")\n",
        "print(\"3: Breast Cancer\")\n",
        "\n",
        "dataset_number_str = input(\"番号を入力してください: \")\n",
        "\n",
        "# 全角数字を半角に変換\n",
        "dataset_number_str = dataset_number_str.translate(str.maketrans('１２３', '123'))\n",
        "\n",
        "try:\n",
        "    dataset_number = int(dataset_number_str)\n",
        "except ValueError:\n",
        "    dataset_number = None\n",
        "\n",
        "from sklearn.datasets import load_iris, load_wine, load_breast_cancer\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import sys\n",
        "from IPython.display import display\n",
        "\n",
        "# データセットをロードする関数\n",
        "def load_dataset(dataset_number):\n",
        "    if dataset_number == 1:\n",
        "        dataset = load_iris()\n",
        "        dataset_name = \"Iris\"\n",
        "        feature_names = dataset.feature_names\n",
        "        # feature_names = [\"ガクの長さ\", \"ガクの幅\", \"花弁の長さ\", \"花弁の幅\"]\n",
        "        target_names = dataset.target_names\n",
        "    elif dataset_number == 2:\n",
        "        dataset = load_wine()\n",
        "        dataset_name = \"Wine\"\n",
        "        feature_names = dataset.feature_names\n",
        "        target_names = dataset.target_names\n",
        "    elif dataset_number == 3:\n",
        "        dataset = load_breast_cancer()\n",
        "        dataset_name = \"Breast Cancer\"\n",
        "        feature_names = dataset.feature_names\n",
        "        # target_names = dataset.target_names\n",
        "        target_names = [\"Malignant (bad)\", \"Benign (good)\"]\n",
        "    else:\n",
        "        print(\"誤った番号です\")\n",
        "        return None, None, None, None, None, None\n",
        "\n",
        "    X = dataset.data\n",
        "    y = dataset.target\n",
        "    return dataset.data, dataset.target, pd.DataFrame(X, columns=feature_names), pd.Series(y, name=\"CLASS\"), dataset_name, target_names, feature_names\n",
        "\n",
        "# データセットのロード\n",
        "X, y, feature_df, y_df, dataset_name, target_names, feature_names = load_dataset(dataset_number)\n",
        "\n",
        "if X is not None:\n",
        "    # クラス名を追加\n",
        "    y_class_names = y_df.map(lambda x: target_names[x]).rename(\"CLASS_NAME\")\n",
        "\n",
        "    # 特徴量とクラス、クラス名を結合したDataFrameを作成\n",
        "    df_combined = pd.concat([y_df, y_class_names, feature_df], axis=1)\n",
        "\n",
        "    # データをシャッフル\n",
        "    df_shuffled = shuffle(df_combined, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # 表示\n",
        "    display(df_shuffled.head())"
      ],
      "metadata": {
        "id": "iZ1TW4sRKTEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題 1**\n",
        "- ひとつのサンプル（花，ワイン，患者）は行ですか，列ですか？\n",
        "- この表では，各サンプルの\n",
        " - クラスはどこですか？\n",
        " - １つのサンプルごとに何次元の特徴量がありますか？\n",
        "- 今回の機械学習モデルでは、各サンプルがもつ ＿＿＿＿＿＿＿ に基づいて、そのデータが各クラスに属する ＿＿＿＿ をそれぞれ計算し、その値が最大となるクラスに分類します。"
      ],
      "metadata": {
        "id": "PeZkpNFxapUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## データの準備と可視化\n",
        "\n",
        "### データの構造を理解するための可視化\n",
        "\n",
        "機械学習の第一歩として、データを理解することが重要です。\n",
        "\n",
        "まず、データセットの特徴量のうち2つを選んで散布図を描き、データの分布とクラスごとの関係を視覚化します。\n"
      ],
      "metadata": {
        "id": "tKnOnqahKpGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 任意のクラス数に対応するカラーパレットを生成する関数\n",
        "def generate_color_palette(n_colors):\n",
        "    return plt.cm.get_cmap('tab10', n_colors)\n",
        "\n",
        "# 特徴量のうち、2つを選んで散布図を描く関数\n",
        "def plot_2d_projection(X, y, feature_indices, feature_names, target_names, title=\"\"):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    palette = generate_color_palette(len(np.unique(y)))\n",
        "    for target in np.unique(y):\n",
        "        subset = X[y == target]\n",
        "        plt.scatter(subset[:, feature_indices[0]], subset[:, feature_indices[1]], label=f\"Class {target}: {target_names[target]}\", color=palette(target))\n",
        "    plt.xlabel(f\"Feature {feature_indices[0]}: {feature_names[0]}\")\n",
        "    plt.ylabel(f\"Feature {feature_indices[1]}: {feature_names[1]}\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 特徴量の組み合わせを選んでプロット\n",
        "if X is not None:\n",
        "    plot_2d_projection(X, y, feature_indices=[0, 1], feature_names=feature_names, target_names=target_names, title=f\"{dataset_name} Data - Features 0 and 1\")"
      ],
      "metadata": {
        "id": "SgJ_PnnmKrzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### データの分割\n",
        "\n",
        "データセット全体を1つの塊として使用するのではなく、以下の3つの部分に分割します：\n",
        "\n",
        "1. **教師データ (training data)**:\n",
        "   モデルの学習に使用します。このデータを使用して、モデルは特徴量と目標変数（ラベル）の関係を学びます。\n",
        "\n",
        "2. **検証データ (validation data)**:\n",
        "   モデルの性能を評価し、ハイパーパラメータの調整に使用します。教師データに含まれないデータを使うことで、モデルの汎化性能を確認します。\n",
        "\n",
        "3. **テストデータ (test data)**:\n",
        "   最終的なモデルの性能を評価するために使用します。このデータも教師データには含まれていません。テストデータの性能が高ければ、モデルは未知のデータに対しても良好に動作することが期待されます。\n"
      ],
      "metadata": {
        "id": "_oBLnuEcK2CD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hotエンコーディング\n",
        "encoder = OneHotEncoder()\n",
        "y_onehot = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "\n",
        "# データを分割 (70% training, 15% validation, 15% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y_onehot, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 分割結果の確認\n",
        "\n",
        "# DataFrameの作成\n",
        "def create_df(X, y, feature_names, target_names):\n",
        "    y_classes = [target_names[i] for i in y.argmax(axis=1)]\n",
        "    y_class_series = pd.Series(y_classes, name=\"CLASS_NAME\")\n",
        "    return pd.concat([pd.DataFrame(X, columns=feature_names), y_class_series], axis=1)\n",
        "\n",
        "df_train = create_df(X_train, y_train, feature_names, target_names)\n",
        "df_val = create_df(X_val, y_val, feature_names, target_names)\n",
        "df_test = create_df(X_test, y_test, feature_names, target_names)\n",
        "\n",
        "# 表示\n",
        "print(\"\\n[教師データ]\")\n",
        "print(\"サイズ:\", X_train.shape)\n",
        "display(df_train.head())\n",
        "\n",
        "print(\"\\n[検証データ]\")\n",
        "print(\"サイズ:\", X_val.shape)\n",
        "display(df_val.head())\n",
        "\n",
        "print(\"\\n[テストデータ]\")\n",
        "print(\"サイズ:\", X_test.shape)\n",
        "display(df_test.head())"
      ],
      "metadata": {
        "id": "rhaAbZgLK1Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題２**\n",
        "すべてのデータを３個に分割しました．それぞれ何に使われますか？\n",
        "\n",
        "- 教師データ　：\n",
        "- 検証データ　：\n",
        "- テストデータ：\n"
      ],
      "metadata": {
        "id": "pRcMbvK1tifq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ニューラルネットワークによるモデルの構築とトレーニング\n",
        "\n",
        "次に、ニューラルネットワークを使用してモデルを構築し、トレーニングを行います。ここでは、scikit-learnの`MLPClassifier`を使用します。\n",
        "\n",
        "### ニューラルネットワークの概要\n",
        "\n",
        "ニューラルネットワークは、以下のような主要な要素で構成されます：\n",
        "\n",
        "1. **入力層 (Input Layer)**:\n",
        "   特徴量を入力する層です。特徴量の数だけノードがあります。\n",
        "\n",
        "2. **隠れ層 (Hidden Layer)**:\n",
        "   入力層と出力層の間にある層です。ニューラルネットワークの学習能力を高めるために使用されます。隠れ層の数や各層のノード数はハイパーパラメータです。\n",
        "\n",
        "3. **出力層 (Output Layer)**:\n",
        "   クラスの数だけノードがあります。ここでは、各ノードが特定のクラスに対応します。\n",
        "\n",
        "### 特徴量の次元数の選択\n",
        "\n",
        "データをそのまま使用する方法と、plot_2d_projectionで選んだ2次元のみを使用する方法のいずれかで学習を行います。\n",
        "\n",
        "最初は視覚的な理解のために、2個の特徴量のみを用います。<br>\n",
        "精度を上げることを目指すには3個以上の特徴量を使うべきですので、`use_two_features = False` に変更してください。その場合は特徴空間や決定境界を可視化できなくなるのでご注意ください。"
      ],
      "metadata": {
        "id": "Oh3lkD5jK951"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2次元の特徴量を選択する関数\n",
        "def select_two_features(X, feature_indices):\n",
        "    return X[:, feature_indices]\n",
        "\n",
        "# 特徴量の次元数を選択するかどうか\n",
        "use_two_features = True\n",
        "feature_indices = [0, 1]\n",
        "\n",
        "if use_two_features:\n",
        "    X_train_transformed = select_two_features(X_train, feature_indices)\n",
        "    X_val_transformed = select_two_features(X_val, feature_indices)\n",
        "    X_test_transformed = select_two_features(X_test, feature_indices)\n",
        "    clms = feature_df.columns[0:2]\n",
        "else:\n",
        "    X_train_transformed = X_train\n",
        "    X_val_transformed = X_val\n",
        "    X_test_transformed = X_test\n",
        "    clms = feature_df.columns"
      ],
      "metadata": {
        "id": "TbJcHsV-LPDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルのトレーニング\n",
        "\n",
        "まずモデルを定義します。\n",
        "\n",
        "隠れ層（中間層）の数やノード数を調節してください。<br>\n",
        "複雑にすればするほど難しい問題が解けるようになる反面、教師データが少ない場合に過学習が起きやすくなったり、計算時間が増えたりします。"
      ],
      "metadata": {
        "id": "MKuaKXf1LOdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_sizes = [5, ]  # 隠れ層のノード数。複数の層にしたい場合は [3,4] や [3,5,4] など\n",
        "batch_size = 32            # バッチサイズ（ミニバッチサイズ）。１回の学習に用いられるサンプルの数\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "def draw_mlp_network(input_nodes, hidden_layers, output_nodes):\n",
        "    \"\"\"\n",
        "    MLPのネットワークを描画する関数\n",
        "\n",
        "    Parameters:\n",
        "    - input_nodes: 入力層のノード数\n",
        "    - hidden_layers: 隠れ層のノード数のリスト\n",
        "    - output_nodes: 出力層のノード数\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # ノードの追加\n",
        "    layers = [input_nodes] + hidden_layers + [output_nodes]\n",
        "    layer_pos = []\n",
        "    max_nodes = max(layers)\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        # 中央揃えにするためのオフセット計算\n",
        "        offset = (max_nodes - layer) / 2\n",
        "        current_layer = []\n",
        "        for j in range(layer):\n",
        "            if i == 0:\n",
        "                node_id = f'in_{j}'\n",
        "            elif i == len(layers) - 1:\n",
        "                node_id = f'out_{j}'\n",
        "            else:\n",
        "                node_id = f'hidden{i-1}_{j}'\n",
        "            G.add_node(node_id, pos=(i, max_nodes - (j + offset)))\n",
        "            current_layer.append(node_id)\n",
        "        layer_pos.append(current_layer)\n",
        "\n",
        "    # エッジの追加\n",
        "    for i in range(len(layers) - 1):\n",
        "        for node_start in layer_pos[i]:\n",
        "            for node_end in layer_pos[i + 1]:\n",
        "                G.add_edge(node_start, node_end)\n",
        "\n",
        "    # ノードの位置を取得\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "\n",
        "    # グラフの描画\n",
        "    plt.figure(figsize=(8, 6))  # 横幅を800pxに設定するための調整\n",
        "    nx.draw(G, pos, with_labels=True, node_size=1500, node_color=\"lightblue\", font_size=8)\n",
        "    plt.title('MLP Network Structure')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# モデルの構築\n",
        "mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=1, warm_start=True, random_state=random_state, batch_size=batch_size)\n",
        "\n",
        "# 構造の描画\n",
        "draw_mlp_network(input_nodes=X_train_transformed.shape[1], hidden_layers=hidden_layer_sizes, output_nodes=len(target_names))"
      ],
      "metadata": {
        "id": "ZjqbyRKfwDsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルを定義できたら学習を行いましょう。\n",
        "最大エポック数を調節してください。\n",
        "\n",
        "**ネットワーク構造を書き換えた場合、上のセルを必ず実行して、反映させてから学習してください。**"
      ],
      "metadata": {
        "id": "NSZ7Q5HvxnbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_epochs = 1000           # 最大エポック数\n",
        "\n",
        "# トレーニングループ\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# tqdmを使って進行状況バーを表示\n",
        "with tqdm(total=max_epochs, leave=False) as pbar:\n",
        "    for epoch in range(max_epochs):\n",
        "        mlp.fit(X_train_transformed, y_train)\n",
        "\n",
        "        # 教師データでの予測と評価\n",
        "        y_train_pred = mlp.predict(X_train_transformed)\n",
        "        y_train_pred_proba = mlp.predict_proba(X_train_transformed)\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        train_loss = log_loss(y_train, y_train_pred_proba)\n",
        "\n",
        "        # 検証データでの予測と評価\n",
        "        y_val_pred = mlp.predict(X_val_transformed)\n",
        "        y_val_pred_proba = mlp.predict_proba(X_val_transformed)\n",
        "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "        val_loss = log_loss(y_val, y_val_pred_proba)\n",
        "\n",
        "        # 記録\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # tqdmの説明文を更新\n",
        "        pbar.set_description(f\"Epoch {epoch + 1}/{max_epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Train acc: {train_accuracy:.4f}, Val acc: {val_accuracy:.4f}\")\n",
        "        pbar.update(1)\n",
        "\n",
        "# 最後のエポックの結果を表示\n",
        "print(f\"Epoch {max_epochs}/{max_epochs} - Train accuracy: {train_accuracy:.4f}, Train loss: {train_loss:.4f}, Val accuracy: {val_accuracy:.4f}, Val loss: {val_loss:.4f}\")\n",
        "\n",
        "# プロット用の間引き\n",
        "def reduce_points(data, num_points):\n",
        "    if len(data) > num_points:\n",
        "        indices = np.linspace(0, len(data) - 1, num_points, dtype=int)\n",
        "        return [data[i] for i in indices]\n",
        "    return data\n",
        "\n",
        "# グラフで学習の様子を表示\n",
        "\n",
        "num_points = 100\n",
        "epochs = range(1, max_epochs + 1)\n",
        "epochs_reduced = reduce_points(list(epochs), num_points)\n",
        "train_accuracies_reduced = reduce_points(train_accuracies, num_points)\n",
        "val_accuracies_reduced = reduce_points(val_accuracies, num_points)\n",
        "train_losses_reduced = reduce_points(train_losses, num_points)\n",
        "val_losses_reduced = reduce_points(val_losses, num_points)\n",
        "\n",
        "# プロット\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_reduced, train_accuracies_reduced, label='Train Accuracy')\n",
        "plt.plot(epochs_reduced, val_accuracies_reduced, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.title('Accuracy per Epoch')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_reduced, train_losses_reduced, label='Train Loss')\n",
        "plt.plot(epochs_reduced, val_losses_reduced, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0, max(max(train_losses_reduced), max(val_losses_reduced)))\n",
        "plt.legend()\n",
        "plt.title('Loss per Epoch')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "siueLWKXLXsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題３**\n",
        "\n",
        "- 学習では ＿＿＿＿＿＿ の値を小さくすることを目指します．\n",
        " - ある入力サンプルから得られた仮の出力（各クラスに属する＿＿＿＿）と，正解の ＿＿＿＿＿＿ 表現を比較して，その値が少しだけ小さくなるようにネットワークを更新します．この比較に用いる関数は ＿＿＿＿＿関数とよばれます．\n",
        "\n",
        "- 左のグラフは＿＿＿＿＿＿，右のグラフは＿＿＿＿＿＿です．\n",
        " - 学習で小さくしようとしているのは右ですか，左ですか？\n",
        "\n",
        "- 今回は最大エポック数を指定して学習を繰り返しましたが、エポック数よりもずっと多くの回数の学習が行われます。\n",
        "\n",
        " - 学習を繰り返すのち、＿＿＿＿＿ をひととおり使い切ることを 1 エポックと数える。\n",
        "\n",
        " - データ量が多すぎて一度に使い切れないので、少数のデータを取り出しては学習を行う。取り出す単位を ＿＿＿＿＿＿ と呼ぶ。これを略してバッチといい、１個に含まれる個数をプログラム中では `batch_size`（バッチサイズ）としている。今回のバッチサイズは ＿＿＿＿＿ である。\n",
        "\n",
        " - 学習のたび、＿＿＿＿＿関数を用いて ＿＿＿＿＿＿＿ と ＿＿＿＿＿＿ を比較し、その関数が小さくなるようにネットワークを修正する。その修正の処理を ＿＿＿＿＿ という[（参考）](https://hogetech.info/ml/dl/backpropagation)。\n"
      ],
      "metadata": {
        "id": "SktsNjzTxogs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 教師データや検証データで試す"
      ],
      "metadata": {
        "id": "lf1lMnjgWKaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 教師データでの予測\n",
        "y_train_pred = mlp.predict(X_train_transformed)\n",
        "y_train_pred_proba = mlp.predict_proba(X_train_transformed)\n",
        "\n",
        "# 教師データでの評価\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "train_loss = log_loss(y_train, y_train_pred_proba)\n",
        "\n",
        "print(f\"教師データでの精度: {train_accuracy:.3f}\")\n",
        "print(f\"教師データでの損失: {train_loss:.3f}\")\n",
        "\n",
        "# 検証データでの予測\n",
        "y_val_pred = mlp.predict(X_val_transformed)\n",
        "y_val_pred_proba = mlp.predict_proba(X_val_transformed)\n",
        "\n",
        "# 検証データでの評価\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_loss = log_loss(y_val, y_val_pred_proba)\n",
        "\n",
        "print(f\"検証データでの精度: {val_accuracy:.3f}\")\n",
        "print(f\"検証データでの損失: {val_loss:.3f}\")"
      ],
      "metadata": {
        "id": "j0wzWMm8OgwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 検証データを１個入力して試す\n",
        "\n",
        "作成されたネットワークがどのように動作するのか、検証データを１個用いて確認してみます。\n",
        "\n",
        "- **ネットワークへの入力（特徴量）**\n",
        "\n",
        "    検証データから特定のサンプルを選び、その特徴量ベクトルを表示します。これは、ネットワーク（モデル）に入力されるデータで、各特徴量が数値として表現されています。\n",
        "\n",
        "- **ネットワークの出力（確率）**\n",
        "\n",
        "    選択した入力に対して、モデルが各クラスに属する確率を計算します。この確率は、入力がそれぞれのクラスに分類される可能性を示しており、各クラスに対して1つの確率値が出力されます。\n",
        "\n",
        "- **推定されたクラス番号の one-hot 表現**\n",
        "\n",
        "    モデルが予測したクラスのone-hot表現を表示します。one-hot表現とは、正解クラスを1、それ以外のクラスを0とするベクトル形式のことです。例えば4クラスの場合、[0, 0, 1, 0]はクラス3を示します。\n",
        "\n",
        "- **クラス推定の表示（整数表現）**\n",
        "\n",
        "    one-hot表現のベクトルを整数に変換し、予測されたクラスを数値として示します。"
      ],
      "metadata": {
        "id": "28kqsh6QWpxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 特徴量ベクトルを入力し、各クラスの確率を出力\n",
        "sample_index = 1  # 任意のサンプルインデックス\n",
        "sample_input = X_val_transformed[sample_index].reshape(1, -1)  # 1サンプルの入力データ\n",
        "print(\"ネットワークへの入力（特徴量）　　 ：\", sample_input)\n",
        "\n",
        "# 確率の出力\n",
        "class_probabilities = mlp.predict_proba(sample_input)\n",
        "print(\"ネットワークの出力（確率）　　 　　：\", class_probabilities)\n",
        "\n",
        "# クラス推定\n",
        "predicted_class = mlp.predict(sample_input)\n",
        "print(\"推定されたクラス番号の one-hot 表現：\", predicted_class)\n",
        "print(\"推定されたクラス番号　　　　　　　 ：\", np.argmax(predicted_class))\n",
        "\n",
        "# クラス推定\n",
        "predicted_class = mlp.predict(sample_input)\n",
        "print(\"正解の one-hot 表現　　　　　　　　：\", y_train[sample_index])\n",
        "print(\"正解　　　　　　　　　　　　　　　 ：\", np.argmax(y_train[sample_index]))"
      ],
      "metadata": {
        "id": "wo3wEk2AWeSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題４**\n",
        "\n",
        "- 出力を読み取りながら埋めてください：\n",
        "\n",
        "  あるサンプルの ＿＿＿＿＿＿＿ をネットワークに入力したところ，＿＿＿＿＿＿ が出力された．\n",
        "\n",
        "  出力は，そのサンプルが各クラスに属する確率を表すものである．その中では ＿＿＿＿ 番目のクラスが最大となったことから，推定されたクラスは ＿＿＿＿ となった．"
      ],
      "metadata": {
        "id": "Vj9yir4s0OzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### テストデータでの評価\n",
        "\n",
        "モデルの精度評価としてテストデータを使用します。<br>\n",
        "学習中に精度を確認するのに用いた検証データとは異なり、学習中にはテストデータはまったく使用されていません。"
      ],
      "metadata": {
        "id": "TTAZIzrKLeAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 全テストデータの結果を表示\n",
        "all_test_samples = []\n",
        "\n",
        "for i in range(len(X_test_transformed)):\n",
        "    test_sample = X_test_transformed[i].reshape(1, -1)\n",
        "    probabilities = mlp.predict_proba(test_sample)[0]\n",
        "    predicted_class_index = np.argmax(probabilities)\n",
        "    predicted_class_name = target_names[predicted_class_index]\n",
        "    true_class_index = np.argmax(y_test[i])\n",
        "    true_class_name = target_names[true_class_index]\n",
        "\n",
        "    test_sample_df = pd.DataFrame(test_sample, columns=clms)\n",
        "    test_sample_df[\"正解\"] = f\"クラス{true_class_index}-{true_class_name}\"\n",
        "    test_sample_df[\"推論\"] = f\"クラス{predicted_class_index}-{predicted_class_name}\"\n",
        "\n",
        "    # 各クラスの確率を追加\n",
        "    for class_index, prob in enumerate(probabilities):\n",
        "        test_sample_df[f\"クラス{class_index}の確率\"] = round(prob, 3)\n",
        "\n",
        "    # カラムの順番を調整\n",
        "    probability_columns = [f\"クラス{class_index}の確率\" for class_index in range(len(probabilities))]\n",
        "    test_sample_df = test_sample_df[\n",
        "        [\"正解\", \"推論\"] + probability_columns + clms.tolist()\n",
        "    ]\n",
        "\n",
        "    all_test_samples.append(test_sample_df)\n",
        "\n",
        "all_test_samples_df = pd.concat(all_test_samples, ignore_index=True)\n",
        "\n",
        "print(\"全テストデータの結果:\")\n",
        "display(all_test_samples_df)\n"
      ],
      "metadata": {
        "id": "K0QedxwAc94z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "さらに、テストデータにおいて精度（正解率）および損失を計算してみましょう。"
      ],
      "metadata": {
        "id": "sXANa8uudS9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# テストデータでの予測\n",
        "y_test_pred = mlp.predict(X_test_transformed)\n",
        "y_test_pred_proba = mlp.predict_proba(X_test_transformed)\n",
        "\n",
        "# テストデータでの評価\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "test_loss = log_loss(y_test, y_test_pred_proba)\n",
        "\n",
        "print(f\"テストデータでの精度: {test_accuracy:.4f}\")\n",
        "print(f\"テストデータでの損失: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "DNlCHrSHLdjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題５**\n",
        "- 「全テストデータの結果」の表を読み取ってください\n",
        " - たとえば最初の行のサンプルは、（各クラスの確率を写してください）と出力された。最大の確率をもつクラスが ＿＿＿ であったため、「推論」の結果も ＿＿＿ となり、（正解に一致していた／正解と異なっていた）。"
      ],
      "metadata": {
        "id": "dgW3fwgUQXF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2次元プロットによるモデルの可視化\n",
        "\n",
        "2個（2次元）の特徴量のみで学習を行った場合、得られたモデルの分類領域を2次元で視覚化できます。<br>\n",
        "テストデータに対して、その点に対するモデルの予測結果をプロットします。"
      ],
      "metadata": {
        "id": "BGvykzFKLlO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# グリッドサーチによる領域プロット\n",
        "def plot_decision_boundary(clf, X, y, feature_indices, feature_names, title=\"\"):\n",
        "    # グリッドを生成\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                         np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "    # グリッド全体で予測を実行\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = np.argmax(Z, axis=1)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # プロットの描画\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    palette = plt.cm.get_cmap('tab10', len(np.unique(np.argmax(y, axis=1))))\n",
        "    for target in np.unique(np.argmax(y, axis=1)):\n",
        "        subset = X[np.argmax(y, axis=1) == target]\n",
        "        plt.scatter(subset[:, feature_indices[0]], subset[:, feature_indices[1]], label=f\"Class {target}: {target_names[target]}\", color=palette(target))\n",
        "    plt.xlabel(f\"Feature {feature_indices[0]}: {feature_names[0]}\")\n",
        "    plt.ylabel(f\"Feature {feature_indices[1]}: {feature_names[1]}\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# 特徴量の組み合わせを選んでプロット\n",
        "if use_two_features == True:\n",
        "  plot_decision_boundary(mlp, X_test_transformed, y_test, feature_indices, feature_names, title=f\"Decision Boundary for {dataset_name} Dataset\")\n",
        "else:\n",
        "  print(\"3次元以上の特徴量を使用しているため実行されません\")"
      ],
      "metadata": {
        "id": "y-aXjWVILkSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題６**\n",
        "\n",
        "- クラスを分ける境界のことを ＿＿＿＿＿＿＿＿ という．\n",
        " - これを適切な位置に決めることが学習であるともいえる．\n",
        " - 各クラスが概ね良好に分離できていれば学習は成功\n",
        " - あまりにも細かくクネクネと分離している状況は ＿＿＿＿＿ とされる．教師データでは高い精度を得られるが，テストデータにはむしろ精度が下がってしまう．学習のしすぎを防ぐために，＿＿＿データを用いてハイパーパラメータを調節したり，ちょうどよいタイミングでで学習を停止したりする工夫が行われる．"
      ],
      "metadata": {
        "id": "u0JmbxqH2LzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **課題７**\n",
        "\n",
        "学んだことをもとに，ご自身の言葉でまとめてください．\n",
        "\n",
        "- ニューラルネットワークの入力と出力\n",
        "- 分類問題における学習とは、何を用いて何をするもの？\n",
        " - クラス・サンプル・特徴量・決定境界・損失・one-hot といった言葉を用いてください\n",
        "- 学習されたモデルをどうやって使うと，新たなサンプルのクラスを推定できる？\n"
      ],
      "metadata": {
        "id": "pc6diR8k3rz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 発展\n",
        "\n",
        "今回，１サンプルの特徴量は，多くても 30 次元でした．\n",
        "\n",
        "しかし，たとえば画像を判別するなら，とても小さな画像でも 縦 32 × 横 32 × RGB 3チャンネル ＝ 3072 次元くらいはあります．この場合，画像を特徴量に変換（**特徴抽出**）した上で，判別に使うことになります．\n",
        "\n",
        "この特徴抽出をネットワーク内部で行ってくれるものとして有名なのが **畳み込み層**（Convolutional Layer）です．また，それを含むニューラルネットワークが畳み込みニューラルネットワーク（Convolutional Neural Network; **CNN**）です．\n",
        "\n",
        "- 多次元配列の扱いについては [NumPy の演習](https://github.com/shizoda/education/blob/main/machine_learning/numpy.ipynb)\n",
        "\n",
        "- デジタル画像の表現については [画像表現の演習](https://github.com/shizoda/education/blob/main/image/colors.ipynb)\n",
        "\n",
        "- 畳み込みやフィルタ処理については [畳み込みの演習](https://github.com/shizoda/education/blob/main/image/conv.ipynb)\n",
        "\n",
        "これらをご存知の方はこのまま\n",
        "\n",
        "- [CNN を用いた画像分類の演習](https://github.com/shizoda/education/blob/main/machine_learning/cnn/cifar10_pytorch.ipynb)\n",
        "\n",
        "に進んでください．"
      ],
      "metadata": {
        "id": "u9SgYRGm-yZ5"
      }
    }
  ]
}
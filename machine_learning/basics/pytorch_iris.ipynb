{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB0qnsptvSzrf0AfoZu1NU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/machine_learning/basics/pytorch_iris.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorchで体験するニューラルネットワーク構築（Irisデータセット）\n",
        "\n",
        "この演習では、**PyTorch (パイトーチ)** という、機械学習（特にディープラーニング）のためのライブラリを使用して、基本的なニューラルネットワークをゼロから構築する流れを学びます。\n",
        "\n",
        "### 💬 PyTorchとは？\n",
        "\n",
        "PyTorchは、Pythonで広く使われている機械学習ライブラリです。主な特徴は以下の通りです。\n",
        "\n",
        "1.  **テンソル (Tensor) の計算:** NumPyの配列（ndarray）に似ていますが、GPU（画像処理装置）を使った高速な並列計算に最適化されています。ニューラルネットワークは大量の行列計算を行うため、GPUによる高速化が不可欠です。\n",
        "2.  **自動微分機能:** ニューラルネットワークの学習（パラメータの最適化）には、微分計算が必要です。PyTorchは、複雑な計算グラフに対しても、その微分（勾配）を自動で計算する機能（`autograd`）を提供します。\n",
        "3.  **柔軟なモデル構築:** モデルの定義や学習プロセスをPythonのコードで直感的に記述でき、研究者や開発者に人気があります。\n",
        "\n",
        "この演習では、PyTorchの基本的な構成要素（層、損失関数、オプティマイザ）を自分で組み合わせて、シンプルなニューラルネットワーク（多層パーセプトロン）を構築していきます。"
      ],
      "metadata": {
        "id": "XoKwDWVvS3db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 ニューラルネットワークの基本構造\n",
        "\n",
        "ニューラルネットワークは、人間の脳の神経細胞（ニューロン）の仕組みを単純化して模倣した数学モデルです。データから複雑なパターンを学習することができます。\n",
        "\n",
        "基本的なニューラルネットワークは、いくつかの「層 (Layer)」から構成されます。\n",
        "\n",
        "1.  **入力層 (Input Layer):**\n",
        "    * データ（特徴量）を最初に入力する層です。\n",
        "    * Irisデータセットの場合、「ガクの長さ」「ガクの幅」「花弁の長さ」「花弁の幅」の4つの特徴量があるので、入力層のノード（丸い部分）数は4になります。\n",
        "\n",
        "2.  **隠れ層 (Hidden Layer):**\n",
        "    * 入力層と出力層の間にあり、目に見えない中間的な計算を行う層です。\n",
        "    * この層のノード数や層の数（深さ）を増やすことで、モデルはより複雑なパターンを学習できるようになります。\n",
        "    * ノード同士は「重み (Weight)」と呼ばれるパラメータで接続されており、学習とはこの「重み」を適切な値に調整していく作業を指します。\n",
        "\n",
        "3.  **出力層 (Output Layer):**\n",
        "    * モデルの最終的な計算結果（予測）を出力する層です。\n",
        "    * Irisデータセットの場合、「Setosa」「Versicolor」「Virginica」の3つの品種に分類する問題なので、出力層のノード数は3になります。各ノードは、それぞれの品種である「確率」や「スコア」を出力します。\n",
        "\n",
        "PyTorchを使えば、これらの層を定義し、それらを `forward` という関数でつなぎ合わせるだけで、ニューラルネットワークモデルを比較的簡単に記述できます。"
      ],
      "metadata": {
        "id": "4xe8NAHLU_g8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 📚 1. 準備: ライブラリのインポート\n",
        "\n",
        "まず、演習に必要なライブラリをインポートします。\n",
        "それぞれのライブラリの役割は以下の通りです。\n",
        "\n",
        "* `torch`: PyTorch本体です。テンソルの計算や自動微分など、中心的な機能が含まれます。\n",
        "* `torch.nn (Neural Network)`: ニューラルネットワークの層（`nn.Linear`など）、損失関数（`nn.CrossEntropyLoss`など）が定義されています。\n",
        "* `torch.optim (Optimizer)`: パラメータを最適化するアルゴリズム（`optim.Adam`など）が含まれます。\n",
        "* `torch.utils.data`: データを効率的に扱うための `DataLoader` や `TensorDataset` が含まれます。\n",
        "* `sklearn`: scikit-learnという機械学習ライブラリです。今回はデータセットのロード、標準化、データ分割に使用します。\n",
        "* `numpy`: Pythonで数値計算を行うための基本的なライブラリです。\n",
        "* `matplotlib.pyplot`: グラフを描画するために使用します。\n",
        "* `tqdm.auto`: 学習の進捗状況をプログレスバーで表示するために使用します。"
      ],
      "metadata": {
        "id": "zi2KxUUkVD5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZb94HmmSvI7"
      },
      "outputs": [],
      "source": [
        "# torch をインポートしてください\n",
        "# torch.nn というモジュールを nn という名前でインポートしてください\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 乱数シードの固定（結果の再現性を高めるため）\n",
        "# これにより、何度実行しても同じ結果が得られやすくなります\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 🔢 2. データの準備\n",
        "\n",
        "#### データのロードと「標準化」\n",
        "scikit-learnを使ってIrisデータセットをロードします。\n",
        "\n",
        "次に、**標準化 (Standardization)** を行います。これは、各特徴量（ガクの長さ、幅など）の平均を0、分散を1に揃える処理です。\n",
        "例えば、「ガクの長さ」は 4.3〜7.9 の範囲ですが、「ガクの幅」は 2.0〜4.4 の範囲です。このように特徴量ごとにスケール（値の範囲）が大きく異なると、学習が非効率になったり、特定の（値が大きい）特徴量に学習が偏ったりすることがあります。\n",
        "標準化によってスケールを揃えることは、ニューラルネットワークの学習をスムーズに進めるための一般的な前処理です。"
      ],
      "metadata": {
        "id": "b1JpjhCpTN9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データをロード\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# データを標準化 (平均0, 分散1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(f\"標準化前の平均: {X.mean(axis=0)}\")\n",
        "print(f\"標準化後の平均: {X_scaled.mean(axis=0)}\")\n",
        "print(f\"標準化後の標準偏差: {X_scaled.std(axis=0)}\")"
      ],
      "metadata": {
        "id": "3cPjv5FwTQUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 「テンソル」への変換\n",
        "PyTorchは、**Tensor（テンソル）** という専用のデータ形式で計算を行います。これはNumpyの配列（ndarray）に似ていますが、GPUでの高速計算に対応している点が大きな特徴です。\n",
        "Numpyの配列をPyTorchのテンソルに変換します。\n",
        "\n",
        "**【課題1】** `( XXXX )` を埋めて、Numpy配列 `X_scaled` と `y` をテンソルに変換してください。\n",
        "* 特徴量 `X` は、小数点以下の数値を含むため `torch.FloatTensor` を使います。\n",
        "* ラベル `y` は、0, 1, 2 といったクラス番号（整数）であり、後で使う損失関数 `CrossEntropyLoss` の要求仕様でもあるため `torch.LongTensor` を使います。"
      ],
      "metadata": {
        "id": "Qu7KyeslTS9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データをPyTorchテンソルに変換\n",
        "# 課題1: ( XXXX ) を埋めてください\n",
        "X_tensor = torch.FloatTensor( ( XXXX ) )\n",
        "y_tensor = torch.LongTensor( ( XXXX ) )\n",
        "\n",
        "print(f\"特徴量テンソルの形状: {X_tensor.shape}\")\n",
        "print(f\"ラベルテンソルの形状: {y_tensor.shape}\")"
      ],
      "metadata": {
        "id": "xGg_POOWTSa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### データの分割\n",
        "モデルの性能を正しく評価するため、データセットを3つに分割します。これは機械学習において非常に重要なステップです。\n",
        "\n",
        "1.  **教師データ (Training Data):**\n",
        "    * モデルの学習（パラメータの更新）に直接使用します。\n",
        "    * モデルはこのデータを見て、入力（特徴量）と出力（正解ラベル）の関係を学びます。\n",
        "\n",
        "2.  **検証データ (Validation Data):**\n",
        "    * 学習中のモデルの性能を「監視」するために使用します。\n",
        "    * モデルは**このデータを見て学習（重みの更新）をしません。**\n",
        "    * 主な役割は、「過学習（Overfitting）」をチェックすることです。教師データでの性能は上がっているのに、検証データでの性能が下がり始めたら、モデルが教師データを「丸暗記」し始めている兆候（過学習）であり、学習を止めるべきタイミング（**Early Stopping**）だと判断できます。\n",
        "    * また、学習率や隠れ層のノード数といった「ハイパーパラメータ」を調整する際にも、この検証データの性能を指標にします。\n",
        "\n",
        "3.  **テストデータ (Test Data):**\n",
        "    * 学習プロセス（学習・検証）では**一切使用しません。**\n",
        "    * 学習とハイパーパラメータ調整がすべて完了した後、最終的に完成したモデルの「真の性能（汎化性能）」を評価するために、**一度だけ**使用します。\n",
        "    * 検証データも調整に使ってしまった以上、モデルは（間接的に）検証データに最適化されている可能性があるため、完全に未知のデータであるテストデータでの評価が最も客観的な指標となります。\n",
        "\n",
        "ここでは、`stratify=y_tensor` というオプションを指定しています。これは、元のデータセットのクラスの比率（Setosa, Versicolor, Virginicaが約1:1:1）が、分割後の教師データ、検証データ、テストデータのすべてにおいて保たれるようにする（層化サンプリング）ための指定です。\n",
        "\n",
        "**【課題2】** `( XXXX )` を埋めて、データを分割してください。\n",
        "* ヒント: 最初の分割では `X_tensor` と `y_tensor` を使います。2回目で、1回目の残り（`X_temp`, `y_temp`）をさらに半分にします。"
      ],
      "metadata": {
        "id": "LW2QIhj8TWWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データを分割 (教師 70%, 検証 15%, テスト 15%)\n",
        "\n",
        "# 課題2: ( XXXX ) を埋めてください\n",
        "\n",
        "# まず、教師データ(70%)と一時データ(30%)に分ける\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    ( XXXX ), ( XXXX ),\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y_tensor # ラベルの比率を保つ\n",
        ")\n",
        "\n",
        "# 次に、一時データ(30%)を検証データ(15%)とテストデータ(15%)に分ける\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    ( XXXX ), ( XXXX ),\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=y_temp # ラベルの比率を保つ\n",
        ")\n",
        "\n",
        "print(f\"教師データ数: {len(y_train)}\")\n",
        "print(f\"検証データ数: {len(y_val)}\")\n",
        "print(f\"テストデータ数: {len(y_test)}\")"
      ],
      "metadata": {
        "id": "NC4s1ychTXqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 📦 3. データローダーの作成\n",
        "\n",
        "`DataLoader` は、PyTorchでの学習を効率化するための仕組みです。主な役割は以下の2つです。\n",
        "\n",
        "1.  **バッチ処理 (Batch Processing):**\n",
        "    * データセット全体（例: 105個）を一度にモデルに入力するのではなく、「**バッチサイズ (Batch Size)**」という小さなまとまり（例: 16個ずつ）に分けて供給します。\n",
        "    * これにより、GPUメモリの使用量を抑えつつ、学習を安定させる効果があります（一度に大量のデータで計算するより、少しずつ計算して頻繁にパラメータを更新する方が、効率的に最適解にたどり着きやすいため）。\n",
        "\n",
        "2.  **シャッフル (Shuffle):**\n",
        "    * 教師データ（`train_loader`）について、学習を1周（1エポック）するごとにデータの順序をランダムに並び替えます。\n",
        "    * これにより、モデルがデータの「順序」を学習してしまう（例えば「Setosaの次にはVersicolorが来やすい」といった、本来学習すべきでないパターンを覚えてしまう）ことを防ぎます。\n",
        "\n",
        "まず、特徴量（X）とラベル（y）をペアにした `TensorDataset` を作成し、それを `DataLoader` に渡します。\n",
        "\n",
        "**【課題3】** `( XXXX )` を埋めて、各データセット（train, val, test）の `TensorDataset` を作成してください。"
      ],
      "metadata": {
        "id": "MaEjvUBSViHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 課題3: ( XXXX ) を埋めてください\n",
        "\n",
        "# 1. TensorDatasetの作成 (Xとyをペアにする)\n",
        "train_dataset = TensorDataset( ( XXXX ), ( XXXX ) )\n",
        "val_dataset = TensorDataset( ( XXXX ), ( XXXX ) )\n",
        "test_dataset = TensorDataset( ( XXXX ), ( XXXX ) )\n",
        "\n",
        "# 2. DataLoaderの作成\n",
        "BATCH_SIZE = 16 # バッチサイズ（一度に学習するサンプル数）\n",
        "\n",
        "# shuffle=True は教師データのみ（学習時に順序をランダム化するため）\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "Sb7gUplOVix4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 🧠 4. モデル（ネットワーク）の定義\n",
        "\n",
        "いよいよニューラルネットワークの「設計図」を作成します。\n",
        "PyTorchでは `torch.nn.Module` を継承したクラスとして定義するのが標準的です。\n",
        "\n",
        "* `__init__` (コンストラクタ):\n",
        "    * ネットワークの「部品」（層）をあらかじめ定義しておく場所です。\n",
        "    * `nn.Linear(A, B)` は、A個の入力ノードからB個の出力ノードへの**全結合層（Linear Layer）**を意味します。\n",
        "* `forward` (順伝播):\n",
        "    * データがネットワークをどのように流れるか（計算の順序）を定義する場所です。\n",
        "    * `__init__` で定義した部品を、ここで順番に組み合わせていきます。\n",
        "\n",
        "#### 活性化関数 (Activation Function) とは？\n",
        "`nn.Linear` は「線形変換」（入力に重みを掛けて足し合わせる）しかできません。線形変換だけをいくら重ねても、結果は線形のままです。\n",
        "しかし、現実世界のパターン（例: Irisの品種の境界線）は単純な直線（線形）では分けられないことがほとんどです。\n",
        "\n",
        "そこで、層と層の間に「**活性化関数**」と呼ばれる「非線形 (Non-linear)」な関数を挟みます。これにより、モデルは複雑な曲線的な境界線を学習できるようになります。\n",
        "\n",
        "今回は、代表的な活性化関数である `nn.ReLU()` (ランプ関数) を使用します。これは、入力が0以下なら0を、0より大きければその値をそのまま出力する、非常にシンプルな非線形関数です。\n",
        "\n",
        "**【課題4】** `( XXXX )` を埋めて、入力層(4) → 隠れ層(10) → 出力層(3) のネットワークを完成させてください。"
      ],
      "metadata": {
        "id": "HVTuHFGjVki7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        # 親クラス(nn.Module)の初期化を呼び出す\n",
        "        super(SimpleNeuralNet, self).__init__()\n",
        "\n",
        "        # 課題4 (1/3): ネットワークの層を定義\n",
        "        # 第1層 (入力層 -> 隠れ層)\n",
        "        # 入力ノード数: input_dim, 出力ノード数: hidden_dim\n",
        "        self.fc1 = nn.Linear( ( XXXX ) , ( XXXX ) )\n",
        "\n",
        "        # 活性化関数 (今回はReLUを使います)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # 第2層 (隠れ層 -> 出力層)\n",
        "        # 入力ノード数: hidden_dim, 出力ノード数: output_dim\n",
        "        self.fc2 = nn.Linear( ( XXXX ) , ( XXXX ) )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 課題4 (2/3): データが流れる順序を定義\n",
        "        # x -> fc1 -> relu -> fc2 -> output\n",
        "\n",
        "        # 第1層を通して...\n",
        "        out = self. ( XXXX ) (x)\n",
        "        # 活性化関数を通して...\n",
        "        out = self. ( XXXX ) (out)\n",
        "        # 第2層を通して...\n",
        "        out = self. ( XXXX ) (out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# --- モデルのパラメータ設定 ---\n",
        "INPUT_DIM = 4      # 入力層のノード数（特徴量の数）\n",
        "HIDDEN_DIM = 10    # 隠れ層のノード数（この値は自由に変更して試せます）\n",
        "OUTPUT_DIM = 3     # 出力層のノード数（クラスの数）\n",
        "\n",
        "# 課題4 (3/3): モデルのインスタンス（実体）を作成\n",
        "model = SimpleNeuralNet( ( XXXX ) , ( XXXX ) , ( XXXX ) )\n",
        "\n",
        "# モデルの構造を確認\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Mvjg77WxVmKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 🎯 5. 損失関数とオプティマイザの選択\n",
        "\n",
        "モデルの学習に必要な「学習方針」を決める、重要な構成要素を2つ選びます。\n",
        "\n",
        "1.  **損失関数 (Loss Function / Criterion):**\n",
        "    * モデルの予測と正解の「ズレ（誤差）」を数値化する関数です。\n",
        "    * 学習の目標は、この**損失の値を最小にすること**です。\n",
        "    * 今回は、多クラス分類問題で最も一般的に使われる「**クロスエントロピー損失**」 (`nn.CrossEntropyLoss`) を使用します。これは、モデルが予測した「確率分布（3クラスの確率）」と「正解（例: クラス2が正解）」が、どれだけ異なっているかを測る指標です。\n",
        "\n",
        "2.  **オプティマイザ (Optimizer / 最適化手法):**\n",
        "    * 損失を最小化するために、モデルのパラメータ（重み）を「どのように更新するか」を決めるアルゴリズムです。\n",
        "    * 損失の値（誤差）に基づいて、各パラメータがどちらの方向に（プラスかマイナスか）、どれくらい動けば損失が減るか（＝勾配）を計算し、パラメータを少しずつ更新していきます。\n",
        "    * 今回は「**Adam**」 (`optim.Adam`) という、効率的で高性能なため広く使われているアルゴリズムを使用します。\n",
        "\n",
        "3.  **学習率 (Learning Rate):**\n",
        "    * オプティマイザがパラメータを一度にどれくらいの「歩幅」で更新するかを決める値です。\n",
        "    * 大きすぎると最適解を通り過ぎて学習が不安定になり、小さすぎると学習に時間がかかりすぎます。Adamは、この学習率をある程度自動で調整してくれる機能も持っています。\n",
        "\n",
        "**【課題5】** `( XXXX )` を埋めて、損失関数とオプティマイザを設定してください。"
      ],
      "metadata": {
        "id": "eoRDeSWgV_WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習率 (Learning Rate)\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# 課題5: ( XXXX ) を埋めてください\n",
        "\n",
        "# 損失関数 (Cross Entropy Loss)\n",
        "# nn.CrossEntropyLoss は、内部でSoftmaxの計算も含むため、モデルの出力層にSoftmaxを入れなくてもよい\n",
        "criterion = nn. ( XXXX ) ()\n",
        "\n",
        "# オプティマイザ (Adam)\n",
        "# model.parameters() で、モデル内の学習対象パラメータ（重み）をすべて渡します。\n",
        "optimizer = optim. ( XXXX ) (model.parameters(), lr= ( XXXX ) )"
      ],
      "metadata": {
        "id": "kM6NE1KPVpyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 🏋️ 6. 学習（トレーニング）の実行\n",
        "\n",
        "いよいよモデルを学習させます。PyTorchでは、学習ループ（エポックごとの繰り返し）を自分で書く必要があります。\n",
        "\n",
        "**エポック (Epoch)** とは、教師データセット全体を1周学習することです。\n",
        "\n",
        "**学習の1ステップ（ミニバッチごと）:**\n",
        "学習ループの中では、`train_loader` からバッチ（16個のデータ）を一つずつ取り出し、以下の処理を繰り返します。\n",
        "\n",
        "1.  `optimizer.zero_grad()`: **勾配のリセット**。PyTorchは勾配を（意図的に）累積する仕様のため、各バッチの計算開始時に必ずリセットが必要です。\n",
        "2.  `outputs = model(inputs)`: **順伝播 (Forward Propagation)**。データをモデルに入力し、予測（出力）を得ます。\n",
        "3.  `loss = criterion(outputs, labels)`: **損失の計算**。予測と正解ラベルを比べ、損失（誤差）を計算します。\n",
        "4.  `loss.backward()`: **誤差逆伝播 (Backpropagation)**。損失を各パラメータに逆伝播させ、勾配（損失を減らすためのパラメータの更新方向）を自動計算します。\n",
        "5.  `optimizer.step()`: **パラメータの更新**。計算された勾配に基づき、オプティマイザがパラメータを更新します。\n",
        "\n",
        "---\n",
        "**モード切り替え:**\n",
        "* `model.train()`: 学習モード。Dropout層など、学習時と評価時で挙動が異なる層を正しく動作させるために呼び出します。（今回は使いませんが、習慣として記述します）\n",
        "* `model.eval()`: 評価モード。学習を行わない（Dropoutを無効化するなど）状態にします。検証データやテストデータを評価する前に呼び出します。\n",
        "\n",
        "**勾配計算の停止:**\n",
        "* `with torch.no_grad()`: 評価時（`val_loader` や `test_loader` を使う時）は、パラメータの更新も誤差逆伝播も不要です。このブロックで囲むことで、勾配計算をすべて停止し、余計な計算を省いてメモリ効率と速度を向上させます。\n",
        "\n",
        "**【課題6】** `( XXXX )` を埋めて、学習ループの核となる5ステップを完成させてください。"
      ],
      "metadata": {
        "id": "uqEX6oppVrNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 200      # 最大エポック数\n",
        "PATIENCE = 10     # Early Stoppingのための我慢回数（この回数連続で改善がなければ停止）\n",
        "\n",
        "# ログ（履歴）の保存用\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_loss = float('inf') # 検証損失のベストスコアを無限大で初期化\n",
        "trigger_times = 0 # 損失が改善しなかった回数のカウンター\n",
        "\n",
        "# tqdmを使って進捗バーを表示\n",
        "pbar = tqdm(range(EPOCHS), desc=\"Epochs\")\n",
        "\n",
        "for epoch in pbar:\n",
        "\n",
        "    # --- 教師データでの学習 (Train) ---\n",
        "    model.train() # 学習モードに設定\n",
        "    total_train_loss = 0\n",
        "    total_train_correct = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "\n",
        "        # 課題6 (1/5): 勾配をリセット\n",
        "        optimizer. ( XXXX ) ()\n",
        "\n",
        "        # 課題6 (2/5): 順伝播 (forward)\n",
        "        outputs = ( XXXX ) (inputs)\n",
        "\n",
        "        # 課題6 (3/5): 損失の計算\n",
        "        loss = ( XXXX ) (outputs, labels)\n",
        "\n",
        "        # 課題6 (4/5): 逆伝播 (backward)\n",
        "        loss. ( XXXX ) ()\n",
        "\n",
        "        # 課題6 (5/5): パラメータの更新\n",
        "        optimizer. ( XXXX ) ()\n",
        "\n",
        "        # --- ここから下は集計・評価処理 ---\n",
        "        total_train_loss += loss.item() * inputs.size(0) # バッチごとの損失を累積\n",
        "        _, predicted = torch.max(outputs.data, 1) # 最も確率の高いクラスを予測とする\n",
        "        total_train_correct += (predicted == labels).sum().item() # 正解数を累積\n",
        "\n",
        "    # エポックごとの平均損失と平均精度を計算\n",
        "    avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
        "    avg_train_acc = total_train_correct / len(train_loader.dataset)\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['train_acc'].append(avg_train_acc)\n",
        "\n",
        "    # --- 検証データでの評価 (Validation) ---\n",
        "    model.eval() # 評価モードに設定\n",
        "    total_val_loss = 0\n",
        "    total_val_correct = 0\n",
        "\n",
        "    with torch.no_grad(): # 勾配計算を無効化\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
        "    avg_val_acc = total_val_correct / len(val_loader.dataset)\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    history['val_acc'].append(avg_val_acc)\n",
        "\n",
        "    # 進捗バーに情報を表示\n",
        "    pbar.set_postfix({\n",
        "        'TrainLoss': f'{avg_train_loss:.4f}',\n",
        "        'ValLoss': f'{avg_val_loss:.4f}',\n",
        "        'TrainAcc': f'{avg_train_acc:.4f}',\n",
        "        'ValAcc': f'{avg_val_acc:.4f}'\n",
        "    })\n",
        "\n",
        "    # --- Early Stoppingの判定 ---\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        # 検証損失が改善した場合\n",
        "        best_val_loss = avg_val_loss\n",
        "        trigger_times = 0 # カウンターをリセット\n",
        "        # (オプション: 最も性能の良いモデルのパラメータを保存)\n",
        "        # torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        # 検証損失が改善しなかった場合\n",
        "        trigger_times += 1 # カウンターを増やす\n",
        "        if trigger_times >= PATIENCE:\n",
        "            # 我慢回数を超えたら学習を停止\n",
        "            print(f'\\nEarly stopping at epoch {epoch+1}')\n",
        "            pbar.close()\n",
        "            break\n",
        "\n",
        "if (epoch + 1) == EPOCHS:\n",
        "    print('\\nFinished Training (Reached max epochs)')"
      ],
      "metadata": {
        "id": "WBgAElMVVsnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 📊 7. 結果の可視化\n",
        "\n",
        "学習の経過（損失と精度）をグラフで確認します。\n",
        "\n",
        "* **損失 (Loss) グラフ:** 学習が進むにつれて、教師データ（Train Loss）と検証データ（Validation Loss）の両方が下がっていくのが理想です。\n",
        "* **精度 (Accuracy) グラフ:** 逆に、両方の精度が上がっていくのが理想です。\n",
        "\n",
        "#### 「過学習 (Overfitting)」の兆候\n",
        "もし、`Train Loss` は下がり続けるのに `Validation Loss` が上昇し始めたり（グラフが「V」字に開く）、`Train Accuracy` が上がり続けるのに `Validation Accuracy` が横ばいや低下し始めたりした場合、それが**過学習**の兆候です。\n",
        "モデルは教師データに過剰に適合（ほぼ丸暗記）してしまい、未知のデータ（検証データ）に対する性能が落ちていることを示します。\n",
        "`Early Stopping` は、`Validation Loss` が上昇し始める（または改善しなくなる）時点で学習を止めることで、過学習を防ぐテクニックです。"
      ],
      "metadata": {
        "id": "7laTVlP5VueD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフの描画\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# 損失 (Loss)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# 精度 (Accuracy)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F2DI-bEsVvqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 💯 8. テストデータによる最終評価\n",
        "\n",
        "最後に、学習にも検証（Early Stoppingの判断）にも使っていない、完全に未知の「テストデータ」 (`test_loader`) を使って、モデルの最終的な性能（汎化性能）を評価します。\n",
        "これが、このモデルの「実力」を示す最も客観的な数値となります。\n",
        "\n",
        "**【課題7】** `( XXXX )` を埋めて、テストデータで評価するコードを完成させてください。\n",
        "\n",
        "* ヒント: 検証データの評価ループとほとんど同じですが、使用するデータローダーが異なります。"
      ],
      "metadata": {
        "id": "G9gr15k-VyMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # 評価モード\n",
        "total_test_correct = 0\n",
        "\n",
        "# (オプション: Early Stoppingで保存したベストモデルをロードする場合)\n",
        "# model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # 課題7 (1/3): 使用するデータローダー\n",
        "    for inputs, labels in ( XXXX ):\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        # 予測結果（最もスコアが高いクラスのインデックス）を取得\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # 課題7 (2/3): 正解(labels)と比較\n",
        "        total_test_correct += ( ( XXXX ) == ( XXXX ) ).sum().item()\n",
        "\n",
        "# 課題7 (3/3): テストデータセット全体で割る\n",
        "test_accuracy = total_test_correct / len( ( XXXX ) .dataset)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f} %')"
      ],
      "metadata": {
        "id": "bt-6PzmmVxqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### 🎉 9. まとめと発展\n",
        "\n",
        "この演習では、PyTorchを使ってニューラルネットワークを構成する基本的な要素（データ準備、テンソル、データローダー、モデル定義、活性化関数、損失関数、オプティマイザ、学習ループ、評価）を学びました。\n",
        "各ステップを自分で定義することで、ライブラリが内部で何を行っているかの理解が深まったかと思います。\n",
        "\n",
        "#### 💡 発展課題\n",
        "* `HIDDEN_DIM` (隠れ層のノード数) や `LEARNING_RATE` (学習率) を変えると、学習結果（グラフや最終精度）はどのように変わるか試してみましょう。\n",
        "* `class SimpleNeuralNet` を変更して、隠れ層をもう1層増やしてみてください（`__init__` で `self.fc3` などを定義し、`forward` の流れを変更する必要があります）。精度は向上するでしょうか？"
      ],
      "metadata": {
        "id": "_N-xkMqNV0f4"
      }
    }
  ]
}
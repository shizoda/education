{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsYfw7KyoNkLrIxVSIPzDj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/agent/LangGraph_and_Gradio(1)_WebUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio を用いた Web 上の AI エージェントの実装\n",
        "\n",
        "本ノートブックでは、LangGraph を用いたツール実行（Function Calling）可能なチャットボットバックエンドの構築と、Gradio を用いた Web UI の実装プロセスを解説します。\n",
        "\n",
        "主な学習項目は以下の通りです。\n",
        "1.  **ツール定義**: Python 関数を LLM が利用可能な形式で定義する方法。\n",
        "2.  **ステート管理**: LangGraph を使用し、LLM の推論結果に基づいて条件分岐（ツール実行の可否判断）を行うフローの構築。\n",
        "3.  **Web UI 実装**: バックエンドのロジックを Gradio インターフェースに統合し、ブラウザ上で対話可能なアプリケーションとして公開する方法。\n",
        "\n",
        "LLM バックエンドには OpenRouter 経由で DeepSeek モデルを使用しますが、LangChain の抽象化により他の OpenAI 互換モデルでも同様の構造で動作します。"
      ],
      "metadata": {
        "id": "_Ct4fLkSIbsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリのインストール\n",
        "!pip install -qU langgraph langchain-openai langchain-core gradio termcolor"
      ],
      "metadata": {
        "id": "FtqHo0w1IwBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 環境設定\n",
        "\n",
        "OpenRouter API キーを環境変数に設定し、モデルを初期化します。"
      ],
      "metadata": {
        "id": "CNfJNMBdIzJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from termcolor import colored\n",
        "\n",
        "# APIキーの読み込み\n",
        "try:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "    print(colored(\"OPENROUTER_API_KEY loaded.\", \"green\"))\n",
        "except Exception as e:\n",
        "    print(colored(\"Error: OPENROUTER_API_KEY not found in Secrets.\", \"red\"))\n",
        "\n",
        "# モデル名の定義\n",
        "MODEL_NAME = \"deepseek/deepseek-chat-v3-0324\""
      ],
      "metadata": {
        "id": "Bf7s5syUIyrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ツール (関数) の定義\n",
        "\n",
        "LLM が外部ツールとして利用する Python 関数を定義します。LangChain の `@tool` デコレータを使用することで、関数の型ヒントとドキュメント文字列からツールスキーマが生成されます。\n",
        "\n",
        "ここでは、単純な計算処理を行う `calculate_add` 関数を実装し、エージェントが数値計算リクエストに対してこの関数を呼び出すように設計します。"
      ],
      "metadata": {
        "id": "hg8RyMe0I1qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def calculate_add(a: int, b: int) -> int:\n",
        "    \"\"\"\n",
        "    2つの整数の加算を行います。\n",
        "    ユーザーが計算を求めた場合に使用されます。\n",
        "\n",
        "    Args:\n",
        "        a: 加算する1つ目の整数\n",
        "        b: 加算する2つ目の整数\n",
        "    \"\"\"\n",
        "    result = a + b\n",
        "    # 実行確認用ログ\n",
        "    print(colored(f\"[Function Execution] {a} + {b} = {result}\", \"cyan\"))\n",
        "    return result\n",
        "\n",
        "# ツールリストの作成\n",
        "tools = [calculate_add]\n",
        "\n",
        "# スキーマの確認\n",
        "print(f\"Tool Name: {tools[0].name}\")\n",
        "print(f\"Tool Args: {tools[0].args}\")"
      ],
      "metadata": {
        "id": "JfE-WyDMI1CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ツール (関数) の定義\n",
        "\n",
        "LLM が外部ツールとして利用する Python 関数を定義します。LangChain の `@tool` デコレータを使用することで、関数の型ヒントとドキュメント文字列からツールスキーマが生成されます。\n",
        "\n",
        "ここでは、単純な計算処理を行う `calculate_add` 関数を実装し、エージェントが数値計算リクエストに対してこの関数を呼び出すように設計します。"
      ],
      "metadata": {
        "id": "CdhhZZpRI4iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def calculate_add(a: int, b: int) -> int:\n",
        "    \"\"\"\n",
        "    2つの整数の加算を行います。\n",
        "    ユーザーが計算を求めた場合に使用されます。\n",
        "\n",
        "    Args:\n",
        "        a: 加算する1つ目の整数\n",
        "        b: 加算する2つ目の整数\n",
        "    \"\"\"\n",
        "    result = a + b\n",
        "    # 実行確認用ログ\n",
        "    print(colored(f\"[Function Execution] {a} + {b} = {result}\", \"cyan\"))\n",
        "    return result\n",
        "\n",
        "# ツールリストの作成\n",
        "tools = [calculate_add]\n",
        "\n",
        "# スキーマの確認\n",
        "print(f\"Tool Name: {tools[0].name}\")\n",
        "print(f\"Tool Args: {tools[0].args}\")"
      ],
      "metadata": {
        "id": "rrZkHnreI5wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangGraph によるグラフ構築\n",
        "\n",
        "バックエンドのロジックをステートグラフとして定義します。\n",
        "\n",
        "### 構成要素\n",
        "* **State**: 会話履歴 (`messages`) を保持するデータ構造。\n",
        "* **Chatbot Node**: LLM を呼び出し、応答またはツール呼び出しリクエストを生成するノード。\n",
        "* **Tools Node**: LLM からのリクエストに基づき、実際の Python 関数を実行するノード。\n",
        "* **Conditional Edge**: LLM の出力にツール呼び出しが含まれるかを判定し、次に行うべき処理（ツール実行または終了）へ分岐させるロジック。"
      ],
      "metadata": {
        "id": "WfSNe_bBI7ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# State 定義: メッセージ履歴を管理\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# LLM の初期化\n",
        "llm = ChatOpenAI(\n",
        "    model=MODEL_NAME,\n",
        "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.environ[\"OPENROUTER_API_KEY\"]\n",
        ")\n",
        "\n",
        "# ツールバインディング: LLM に利用可能なツール情報を渡す\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# ノード関数: チャットボットの応答生成\n",
        "def chatbot_node(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# ノード関数: ツールの実行（LangGraph 組み込みの ToolNode を使用）\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# グラフビルダーの初期化\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# ノードの追加\n",
        "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# エッジの定義\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# 条件付きエッジ: ツール呼び出しの有無による分岐\n",
        "# tools_condition は、メッセージに tool_calls が含まれていれば \"tools\" へ、\n",
        "# そうでなければ END へ遷移する判定ロジック\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition\n",
        ")\n",
        "\n",
        "# ツール実行後は再び chatbot ノードへ戻り、結果を言語化させる\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "# グラフのコンパイル\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "ejYmGbxHI8hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# コンソールでの動作検証\n",
        "\n",
        "Gradio 実装の前に、グラフが正常に動作するかをスクリプト上で確認します。\n",
        "入力に対してツール呼び出しが発生し、その結果が LLM に渡されているかを確認します。"
      ],
      "metadata": {
        "id": "Nd6J9KGYI-Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# 検証用クエリ\n",
        "query = \"3と5を足してください。\"\n",
        "print(f\"Input: {query}\\n\" + \"-\"*30)\n",
        "\n",
        "inputs = {\"messages\": [HumanMessage(content=query)]}\n",
        "\n",
        "# グラフの実行とストリーミング出力\n",
        "for event in graph.stream(inputs, stream_mode=\"values\"):\n",
        "    message = event[\"messages\"][-1]\n",
        "    msg_type = type(message).__name__\n",
        "\n",
        "    print(f\"Step Output [{msg_type}]:\")\n",
        "\n",
        "    # ツール呼び出し情報の表示\n",
        "    if hasattr(message, \"tool_calls\") and message.tool_calls:\n",
        "        for tool_call in message.tool_calls:\n",
        "            print(colored(f\"  Tool Call: {tool_call['name']} args={tool_call['args']}\", \"yellow\"))\n",
        "\n",
        "    # コンテンツの表示\n",
        "    if message.content:\n",
        "        print(f\"  Content: {message.content}\")\n",
        "\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "7zmNXNh_I-9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio による Web UI の実装\n",
        "\n",
        "構築した LangGraph アプリケーションを Web ブラウザから利用可能なチャット UI として実装します。\n",
        "Gradio の `ChatInterface` を使用することで、チャット履歴の表示や入力フォームなどの標準的な UI コンポーネントを容易に構築できます。\n",
        "\n",
        "### 実装のポイント\n",
        "* **ハンドラ関数**: Gradio からの入力を受け取り、LangGraph を実行し、結果のテキストのみを抽出して UI に返す関数 (`process_chat`) を定義します。\n",
        "* **UI 構成**: `gr.Blocks` を使用してレイアウトを定義し、タイトルや説明文を追加します。"
      ],
      "metadata": {
        "id": "jSfojdzyJALd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def process_chat(message, history):\n",
        "    \"\"\"\n",
        "    Gradio の入力ハンドラ。\n",
        "    type=\"messages\" を使用しない場合、history は [[user_msg, bot_msg], ...] の形式で渡されますが、\n",
        "    今回は LangGraph に現在のメッセージだけを渡す単純な構成のため、history は使用しません。\n",
        "\n",
        "    Args:\n",
        "        message (str): ユーザーの現在の入力\n",
        "        history (list): 過去の会話履歴\n",
        "    \"\"\"\n",
        "    # グラフへの入力データ作成\n",
        "    inputs = {\"messages\": [HumanMessage(content=message)]}\n",
        "\n",
        "    # グラフの実行\n",
        "    final_state = graph.invoke(inputs)\n",
        "\n",
        "    # 最終メッセージの内容を返却\n",
        "    return final_state[\"messages\"][-1].content\n",
        "\n",
        "# UI 構成\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# LangGraph Tool Calling UI\")\n",
        "    gr.Markdown(\n",
        "        \"LangGraph バックエンドによるツール実行デモです。\\n\"\n",
        "        \"入力例: **「3と5を足して」** と入力すると、内部で Python 関数が実行され、結果が返されます。\"\n",
        "    )\n",
        "\n",
        "    chat_interface = gr.ChatInterface(\n",
        "        fn=process_chat,\n",
        "        # type=\"messages\" を削除 (互換性確保のため)\n",
        "        examples=[\"3と5を足してください\", \"こんにちは\"],\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # share=True に設定することで、外部アクセス可能なURL (例: https://xxxxx.gradio.live) が発行されます\n",
        "    demo.launch(share=True)"
      ],
      "metadata": {
        "id": "xH6N7b9DJBHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
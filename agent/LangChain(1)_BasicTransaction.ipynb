{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpkPyFNeBfd1lXG4SIAQgW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/agent/LangChain(1)_BasicTransaction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📖 LangChain とは何か：LLM 開発の「無秩序」を防ぐための標準規格\n",
        "\n",
        "## 1. API を叩くだけなら LangChain は要らない\n",
        "まず誤解を恐れずに言えば、「ただ AI とチャットするだけの短いスクリプト」を書くなら、LangChain は不要です。OpenAI や DeepSeek の API を直接 `requests` や公式ライブラリで叩けば、わずか数行で動きます。\n",
        "\n",
        "しかし、**「プロトタイプ（試作品）」を「プロダクト（実用的なシステム）」にしようとした瞬間、開発の難易度は跳ね上がります。**\n",
        "\n",
        "LLM（大規模言語モデル）は、従来のソフトウェア部品とは異なり、**「確率的で、曖昧で、ステートレス（記憶を持たない）」** という扱いづらい性質を持っています。この扱いづらい LLM を、システムの中に信頼できる部品として組み込むためのフレームワークが **LangChain** です。\n",
        "\n",
        "---\n",
        "\n",
        "## 2. なぜ LangChain が必要なのか？\n",
        "\n",
        "LangChain を学ばずに、Python の標準機能だけで LLM アプリを作り始めた場合、遅かれ早かれ以下の **3つの「技術的負債」** に直面します。これこそが「LangChain を学んでおかないとまずい理由」です。\n",
        "\n",
        "### ① 「文字列操作スパゲッティ」の壁（保守性の欠如）\n",
        "LLM への入力はすべて「テキスト（プロンプト）」です。\n",
        "最初は Python の f-string（文字列埋め込み）で `f\"あなたは{role}です。次の質問に答えて：{question}\"` と書けば済みます。しかし、機能が増えるとコードは複雑化します。\n",
        "\n",
        "* 「過去の会話履歴を5往復分だけ入れたい」\n",
        "* 「ユーザーが英語で質問してきたら、英語用の指示に変えたい」\n",
        "* 「検索結果が空だった場合は、別のプロンプトを使いたい」\n",
        "\n",
        "これらを `if` 文と文字列結合だけで管理しようとすると、コードは瞬く間に解読不能な状態になります。LangChain は、プロンプトを単なる文字列ではなく **「管理可能なオブジェクト（部品）」** として扱い、きれいに結合する仕組みを提供します。\n",
        "\n",
        "### ② 「モデル依存」の壁（ベンダーロックイン）\n",
        "「OpenAI の GPT-5.2 でアプリを作りました。完璧です」\n",
        "数ヶ月後、上司やクライアントが言います。「DeepSeek R1 の方が安くて速いらしいから、そっちに変えて。あと、特定の処理だけは Claude 4.5 を使って。」\n",
        "\n",
        "LangChain を使っていない場合、あなたはコード内の **API 通信部分、パラメータ設定、エラー処理、レスポンスの受け取り処理をすべて書き直す** ことになります。各社で微妙に仕様が異なるからです。\n",
        "LangChain は、これらを共通のインターフェース（`ChatModel`）でラップしているため、たった1行、モデル名を書き換えるだけで移行が完了します。\n",
        "\n",
        "### ③ 「状態管理」の壁（ステートレスへの対応）\n",
        "HTTP 通信も LLM も、基本的には「ステートレス（前のことを覚えていない）」です。\n",
        "「今の発言」をするためには、「過去の会話履歴」をすべて毎回送信し直す必要があります。\n",
        "\n",
        "* 会話が長くなったら、古い順に捨てるのか？\n",
        "* 要約して圧縮するのか？\n",
        "* データベースに保存して、必要なときだけ取り出すのか？\n",
        "\n",
        "これをゼロから実装するのは非常にバグを生みやすい作業です。LangChain は、この **「記憶（Memory）」の管理** を標準機能として提供しており、データベースとの連携も容易です。\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 結論：LangChain は「LLM アプリの OS」\n",
        "LangChain を学ぶということは、単に便利なライブラリの使い方を覚えるということではありません。\n",
        "\n",
        "* **入力（Prompt）**\n",
        "* **処理（Model / Chain）**\n",
        "* **記憶（Memory）**\n",
        "* **出力（Output Parser）**\n",
        "\n",
        "これらをどのように設計・接続すべきかという **「LLM アプリケーションのアーキテクチャ（設計思想）」** を学ぶことと同義です。\n",
        "\n",
        "本コースでは、便利な機能を表面的になぞるのではなく、**「LangChain が裏側で何を解決してくれているのか」** をコードレベルで理解することを目指します。"
      ],
      "metadata": {
        "id": "wgEr90Q0K3qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔑 API キーの設定\n",
        "\n",
        "Colab の「シークレット（鍵マーク）」に登録した `OPENROUTER_API_KEY` を読み込みます。"
      ],
      "metadata": {
        "id": "HwdnG1izK60J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai langchain-core termcolor\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from termcolor import cprint\n",
        "\n",
        "try:\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "    cprint(\"✅ API Key loaded.\", \"green\")\n",
        "except Exception as e:\n",
        "    cprint(\"❌ Error: API Key not found in Colab secrets.\", \"red\")"
      ],
      "metadata": {
        "id": "S6I2TqdGK76_",
        "outputId": "a379c9d3-826f-46e4-b90f-7a1c5d065f31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/476.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h✅ API Key loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️⃣ LangChain でのモデル呼び出し\n",
        "\n",
        "LangChain では、OpenAI や OpenRouter などの異なるサービスを共通の方法で扱えるようになっています。\n",
        "ここでは `ChatOpenAI` クラスを使用し、OpenRouter 上の `deepseek/deepseek-chat-v3-0324` モデルに接続します。\n",
        "\n",
        "まずは最も単純な、文字列を渡して文字列を受け取る方法を確認します。"
      ],
      "metadata": {
        "id": "9cyMr0ZzK9yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# モデルの初期化\n",
        "# base_url を指定することで、OpenAI ではなく OpenRouter に接続します\n",
        "llm = ChatOpenAI(\n",
        "    model=\"deepseek/deepseek-chat-v3-0324\",\n",
        "    api_key=os.environ[\"OPENROUTER_API_KEY\"],\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# 単純なテキストを送信して実行 (invoke)\n",
        "response = llm.invoke(\"Pythonとは何ですか？一言で説明してください。\")\n",
        "\n",
        "# 結果の表示\n",
        "print(f\"▼ 返答の型: {type(response)}\")\n",
        "cprint(f\"▼ 返答内容:\\n{response.content}\", \"cyan\")"
      ],
      "metadata": {
        "id": "OApO2ArHK_l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2️⃣ ロール（役割）の指定\n",
        "\n",
        "LLM との対話では、単なるテキストではなく **「誰が発言したか（Role）」** という情報が重要になります。\n",
        "LangChain では、これを専用のクラスで管理します。\n",
        "\n",
        "* **SystemMessage:** AI の振る舞いや設定（例：「あなたは数学の先生です」）。\n",
        "* **HumanMessage:** ユーザーからの入力。\n",
        "\n",
        "これらをリスト（配列）にして渡すことで、文脈を持った対話が可能になります。"
      ],
      "metadata": {
        "id": "IBVEz898LD9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# メッセージリストの作成\n",
        "messages = [\n",
        "    SystemMessage(content=\"あなたは河村たかし氏のように名古屋弁を話す気軽なアシスタントです。\"),\n",
        "    HumanMessage(content=\"あつた蓬莱軒で食べようと思ったら満席だった。\")\n",
        "]\n",
        "\n",
        "# リストを渡して実行\n",
        "response_with_role = llm.invoke(messages)\n",
        "\n",
        "cprint(f\"▼ 静岡弁での返答:\\n{response_with_role.content}\", \"cyan\")"
      ],
      "metadata": {
        "id": "7GCj3a56LFic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3️⃣ 内部データ構造の確認（課題）\n",
        "\n",
        "LangChain で `SystemMessage` や `HumanMessage` を使って記述しましたが、**通信のレベルではこれらは単なる辞書（JSON形式のテキストデータ）** です。\n",
        "\n",
        "以下のコードを実行して、LangChain のオブジェクトが最終的にどのようなデータ形式で扱われているかを確認してください。これが LLM が理解している「実体」です。"
      ],
      "metadata": {
        "id": "UXHvWv0fL0Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChainのメッセージオブジェクトの中身を確認する\n",
        "print(\"--- LangChain Objects ---\")\n",
        "print(f\"SystemMessage: {messages[0]}\")\n",
        "print(f\"HumanMessage:  {messages[1]}\")\n",
        "\n",
        "print(\"\\n--- Equivalent Raw Data (JSON format) ---\")\n",
        "# 実際にAPIに送られるときは、以下のような辞書形式（プレインテキスト）になります\n",
        "raw_data = [\n",
        "    {\"role\": \"system\", \"content\": messages[0].content},\n",
        "    {\"role\": \"user\",   \"content\": messages[1].content}\n",
        "]\n",
        "\n",
        "import json\n",
        "print(json.dumps(raw_data, indent=2, ensure_ascii=False))\n",
        "\n",
        "# 【課題】\n",
        "# 上記の `raw_data` の構造を理解した上で、\n",
        "# messages リストにもう一つ HumanMessage を追加して、\n",
        "# 「Pythonで \"Hello\" と表示するコードを書いて」と依頼するコードを書いて、実行してください。\n",
        "# 追加後の messages を llm.invoke(messages) に渡します。\n",
        "\n",
        "# ↓ ここにコードを記述してください"
      ],
      "metadata": {
        "id": "q8pn_6ZjL2r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4️⃣ `invoke` メソッドの正体\n",
        "\n",
        "ここまで当たり前のように `llm.invoke(...)` を使ってきましたが、この `invoke` とは何でしょうか？\n",
        "\n",
        "LangChain における `invoke` は、**「入力（Input）を受け取り、処理を実行し、出力（Output）を返す」** という命令です。\n",
        "LLM オブジェクトに対して `invoke` を実行すると、以下のことが行われます。\n",
        "\n",
        "1.  **送信:** 引数で渡されたメッセージ（リストや文字列）を API サーバーへ送信する。\n",
        "2.  **待機:** サーバーでの計算が終わり、レスポンスが返ってくるまでプログラムの実行を停止して待つ（同期処理）。\n",
        "3.  **ラップ:** 返ってきた JSON データを `AIMessage` という扱いやすいオブジェクトに変換して返す。\n",
        "\n",
        "単に文字列を返しているのではなく、**「メタデータ（付加情報）を含んだメッセージオブジェクト」** を返している点に注目してください。\n",
        "\n",
        "以下の課題で、その戻り値を詳しく見てみましょう。"
      ],
      "metadata": {
        "id": "qbLka5YpMUEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# シンプルなメッセージを作成\n",
        "message = [HumanMessage(content=\"1+1は？\")]\n",
        "\n",
        "print(\"--- invoke 実行開始 ---\")\n",
        "\n",
        "# invokeを実行し、その戻り値をそのまま変数に格納\n",
        "result_object = llm.invoke(message)\n",
        "\n",
        "print(\"--- invoke 実行終了 ---\")\n",
        "\n",
        "# 【課題】 invoke の戻り値を詳しく確認\n",
        "# 以下の print 文の結果を見て、invoke が単なる文字列(\"2\")ではなく、\n",
        "# 情報を包んだオブジェクトを返していることを確認してください。\n",
        "\n",
        "print(f\"1. 戻り値の型 (Type): {type(result_object)}\")\n",
        "print(f\"2. オブジェクトの中身: {result_object}\")\n",
        "print(f\"3. コンテンツの抽出: {result_object.content}\")\n",
        "print(f\"4. API利用状況 (Metadata): {result_object.response_metadata}\")\n",
        "\n",
        "# ※ response_metadata には、トークン使用量やモデル名などが含まれています。\n",
        "# これらは invoke が API からの生のレスポンスを解析して格納したものです。"
      ],
      "metadata": {
        "id": "QU39M2GiMXyJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
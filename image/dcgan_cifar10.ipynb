{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOowP+leZzwsrhrpD+nBLxS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shizoda/education/blob/main/image/dcgan_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¨ DCGAN ã«ã‚ˆã‚‹ç”»åƒç”Ÿæˆï¼ˆCIFAR-10ï¼‰\n",
        "\n",
        "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ **æ·±å±¤ç•³ã¿è¾¼ã¿ç”Ÿæˆæ•µå¯¾ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆDCGAN: Deep Convolutional Generative Adversarial Networksï¼‰** ã‚’ç”¨ã„ã¦ã€CIFAR-10 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”»åƒç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚\n",
        "\n",
        "## ğŸ” å¾“æ¥ã®ç•³ã¿è¾¼ã¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã®æ¯”è¼ƒ\n",
        "\n",
        "DCGAN ã‚’ç†è§£ã™ã‚‹ä¸Šã§ã¯ã€CNN ã‚„ U-Net ã¨ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŠã‚ˆã³å‡ºåŠ›ã®ç›®çš„ã®é•ã„ã«æ³¨ç›®ã—ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "### 1. CNNï¼ˆç”»åƒåˆ†é¡ï¼‰\n",
        "**ç•³ã¿è¾¼ã¿å±¤ï¼ˆConvolutionï¼‰** ã‚’ç”¨ã„ã¦ç”»åƒã‹ã‚‰ç‰¹å¾´ã‚’æŠ½å‡ºã—ã€æœ€çµ‚çš„ã«ãã‚ŒãŒä½•ã§ã‚ã‚‹ã‹ã¨ã„ã†ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ç”»åƒç”Ÿæˆã¨ã¯é€†ã®ã€Œæƒ…å ±ã®åœ§ç¸®ã€ã‚’è¡Œã†ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚\n",
        "\n",
        "*  [PyTorch ã«ã‚ˆã‚‹ CNN ç”»åƒåˆ†é¡ï¼ˆCIFAR-10ï¼‰](https://github.com/shizoda/education/blob/ae8526e388ed2f2e05eb5209db51340d40f46d68/machine_learning/cnn/cifar10_pytorch.ipynb)\n",
        "\n",
        "### 2. U-Netï¼ˆç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰\n",
        "ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ç‰¹å¾´ã‚’æŠ½å‡ºã—ãŸå¾Œã€ **è»¢ç½®ç•³ã¿è¾¼ã¿ï¼ˆTransposed Convolutionï¼‰** ã‚’ç”¨ã„ã¦ç‰¹å¾´é‡ã‹ã‚‰å…ƒã®ç”»åƒã¨åŒã˜è§£åƒåº¦ã® **ç¢ºç‡ãƒãƒƒãƒ—** ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Œã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒã©ã®ç‰©ä½“ã«å±ã™ã‚‹ã‹ã€ã‚’ç‰¹å®šã—ã¾ã™ã€‚\n",
        "\n",
        "*  [U-Net ã«ã‚ˆã‚‹è‚ºãŒã‚“é ˜åŸŸã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://github.com/shizoda/education/blob/ae8526e388ed2f2e05eb5209db51340d40f46d68/machine_learning/unet/unet_lung_cancer.ipynb)\n",
        "\n",
        "### 3. DCGANï¼ˆç”»åƒç”Ÿæˆï¼‰\n",
        "DCGAN ã®ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰ã¯ã€U-Net ã®ãƒ‡ã‚³ãƒ¼ãƒ€ã¨åŒæ§˜ã« **è»¢ç½®ç•³ã¿è¾¼ã¿å±¤** ã‚’æ´»ç”¨ã—ã¾ã™ãŒã€ãã®ç›®çš„ãŒå¤§ããç•°ãªã‚Šã¾ã™ã€‚U-Net ãŒç‰¹å¾´é‡ã‹ã‚‰ç¢ºç‡ãƒãƒƒãƒ—ã‚’å¾©å…ƒã™ã‚‹ã®ã«å¯¾ã—ã€DCGAN ã¯ **ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¤ã‚ºï¼ˆæ½œåœ¨å¤‰æ•° $z$ï¼‰** ã‚’å…¥åŠ›ã¨ã—ã¦ã€ãã“ã‹ã‚‰ **æœ¬ç‰©ã‚‰ã—ã„ç”»åƒãã®ã‚‚ã®ï¼ˆFakeç”»åƒï¼‰** ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚\n",
        "\n",
        "| æ‰‹æ³• | å…¥åŠ› | ä¸»è¦æŠ€è¡“ | å‡ºåŠ› |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **CNN** | ç”»åƒ | ç•³ã¿è¾¼ã¿ | ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ« |\n",
        "| **DCGAN ã® Discriminator (åˆ†é¡å™¨)** | ç”»åƒ | ç•³ã¿è¾¼ã¿ | Real / Fake åˆ¤å®š ï¼ˆReal ã§ã‚ã‚‹ç¢ºç‡ï¼‰|\n",
        "| **U-Net** | ç”»åƒ | ç•³ã¿è¾¼ã¿ + è»¢ç½®ç•³ã¿è¾¼ã¿ | ç¢ºç‡ãƒãƒƒãƒ— |\n",
        "| **DCGAN ã® Generator (ç”Ÿæˆå™¨)** | ãƒã‚¤ã‚º | è»¢ç½®ç•³ã¿è¾¼ã¿ | ç”Ÿæˆç”»åƒï¼ˆFakeï¼‰ |"
      ],
      "metadata": {
        "id": "uNEeiyum5T4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®è©³ç´°\n",
        "\n",
        "DCGAN ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‚’è©³ç´°ã«è§£èª¬ã—ã¾ã™ã€‚ã‚ˆãã‚ã‹ã‚‰ãªã„å ´åˆã¯ã„ã£ãŸã‚“ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œã«é€²ã‚“ã§ãã ã•ã„ï½¡\n",
        "\n",
        "### 1. Generator (Decoupling and Expansion)\n",
        "Generator ã¯ **U-Net** ã®ãƒ‡ã‚³ãƒ¼ãƒ€éƒ¨ã¨åŒæ§˜ã« **Transposed Convolutionï¼ˆè»¢ç½®ç•³ã¿è¾¼ã¿ï¼‰** ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ U-Net ãŒã€Œç‰¹å¾´é‡ã‹ã‚‰ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ç¢ºç‡ãƒãƒƒãƒ—ã€ã‚’å¾©å…ƒã™ã‚‹ã®ã«å¯¾ã—ã€ DCGAN ã¯ 100 æ¬¡å…ƒã®æ½œåœ¨å¤‰æ•° $z$ ã‹ã‚‰ $32 \\times 32$ ã® **Fake Imageï¼ˆç”»åƒãã®ã‚‚ã®ï¼‰** ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
        "\n",
        "* **Spatial Transformation**: ã‚«ãƒ¼ãƒãƒ«ã‚µã‚¤ã‚º 4ã€ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ 2ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° 1 ã®è»¢ç½®ç•³ã¿è¾¼ã¿ã«ã‚ˆã‚Šã€è§£åƒåº¦ã‚’ $4 \\rightarrow 8 \\rightarrow 16 \\rightarrow 32$ ã¸ã¨æ®µéšçš„ã«æ‹¡å¤§ã—ã¾ã™ã€‚\n",
        "* **Activation**: å„å±¤ã§ **Batch Normalization** ã¨ **ReLU** ã‚’é©ç”¨ã—ã€æœ€çµ‚å±¤ã®ã¿ **Tanh** ã‚’ç”¨ã„ã¦ç”»ç´ å€¤ã‚’ $[-1, 1]$ ã«åã‚ã¾ã™ã€‚\n",
        "\n",
        "### 2. Discriminator (Feature Extraction and Compression)\n",
        "Discriminator ã¯å…¸å‹çš„ãª **CNN** ã®æ§‹é€ ã‚’æŒã¡ã¾ã™ã€‚é€šå¸¸ã®ç”»åƒåˆ†é¡å™¨ãŒã€Œç”»åƒ $\\rightarrow$ å¤šã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã€ã‚’å‡ºåŠ›ã™ã‚‹ã®ã«å¯¾ã—ã€ã“ã“ã§ã¯ã€Œç”»åƒ $\\rightarrow$ 1ã¤ã®ã‚¹ã‚«ãƒ©ãƒ¼å€¤ï¼ˆReal ã§ã‚ã‚‹ç¢ºç‡ï¼‰ã€ã¸ã¨æƒ…å ±ã‚’åœ§ç¸®ã—ã¾ã™ã€‚\n",
        "\n",
        "* **Spatial Transformation**: ç•³ã¿è¾¼ã¿ã«ã‚ˆã‚Šè§£åƒåº¦ã‚’ $32 \\rightarrow 16 \\rightarrow 8 \\rightarrow 4$ ã¨ç¸®å°ã—ã¦ã„ãã¾ã™ã€‚\n",
        "* **Activation**: å‹¾é…æ¶ˆå¤±ã‚’é˜²ããŸã‚ã« **LeakyReLU** ã‚’ä½¿ç”¨ã—ã€æœ€çµ‚å±¤ã§ **Sigmoid** ã‚’é©ç”¨ã—ã¦ç¢ºç‡å€¤ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚\n",
        "\n",
        "### 3. DCGAN Connectivity\n",
        "ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã¨ã—ã¦ã¯ã€ Generator ãŒç”Ÿæˆã—ãŸ Fake ç”»åƒã¨ã€ CIFAR-10 ã® Real ç”»åƒãŒ Discriminator ã«å…¥åŠ›ã•ã‚Œã‚‹æ§‹æˆã¨ãªã‚Šã¾ã™ã€‚å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œã€ Generator ã¯ Discriminator ã®åˆ¤æ–­ã‚’æƒ‘ã‚ã›ã‚‹ã»ã©ç²¾å·§ãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "jEfm_loSBKtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ğŸ“Š Visualize Network Structure\n",
        "# @markdown è¡¨ç¤ºã™ã‚‹ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹æˆã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\n",
        "display_target = \"dcgan_combined\" # @param [\"generator_detail\", \"discriminator_detail\", \"dcgan_combined\"]\n",
        "\n",
        "import base64\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def render_mermaid(graph_code):\n",
        "    \"\"\"\n",
        "    Renders Mermaid diagram using mermaid.ink API.\n",
        "    Supports UTF-8 characters and renders high-quality PNG.\n",
        "    \"\"\"\n",
        "    graphbytes = graph_code.encode(\"utf-8\")\n",
        "    base64_bytes = base64.b64encode(graphbytes)\n",
        "    base64_string = base64_bytes.decode(\"ascii\")\n",
        "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
        "\n",
        "# Define Mermaid codes for each component\n",
        "graphs = {\n",
        "    \"generator_detail\": \"\"\"\n",
        "graph TD\n",
        "    Z[Input Noise: 100x1x1] --> G1[ConvTranspose2d: k4 s1 p0]\n",
        "    G1 --> G1_Out[Feature Map: 512x4x4]\n",
        "    G1_Out --> G1_Act[BN / ReLU]\n",
        "    G1_Act --> G2[ConvTranspose2d: k4 s2 p1]\n",
        "    G2 --> G2_Out[Feature Map: 256x8x8]\n",
        "    G2_Out --> G2_Act[BN / ReLU]\n",
        "    G2_Act --> G3[ConvTranspose2d: k4 s2 p1]\n",
        "    G3 --> G3_Out[Feature Map: 128x16x16]\n",
        "    G3_Out --> G3_Act[BN / ReLU]\n",
        "    G3_Act --> G4[ConvTranspose2d: k4 s2 p1]\n",
        "    G4 --> G4_Out[Output Image: 3x32x32]\n",
        "    G4_Out --> G_Tanh[Tanh Activation]\n",
        "    \"\"\",\n",
        "\n",
        "    \"discriminator_detail\": \"\"\"\n",
        "graph TD\n",
        "    Input[Input Image: 3x32x32] --> D1[Conv2d: k4 s2 p1]\n",
        "    D1 --> D1_Out[Feature Map: 64x16x16]\n",
        "    D1_Out --> D1_Act[LeakyReLU]\n",
        "    D1_Act --> D2[Conv2d: k4 s2 p1]\n",
        "    D2 --> D2_Out[Feature Map: 128x8x8]\n",
        "    D2_Out --> D2_Act[BN / LeakyReLU]\n",
        "    D2_Act --> D3[Conv2d: k4 s2 p1]\n",
        "    D3 --> D3_Out[Feature Map: 256x4x4]\n",
        "    D3_Out --> D3_Act[BN / LeakyReLU]\n",
        "    D3_Act --> D4[Conv2d: k4 s1 p0]\n",
        "    D4 --> D4_Out[Logit: 1x1x1]\n",
        "    D4_Out --> D_Sig[Sigmoid Activation]\n",
        "    \"\"\",\n",
        "\n",
        "    \"dcgan_combined\": \"\"\"\n",
        "graph LR\n",
        "    subgraph DCGAN_Connectivity [DCGAN System Structure]\n",
        "        Z[Latent Noise z] --> G[Generator]\n",
        "        G -- Fake Image --> D\n",
        "        Real[Real Image: CIFAR-10] --> D[Discriminator]\n",
        "        D --> Prob[Output: Probability]\n",
        "    end\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "print(f\"Displaying: {display_target}\")\n",
        "render_mermaid(graphs[display_target])"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "64Gqd8kjBP7w",
        "outputId": "1539bf02-2b78-47c1-a2da-6186fd45fd6e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying: dcgan_combined\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSCiAgICBzdWJncmFwaCBEQ0dBTl9Db25uZWN0aXZpdHkgW0RDR0FOIFN5c3RlbSBTdHJ1Y3R1cmVdCiAgICAgICAgWltMYXRlbnQgTm9pc2Ugel0gLS0+IEdbR2VuZXJhdG9yXQogICAgICAgIEcgLS0gRmFrZSBJbWFnZSAtLT4gRAogICAgICAgIFJlYWxbUmVhbCBJbWFnZTogQ0lGQVItMTBdIC0tPiBEW0Rpc2NyaW1pbmF0b3JdCiAgICAgICAgRCAtLT4gUHJvYltPdXRwdXQ6IFByb2JhYmlsaXR5XQogICAgZW5kCiAgICA=\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ› ï¸ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š\n"
      ],
      "metadata": {
        "id": "Sb5yrkidBYXj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEjmJ3nAK2h2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "params = {\n",
        "    \"n_epochs\": 300,      # ã‚¨ãƒãƒƒã‚¯æ•°\n",
        "    \"batch_size\": 128,    # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
        "    \"lr\": 0.0002,         # å­¦ç¿’ç‡\n",
        "    \"beta1\": 0.5,         # Adamã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "    \"nz\": 100,            # æ½œåœ¨å¤‰æ•°ã®æ¬¡å…ƒæ•° (ãƒã‚¤ã‚ºã®ã‚µã‚¤ã‚º)\n",
        "    \"ngf\": 128,            # Generatorã®ç‰¹å¾´ãƒãƒƒãƒ—ã®ã‚µã‚¤ã‚º\n",
        "    \"ndf\": 64,            # Discriminatorã®ç‰¹å¾´ãƒãƒƒãƒ—ã®ã‚µã‚¤ã‚º\n",
        "    \"image_size\": 32,     # CIFAR-10ã®ç”»åƒã‚µã‚¤ã‚ºã¯32x32\n",
        "    \"nc\": 3,              # ç”»åƒã®ãƒãƒ£ãƒ³ãƒãƒ«æ•° (CIFAR-10ã¯ã‚«ãƒ©ãƒ¼ãªã®ã§3)\n",
        "    \"target_class_name\": \"automobile\" # å­¦ç¿’ã—ãŸã„ã‚¯ãƒ©ã‚¹å\n",
        "}\n",
        "\n",
        "# GPUãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèªã—ã€ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ› ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ã¨æ­£è¦åŒ–\n",
        "\n",
        "ã“ã“ã§ã¯ã€CIFAR-10 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ï¼ˆä»Šå›ã¯ **automobile** ï¼‰ã®ã¿ã‚’æŠ½å‡ºã—ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "\n",
        "DCGAN ã®å­¦ç¿’ã«ãŠã„ã¦ã¯ã€Generator ã®æœ€çµ‚å±¤ã« `Tanh` é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒå¤šã„ãŸã‚ãã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ **[-1, 1]** ã®ç¯„å›²ã«æ­£è¦åŒ–ã™ã‚‹ã“ã¨ãŒä¸€èˆ¬çš„ã§ã™ã€‚"
      ],
      "metadata": {
        "id": "ztIClLNG6KmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ç”»åƒã®å‰å‡¦ç†\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(params[\"image_size\"]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # [-1, 1]ã«æ­£è¦åŒ–\n",
        "])\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "full_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "# CIFAR-10ã®ã‚¯ãƒ©ã‚¹å\n",
        "class_names = full_dataset.classes\n",
        "print(f\"CIFAR-10 class names: {class_names}\")\n",
        "\n",
        "# å­¦ç¿’å¯¾è±¡ã®ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
        "try:\n",
        "    target_class_idx = class_names.index(params[\"target_class_name\"])\n",
        "    print(f\"Targeting class: '{params['target_class_name']}' (index: {target_class_idx})\")\n",
        "except ValueError:\n",
        "    print(f\"Error: '{params['target_class_name']}' is not a valid CIFAR-10 class name. Please choose from {class_names}\")\n",
        "    exit()\n",
        "\n",
        "# ç‰¹å®šã®ã‚¯ãƒ©ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º\n",
        "indices = [i for i, label in enumerate(full_dataset.targets) if label == target_class_idx]\n",
        "dataset = Subset(full_dataset, indices)\n",
        "dataloader = DataLoader(dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
        "\n",
        "if len(dataset) == 0:\n",
        "    print(f\"Error: No images found for class '{params['target_class_name']}'. Please check the class name or dataset.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"Number of images for '{params['target_class_name']}': {len(dataset)}\")\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ– (ç¢ºèªç”¨)\n",
        "real_batch = next(iter(dataloader))\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Training Images ({params['target_class_name']})\")\n",
        "plt.imshow(np.transpose(make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(), (1, 2, 0)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-UtyGdj8K5aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ—ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©\n",
        "\n",
        "DCGAN ã¯ **ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰** ã¨ **è­˜åˆ¥å™¨ï¼ˆDiscriminatorï¼‰** ã® 2 ã¤ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒäº’ã„ã«ç«¶ã„åˆã†ã“ã¨ã§å­¦ç¿’ãŒé€²ã¿ã¾ã™ã€‚\n",
        "\n",
        "### ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰\n",
        "ç”Ÿæˆå™¨ã®å½¹å‰²ã¯ã€100æ¬¡å…ƒãªã©ã®ä½æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆãƒã‚¤ã‚ºï¼‰ã‚’ã€æœ¬ç‰©ã®ç”»åƒã«è¦‹ãˆã‚‹å¤šæ¬¡å…ƒã®ãƒ‡ãƒ¼ã‚¿ã¸ã¨å¤‰æ›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã«ã¯ **è»¢ç½®ç•³ã¿è¾¼ã¿ï¼ˆConvTranspose2dï¼‰** ã‚’ç¹°ã‚Šè¿”ã—ä½¿ç”¨ã—ã€è§£åƒåº¦ã‚’æ®µéšçš„ã«æ‹¡å¤§ï¼ˆUp-samplingï¼‰ã—ã¦ã„ãã¾ã™ã€‚\n",
        "\n",
        "### ç”Ÿæˆå™¨ï¼ˆGeneratorï¼‰ã®å®Ÿè£…\n",
        "ç”Ÿæˆå™¨ã¯ã€å…¥åŠ›ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•° $z$ï¼ˆ100æ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’ã€æ®µéšçš„ãªè»¢ç½®ç•³ã¿è¾¼ã¿ã«ã‚ˆã£ã¦ $32 \\times 32$ ãƒ”ã‚¯ã‚»ãƒ«ã®ã‚«ãƒ©ãƒ¼ç”»åƒï¼ˆ3ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰ã¸ã¨å¤‰æ›ã—ã¾ã™ã€‚å„å±¤ã®å¾Œã«ã¯ **ãƒãƒƒãƒæ­£è¦åŒ–ï¼ˆBatch Normalizationï¼‰** ã‚’é©ç”¨ã—ã€å­¦ç¿’ã‚’å®‰å®šã•ã›ã¾ã™ã€‚æœ€çµ‚å±¤ã«ã¯ `Tanh` é–¢æ•°ã‚’ç”¨ã„ã€å‡ºåŠ›ã‚’ **[-1, 1]** ã®ç¯„å›²ã«åã‚ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "N6Oj7SGH6YMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # å…¥åŠ›: æ½œåœ¨å¤‰æ•° z\n",
        "            nn.ConvTranspose2d(params[\"nz\"], params[\"ngf\"] * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(params[\"ngf\"] * 4),\n",
        "            nn.ReLU(True),\n",
        "            # ç‰¹å¾´ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: (ngf*4) x 4 x 4\n",
        "            nn.ConvTranspose2d(params[\"ngf\"] * 4, params[\"ngf\"] * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(params[\"ngf\"] * 2),\n",
        "            nn.ReLU(True),\n",
        "            # ç‰¹å¾´ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: (ngf*2) x 8 x 8\n",
        "            nn.ConvTranspose2d(params[\"ngf\"] * 2, params[\"ngf\"], 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(params[\"ngf\"]),\n",
        "            nn.ReLU(True),\n",
        "            # ç‰¹å¾´ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: (ngf) x 16 x 16\n",
        "            nn.ConvTranspose2d(params[\"ngf\"], params[\"nc\"], 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # å‡ºåŠ›ç”»åƒã‚µã‚¤ã‚º: (nc) x 32 x 32\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
        "netG = Generator(params).to(device)"
      ],
      "metadata": {
        "id": "-GH2P1fxK6tM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ›¡ï¸ è­˜åˆ¥å™¨ï¼ˆDiscriminatorï¼‰\n",
        "\n",
        "è­˜åˆ¥å™¨ã¯ã€å…¥åŠ›ã•ã‚ŒãŸç”»åƒãŒã€Œæœ¬ç‰©ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰ã€ã‹ã€Œå½ç‰©ï¼ˆGeneratorãŒç”Ÿæˆã—ãŸã‚‚ã®ï¼‰ã€ã‹ã‚’åˆ¤å®šã™ã‚‹ãƒã‚¤ãƒŠãƒªåˆ†é¡å™¨ã§ã™ã€‚\n",
        "CNN ç”»åƒåˆ†é¡å™¨ã¨åŒæ§˜ã®æ§‹æˆã‚’æŒã¡ã¾ã™ãŒã€DCGAN ã®è¨­è¨ˆæŒ‡é‡ã«å¾“ã„ã€å…¨çµåˆå±¤ã®ä»£ã‚ã‚Šã« **ç•³ã¿è¾¼ã¿å±¤ï¼ˆConvolutionï¼‰** ã‚’é‡ã­ã¦æƒ…å ±ã‚’é›†ç´„ã—ã¾ã™ã€‚æ´»æ€§åŒ–é–¢æ•°ã«ã¯ **LeakyReLU** ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
        "\n",
        "U-Net ãŒã€Œã©ã®ãƒ”ã‚¯ã‚»ãƒ«ãŒä½•ã§ã‚ã‚‹ã‹ã€ã‚’å‡ºåŠ›ã™ã‚‹ã®ã«å¯¾ã—ã€è­˜åˆ¥å™¨ã¯æœ€çµ‚çš„ã«ç”»åƒå…¨ä½“ã«å¯¾ã—ã¦ã€Œæœ¬ç‰©ã§ã‚ã‚‹ç¢ºç‡ã€ã¨ã„ã† **ã‚¹ã‚«ãƒ©ãƒ¼å€¤** ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "go5Ejqli6fnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # å…¥åŠ›ç”»åƒã‚µã‚¤ã‚º: (nc) x 32 x 32\n",
        "            nn.Conv2d(params[\"nc\"], params[\"ndf\"], 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # ç‰¹å¾´ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: (ndf) x 16 x 16\n",
        "            nn.Conv2d(params[\"ndf\"], params[\"ndf\"] * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(params[\"ndf\"] * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # ç‰¹å¾´ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: (ndf*2) x 8 x 8\n",
        "            nn.Conv2d(params[\"ndf\"] * 2, params[\"ndf\"] * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(params[\"ndf\"] * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # ç‰¹å¾´ãƒãƒƒãƒ—ã‚µã‚¤ã‚º: (ndf*4) x 4 x 4\n",
        "            nn.Conv2d(params[\"ndf\"] * 4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
        "netD = Discriminator(params).to(device)"
      ],
      "metadata": {
        "id": "E8kGAUWkK89Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš–ï¸ å­¦ç¿’ã®è¨­å®šã¨é‡ã¿ã®åˆæœŸåŒ–ï¼ˆæœ¬é¡Œã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰\n",
        "\n",
        "DCGAN ã®è«–æ–‡ã§ã¯ã€å…¨ã¦ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ã‚’å¹³å‡ 0ã€æ¨™æº–åå·® 0.02 ã®æ­£è¦åˆ†å¸ƒã§åˆæœŸåŒ–ã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€æå¤±é–¢æ•°ã«ã¯ **ãƒã‚¤ãƒŠãƒªã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼èª¤å·®ï¼ˆBCELossï¼‰** ã‚’ã€æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ **Adam** ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "QP1Gc_bS63no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# é‡ã¿ã®åˆæœŸåŒ–é–¢æ•°\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# åˆæœŸåŒ–ã®é©ç”¨\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "# æå¤±é–¢æ•°\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# å›ºå®šãƒã‚¤ã‚ºï¼ˆå­¦ç¿’ã®é€²æ—ã‚’åŒã˜è¦–ç‚¹ã§ç¢ºèªã™ã‚‹ãŸã‚ï¼‰\n",
        "fixed_noise = torch.randn(64, params[\"nz\"], 1, 1, device=device)\n",
        "\n",
        "# æœ€é©åŒ–æ‰‹æ³•\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=params[\"lr\"], betas=(params[\"beta1\"], 0.999))"
      ],
      "metadata": {
        "id": "L6KK3xPt66ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸš€ å­¦ç¿’ã®å®Ÿè¡Œ\n",
        "\n",
        "å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§ã¯ã€ä»¥ä¸‹ã® 2 ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚\n",
        "1. **è­˜åˆ¥å™¨ï¼ˆDï¼‰ã®æ›´æ–°**: æœ¬ç‰©ã‚’ã€Œæœ¬ç‰©ï¼ˆ1ï¼‰ã€ã€å½ç‰©ã‚’ã€Œå½ç‰©ï¼ˆ0ï¼‰ã€ã¨æ­£ã—ãåˆ¤å®šã§ãã‚‹ã‚ˆã†ã«å­¦ç¿’ã—ã¾ã™ã€‚\n",
        "2. **ç”Ÿæˆå™¨ï¼ˆGï¼‰ã®æ›´æ–°**: ç”Ÿæˆã—ãŸç”»åƒãŒ D ã«ã€Œæœ¬ç‰©ã€ã¨èª¤èªã•ã‚Œã‚‹ï¼ˆD ã®å‡ºåŠ›ã‚’ 1 ã«è¿‘ã¥ã‘ã‚‹ï¼‰ã‚ˆã†ã«å­¦ç¿’ã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "YzCfSO1f7NLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "img_list = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "for epoch in range(params[\"n_epochs\"]):\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch [{epoch+1}/{params['n_epochs']}]\", leave=False)\n",
        "    for i, data in enumerate(progress_bar):\n",
        "\n",
        "        # --- (1) è­˜åˆ¥å™¨ D ã®æ›´æ–° ---\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), 1.0, dtype=torch.float, device=device) # æœ¬ç‰©ãƒ©ãƒ™ãƒ« = 1\n",
        "\n",
        "        # æœ¬ç‰©ç”»åƒã§ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "\n",
        "        # å½ç‰©ç”»åƒã®ç”Ÿæˆã¨ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰\n",
        "        noise = torch.randn(b_size, params[\"nz\"], 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(0.0) # å½ç‰©ãƒ©ãƒ™ãƒ« = 0\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        # --- (2) ç”Ÿæˆå™¨ G ã®æ›´æ–° ---\n",
        "        netG.zero_grad()\n",
        "        label.fill_(1.0) # ç”Ÿæˆå™¨ã«ã¨ã£ã¦ã¯ã€Dã‚’é¨™ã—ã¦ 1 ã¨è¨€ã‚ã›ãŸã„\n",
        "        output = netD(fake).view(-1)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # ãƒ­ã‚°ã®è¨˜éŒ²\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        progress_bar.set_postfix({\"Loss_D\": f\"{errD.item():.4f}\", \"Loss_G\": f\"{errG.item():.4f}\"})\n",
        "\n",
        "    # ä¸€å®šæœŸé–“ã”ã¨ã«ç”Ÿæˆç”»åƒã‚’ä¿å­˜\n",
        "    if (epoch % 10 == 0) or (epoch == params[\"n_epochs\"]-1):\n",
        "        with torch.no_grad():\n",
        "            fake = netG(fixed_noise).detach().cpu()\n",
        "        img_list.append(make_grid(fake, padding=2, normalize=True))\n",
        "\n",
        "# æå¤±ã®æ¨ç§»\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses, label=\"G\")\n",
        "plt.plot(D_losses, label=\"D\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Training Finished.\")"
      ],
      "metadata": {
        "id": "I0wdFRrlK92O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š çµæœã®å¯è¦–åŒ–\n",
        "\n",
        "å­¦ç¿’å¾Œã®æå¤±é–¢æ•°ã®æ¨ç§»ã¨ã€Generator ãŒç”Ÿæˆã—ãŸç”»åƒã‚’ç¢ºèªã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "Q5LMRkai7Vp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æœ€å¾Œã®_ã‚¨ãƒãƒƒã‚¯ã§ç”Ÿæˆã•ã‚ŒãŸç”»åƒã‚’è¡¨ç¤º\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Generated Images for {params['target_class_name']}\")\n",
        "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "plt.show()\n",
        "\n",
        "if len(dataloader) > 0:\n",
        "    real_batch = next(iter(dataloader))\n",
        "\n",
        "    # --- å¤‰æ›´ç®‡æ‰€ ---\n",
        "\n",
        "    # 1è¡Œ2åˆ—ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ (figsizeã‚’ (15,15) ã‹ã‚‰ (16,8) ã®ã‚ˆã†ãªæ¨ªé•·ã«èª¿æ•´)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    # 1. é †åºã‚’é€†ã«ã™ã‚‹ (Fake Images ã‚’å·¦ã« [0])\n",
        "    axes[0].imshow(np.transpose(img_list[-1], (1, 2, 0)))\n",
        "    axes[0].set_title(f\"Fake Images ({params['target_class_name']})\")\n",
        "\n",
        "    # 2. é †åºã‚’é€†ã«ã™ã‚‹ (Real Images ã‚’å³ã« [1])\n",
        "    # make_gridã®å‡¦ç†ã‚’å¤‰æ•°ã«æ ¼ç´\n",
        "    real_images_grid = make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu()\n",
        "    axes[1].imshow(np.transpose(real_images_grid, (1, 2, 0)))\n",
        "    axes[1].set_title(f\"Real Images ({params['target_class_name']})\")\n",
        "\n",
        "    # 2. ä¸¡æ–¹ã®æ ç·šã‚’èª¿æ•´ (ä¸¡æ–¹ã®è»¸ã‚’éè¡¨ç¤ºã«ã™ã‚‹)\n",
        "    axes[0].axis('off')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆé–“ã®ä½™ç™½ã‚’èª¿æ•´\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "    # --- å¤‰æ›´ã“ã“ã¾ã§ ---\n",
        "\n",
        "else:\n",
        "    print(f\"Cannot display real images as the dataloader for {params['target_class_name']} is empty.\")"
      ],
      "metadata": {
        "id": "FtotqV_7LAAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}